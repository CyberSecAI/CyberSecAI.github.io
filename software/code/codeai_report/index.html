
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://github.com/CyberSecAI/CyberSecAI.github.io/software/code/codeai_report/">
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.33">
    
    
      
        <title>Codeai report - CyberSecAI</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../../../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../../../css/print-site.css">
    
      <link rel="stylesheet" href="../../../css/print-site-material.css">
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-NMMB0KXVGQ"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-NMMB0KXVGQ",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-NMMB0KXVGQ",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="cyan" data-md-color-accent="purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
  THIS GUIDE IS UNDER CONSTRUCTION... BUT AVAILABLE HERE FOR EARLY ACCESS REVIEW & FEEDBACK. 

          </div>
          
        </aside>
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="CyberSecAI" class="md-header__button md-logo" aria-label="CyberSecAI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 3C9.2 3 6.2 5.9 6 9.7l-1.9 2.5c-.2.3 0 .8.4.8H6v3c0 1.1.9 2 2 2h1v3h7v-4.7c2.4-1.1 4-3.5 4-6.3 0-3.9-3.1-7-7-7m4 7h-3v3h-2v-3H9V8h3V5h2v3h3v2Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CyberSecAI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Codeai report
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="cyan" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="CyberSecAI" class="md-nav__button md-logo" aria-label="CyberSecAI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 3C9.2 3 6.2 5.9 6 9.7l-1.9 2.5c-.2.3 0 .8.4.8H6v3c0 1.1.9 2 2 2h1v3h7v-4.7c2.4-1.1 4-3.5 4-6.3 0-3.9-3.1-7-7-7m4 7h-3v3h-2v-3H9V8h3V5h2v3h3v2Z"/></svg>

    </a>
    CyberSecAI
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../introduction/Preface/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preface
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../introduction/Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../introduction/models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../introduction/cybsersecurity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLMs for CyberSecurity
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    NotebookLM
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            NotebookLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM_VulnerabilityStandards/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM Vulnerability Standards
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM_Capec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM CAPEC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM_Attack/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM ATTACK
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM_Config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../NotebookLM/NotebookLM_Secure_Code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM Secure Code
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Grounded Closed System
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Grounded Closed System
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Grounded/grounding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grounding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CWE Assignment
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            CWE Assignment
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CWE_Assignment/cwe_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CWE_Assignment/cwe_gpt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ChatGPT CWE GPT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CWE_Assignment/NotebookLM_Cwe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NotebookLM CWE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CWE_Assignment/vertex_ai/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vertex AI CWE Agent
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CWE_Assignment/Vulnrichment/" class="md-nav__link">
        
  
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m13 18.9 6.1-6.1 2.1 2.1-6.1 6.1H13v-2.1m8.4-7.6 1.3 1.3c.2.2.2.5 0 .7l-1 1-2.1-2 1-1c.1-.1.2-.2.4-.2s.3 0 .4.2M11 21H5c-.5 0-1-.2-1.4-.6-.4-.4-.6-.9-.6-1.4V5c0-.5.2-1 .6-1.4C4 3.2 4.5 3 5 3h14c1.1 0 2 .9 2 2v4h-2V5H5v14h6v2m4-9-5-4v8l5-4Z"/></svg>
  
  <span class="md-ellipsis">
    CISA Vulnrichment
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Prompting
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Prompting
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../prompt_engineering/prompt_engineering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt Engineering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../prompt_engineering/Fabric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fabric
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Engineering
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Software Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software Engineering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software_assurance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software Assurance
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software_artifacts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software Artifacts
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../code_assistant_agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Code Assistant vs. Autonomous Coding Agents
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../leaderboards/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Leaderboards
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../swe_agents_report/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software Engineering Agents Report
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../swe_agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software Engineering Agents
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CyberSecurity Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            CyberSecurity Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../cti_models/cyber_security_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CyberSecurity LLMs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agents/Agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agents
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../agents/Build_agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build Agents
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    TopicModeling
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            TopicModeling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../TopicModeling/BertTopic/" class="md-nav__link">
        
  
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m13 18.9 6.1-6.1 2.1 2.1-6.1 6.1H13v-2.1m8.4-7.6 1.3 1.3c.2.2.2.5 0 .7l-1 1-2.1-2 1-1c.1-.1.2-.2.4-.2s.3 0 .4.2M11 21H5c-.5 0-1-.2-1.4-.6-.4-.4-.6-.9-.6-1.4V5c0-.5.2-1 .6-1.4C4 3.2 4.5 3 5 3h14c1.1 0 2 .9 2 2v4h-2V5H5v14h6v2m4-9-5-4v8l5-4Z"/></svg>
  
  <span class="md-ellipsis">
    BertTopic
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    References
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../research/research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_11" >
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Annex
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Annex
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../annex/Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../annex/Glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../research/research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Codeai report</h1>

<p>From Code Completion to Autonomous Coding Agents: AI Developer Tools by Autonomy LevelThe software development landscape is undergoing a significant transformation, largely propelled by the rapid advancements and integration of Artificial Intelligence (AI). This evolution is underscored by substantial growth in private AI investment, with generative AI alone attracting $33.9 billion globally in 2024, marking an 18.7% increase from the previous year.1 AI is no longer confined to research laboratories; it is becoming a pervasive force in daily life and business operations, evidenced by 78% of organizations reporting AI use in 2024, a notable rise from 55% in 2023.1 This trend extends deeply into the realm of developer tooling, where AI-powered solutions are becoming increasingly sophisticated. The discourse itself is shifting from AI as a tool for mere "automation" to AI exhibiting "autonomy," signifying a deeper capacity for independent operation and decision-making.2 Accenture's Technology Vision for 2025 highlights this transition, projecting a future where AI acts autonomously on behalf of individuals, with a reported 75% of knowledge workers already utilizing generative AI.2 This massive investment and widespread adoption signal strong market confidence and a perceived return on investment, suggesting that AI developer tools are being embraced at a pace necessary to remain competitive and enhance productivity, aligning with research indicating AI's potential to boost productivity and narrow skill gaps.1This report provides an expert-level analysis of these AI developer tools, categorizing them based on a five-level autonomy framework. This framework, analogous to the levels established for autonomous driving systems, serves to delineate the evolving capabilities of these tools, from basic code assistants to highly autonomous coding agents. Such a classification is pertinent because both autonomous driving and AI coding involve tasks traditionally performed by humans that demand complex reasoning, contextual understanding, and decision-making. The staged levels in both domains represent milestones in the delegation of these responsibilities to AI, thereby raising similar questions about reliability, accountability, and the evolving role of human oversight.4 The challenges anticipated for Level 5 autonomy in vehicles, such as regulatory hurdles, technological limitations, and development costs 4, are likely to find parallels in the domain of AI coding. This report will explore the capabilities, interaction modes, use cases, and the pros and cons of representative tools at each autonomy level, ensuring factual accuracy through direct references to current research and official documentation. The increasing integration of AI into development workflows promises substantial productivity gains 1 but also introduces new considerations regarding developer roles, code quality, security, and trust. This document aims to navigate these complexities, offering a clear perspective on the current state and future trajectory of AI in software engineering.Autonomy Levels for AI Coding ToolsTo provide a structured understanding of their capabilities, AI coding tools are classified into autonomy levels, drawing an analogy to the widely recognized levels of autonomous driving. This classification helps to differentiate tools based on the degree of human oversight required and the complexity of tasks they can handle.</p>
<p>Level 1 – Basic Code Completion:AI at this level offers single-line or small code suggestions based on the immediate context, such as the current file content and syntax. The developer remains in full control, accepting or rejecting these suggestions for micro-tasks. This functionality is comparable to basic cruise control in automobiles. Tools categorized under Level 1 are designed to accelerate typing and minimize trivial errors but do not generate complex code blocks, understand broader project context, or make independent decisions regarding logic or structure [User Query]. Their primary function is to react to the developer's current input by predicting likely code sequences. The focus is on immediate lexical and syntactic context rather than deep semantic understanding. For instance, Visual Studio Code's IntelliSense provides completions based on "language semantics and an analysis of your source code," but for many languages, this is limited to "word-based completions" without specific extensions 7, reacting to typed input within the current file. Similarly, Tabnine offers "context-aware suggestions based on your code and patterns" but mainly provides "code completions for current line and multiple lines for full-function implementation" 8, which is also a reactive process. The core function is to "predict likely code to follow," not to comprehend higher-level intent [User Query].</p>
<p>Level 2 – Advanced Code Generation (AI Pair Programmer):AI systems at this level can produce larger code snippets or entire functions based on natural language prompts (e.g., comments within the code) or a broader local context. The AI functions akin to a junior pair programmer. However, each output from the AI is typically a single-step operation that is subsequently reviewed and integrated by the human developer. Developers continue to guide the overall software architecture, verify the correctness of the generated code, and explicitly prompt the AI for each significant generation task [User Query]. These tools are often powered by large language models (LLMs) trained on extensive code repositories. The defining characteristic of Level 2 is the AI's capacity to translate natural language intent into substantial code blocks, moving beyond simple completion to actual generation, yet still necessitating explicit human prompting for each distinct operation. For example, GitHub Copilot can suggest "whole lines or entire functions" based on comments or existing code context 10, with the interaction being a prompt-response for each generated piece. Amazon CodeWhisperer generates "suggestions based on your existing code and comments," which can range from "a single line comment to fully formed functions" 11, initiated by the developer's input. OpenAI Codex and its more advanced successors like GPT-4 are designed to "translates natural language into code" 12, representing a direct prompt-to-code generation step. In all these instances, the human developer acts as the driver, prompting for each segment of code and undertaking its review, as the AI does not autonomously chain actions.</p>
<p>Level 3 – Context-Aware Agents (Supervised Automation):At this level, AI demonstrates more agentic behavior, capable of performing multi-step coding tasks within an iterative loop to achieve a high-level goal defined by the developer (e.g., “Add a profile feature,” “Fix this bug across these files”). The AI takes on the responsibility of planning the necessary steps, writing or editing multiple files, running code or tests, debugging, and iterating with minimal prompting once the initial objective is set. The human developer supervises this process, with the option to intervene or review final changes, but refrains from micromanaging each individual code edit or command execution. This dynamic is comparable to a junior developer autonomously working on a feature under the guidance of a senior colleague [User Query]. These agents typically maintain state, possess a broader understanding of the codebase, and can interact directly with the development environment, including the IDE, command-line interface (CLI), and version control systems. The critical advancement to Level 3 is the AI's capacity to autonomously plan and execute a sequence of actions—multi-step operations—towards a developer-defined goal, which includes interaction with the development environment (e.g., running tests, editing multiple files). For example, GitHub Copilot Agent Mode "performs multi-step coding tasks... analyzing your codebase, reading relevant files, proposing file edits, and running terminal commands and tests... auto-corrects in a loop".14 Cursor's Agent Mode "independently explores your codebase, identifies relevant files, and makes necessary changes... uses all available tools to search, edit, create files, and run terminal commands," and it "breaks complex tasks into manageable steps and executes them in sequence".15 Windsurf's Cascade feature "generates or modifies code... asks for your approval before running it in the terminal... prompts you with follow-up questions" in an iterative cycle 16; its Write Mode can "create multiple files for you, run scripts, test them, and debug them".16 Aider "automatically commits changes with sensible commit messages" and "automatically lint and test your code every time aider makes changes... can fix problems detected by your linters and test suites" 17, indicating multiple automated steps. Anthropic's Claude Code is capable of "edit files and fixing bugs across your codebase... executing and fixing tests... creating commits and PRs" 19, a clear sequence of diverse actions. The common characteristic is the AI's initiative in handling sub-tasks after receiving a high-level directive, a hallmark of agency.</p>
<p>Level 4 – Highly Autonomous Coding:AI tools at this level can manage complex projects or features end-to-end, requiring only minimal human input, such as an initial prompt or specification, and theoretically, no mandatory oversight on each individual change. These systems are envisioned to design solutions, write code across the entire codebase, perform testing and debugging, and even proactively improve code. The output is intended to be production-ready, although a final human review might still be conducted as a precautionary measure. This level of autonomy is comparable to trusting a senior engineer: providing requirements and expecting a complete solution for review [User Query]. These agents incorporate advanced planning capabilities, long-term memory of the project, and sophisticated integration of various tools. Level 4 signifies a shift from supervised autonomy to near-complete operational independence for complex, end-to-end tasks, with the human role transitioning from that of a supervisor to a client or final approver. Google's Jules, for instance, is described as an "autonomous agent that reads your code, understands your intent, and gets to work" asynchronously in the cloud, managing tasks like "building new features" or "fixing bugs" and then presenting a diff of the changes 20; the developer assigns the task and reviews the outcome later. Cognition's Devin is marketed as "the first AI software engineer," designed for "planning and executing complex engineering tasks end-to-end," including building full-stack applications or autonomously fixing bugs.21 While its actual success rate on complex benchmarks like SWE-bench is approximately 13.9% 23, the intended capability aligns with Level 4. Experimental tools such as GPT-Engineer aim to "generate an entire codebase" from a single prompt, engaging in clarifying dialogue before implementation.24 This "one-shot" project generation embodies a high degree of autonomy. The key distinction from Level 3 is the reduced necessity for ongoing supervision and the expectation of a more complete, potentially production-ready deliverable derived from a high-level specification.</p>
<p>Level 5 – Full Autonomy (Experimental/Future):At this largely theoretical stage, an AI coding agent would be capable of setting its own goals and continuously improving software without requiring explicit human-defined tasks. Such an agent might analyze user needs, system metrics, or market trends to autonomously decide to implement new features, refactor existing code, or perform optimizations. As of 2025, this level remains speculative, and no current tools achieve this degree of autonomy, although research projects offer glimpses into this potential future [User Query]. Level 5 transcends mere task execution; it involves AI-driven goal formulation and proactive, continuous software evolution, which introduces substantial challenges concerning alignment, trust, and safety. An AI agent that "sets its own goals" [User Query] aligns with definitions where AI "independently chooses the best actions it needs to perform to achieve those goals" after humans set initial, very high-level objectives; however, Level 5 implies the AI might even define these initial strategic objectives based on broader system analysis.3 Research into "self-improving coding agents" like SICA, which can "autonomously edit itself, and thereby improve its performance" 26, represents a step in this direction. If an agent can improve its own codebase, it is a precursor to proactively improving application codebases. DeepMind's AlphaDev, which discovered novel, more efficient algorithms 28, and Meta AI's research on LLM Compilers for code optimization 31 and the autonomous optimization of Agentic AI systems 33, also touch upon aspects of proactive improvement. However, these systems still operate on predefined benchmarks or human-defined problems. The challenges are immense: establishing "trust, safety, and alignment" [User Query] becomes paramount if an AI is to autonomously alter production systems based on its own derived goals.35 Sourcegraph, for example, currently views AI as "not about building a fully autonomous code-writing A.I.," emphasizing human-AI collaboration instead.37</p>
<p>The following sections will detail popular developer AI tools, grouped by these autonomy levels.Level 1 – Basic Code Completion AssistantsLevel 1 tools provide intelligent in-line code suggestions to speed up typing and reduce trivial errors. They typically complete a single word, line, or simple code block based on the current context (file content, syntax, and maybe local context from open files). The developer must still drive all logic and make all decisions – the AI just predicts likely code to follow, much like an advanced autocomplete. These tools don’t “understand” higher-level intent; they react to immediate context.Characteristics: Limited scope suggestions (often one line at a time), no long-range planning, no ability to perform actions beyond text insertion. They use either static analysis or trained models on code patterns. The interaction is passive: as you type, they offer completions you can accept or ignore.Tools in this category include basic IDE autocompletion and early ML-powered completers:</p>
<p>Visual Studio Code IntelliSense (Level 1):</p>
<p>Autonomy: Level 1 (baseline code assist).
Key Characteristics: Visual Studio Code (VS Code) IntelliSense offers a suite of code editing features, including code completion, parameter information, quick info, and member lists. This functionality is provided out-of-the-box for languages such as JavaScript, TypeScript, JSON, HTML, CSS, SCSS, and Less.7 For other programming languages, VS Code supports word-based completions by default, but this can be significantly enhanced with richer IntelliSense capabilities by installing specific language extensions.7 These language services deliver intelligent code completions grounded in language semantics and a thorough analysis of the developer's source code.7 Consequently, IntelliSense offers various types of completions, encompassing suggestions from language servers, predefined snippets, and straightforward word-based textual completions.7 It is designed to predict variables, function names, and general code syntax as the developer types, leveraging static typing information and the immediate code context within the current file or, for JavaScript projects, a broader project context if a jsconfig.json file is present.38
Mode of Interaction: Suggestions typically appear automatically in an editor dropdown menu as the developer types. They can also be manually triggered using the Ctrl+Space keyboard shortcut or by typing a language-specific trigger character, such as the dot (.) character in JavaScript.7 Developers can accept these suggestions by pressing Enter or Tab.7
Use Cases: The primary use case is to accelerate the typing of known identifiers and boilerplate code. This includes suggesting object property names, completing function signatures, and leveraging JSDoc annotations to provide richer IntelliSense for JavaScript code.38 IntelliSense can also display quick information about symbols and parameter information for method calls.7
Pros: IntelliSense is generally very fast and highly reliable for known APIs and symbols within the current project's context.7 Its core functionalities, based on local analysis, operate offline, which is a significant advantage for uninterrupted development. Furthermore, these local operations alleviate privacy concerns often associated with cloud-based AI tools [User Query]. As described, it “predicts keywords, variable names, and suggests basic syntax” to accelerate coding [User Query].
Cons: IntelliSense does not possess true "intelligence" in the sense of generating novel code or understanding complex, higher-level developer intent beyond the immediate syntactic and semantic context [User Query]. Its suggestions are predicated on static analysis and recognized patterns within the existing project or language specifications, rather than on deep learning from a vast external corpus of code, unless it is augmented by other, more advanced AI tools [User Query]. Consequently, it cannot perform multi-line insertions beyond simple predefined patterns or snippets.7
VS Code IntelliSense, while a Level 1 tool, establishes a crucial baseline for developer assistance in modern Integrated Development Environments (IDEs). Its effectiveness is often enhanced by specific project configurations (like jsconfig.json for JavaScript projects 38) and language-specific extensions, underscoring the principle that even foundational assistance benefits from richer contextual information. The modular design of IntelliSense, relying on extensions for enhanced support in many languages 7, allows for a layered approach where more sophisticated AI capabilities can be integrated. Many Level 2 and higher AI tools, such as GitHub Copilot and Amazon CodeWhisperer, are delivered as VS Code extensions 10, building upon the foundational editing and completion environment provided by the IDE and its IntelliSense.</p>
<p>Tabnine (Level 1, with emerging Level 3 aspects):</p>
<p>Autonomy: Primarily Level 1 for its core code completion functionality. However, Tabnine's introduction of "AI Agents" signals a strategic move towards capabilities that align with Level 3 autonomy for specific tasks.
Key Characteristics: Tabnine is an AI-driven code completion tool that significantly extends basic autocomplete functionalities through the use of machine learning [User Query]. Its underlying models predict code at the token or line level and are capable of completing entire snippets or even function bodies based on the surrounding context.8 Tabnine offers broad language support, covering over 80 programming languages, and provides flexibility in its deployment with both local and cloud-based AI models. A strong emphasis is placed on privacy; proprietary models are never trained on user code, and user code is not stored or shared without explicit permission.8 Furthermore, Tabnine allows for the creation of team-trained models, which learn from an organization's shared codebase to provide more personalized and relevant suggestions.10 The "Tabnine 2 Protected model" is specifically trained only on permissively licensed code to mitigate intellectual property concerns.8
Mode: Tabnine is typically installed as an IDE plugin, delivering on-the-fly suggestions for the current and subsequent lines of code as the developer types.10 In addition to code completion, it also offers an AI-powered chat feature that utilizes leading Large Language Models (LLMs) for Q&amp;A and explanations.9
Use Cases: Tabnine is used to accelerate coding by providing more powerful completions than basic IntelliSense, such as for idiomatic code patterns, suggesting function arguments, and generating common code constructs from minimal typed input [User Query]. Beyond completion, its "AI agents" can perform a range of tasks including documentation generation, code explanation, test generation, and automated code fixing.8
Pros: Tabnine facilitates faster coding through "accurate and personalized code completions for snippets, whole lines, and even full functions".9 It enhances the relevancy of its suggestions by learning from the user's codebase and optionally from global code data.8 The tool is compatible with a wide array of IDEs and offers flexible deployment options (on-premises, Virtual Private Cloud, or SaaS) to cater to various privacy requirements.8 The suite of AI agents for tasks like documentation, code review, and test generation extends its utility beyond simple code completion.8
Cons: The core code completion functionality remains largely reactive (Level 1), meaning it does not autonomously handle multi-step tasks or engage in deep reasoning without the explicit invocation of specific agent functionalities. The quality of suggestions can vary if the contextual information is ambiguous [User Query]. Running large AI models locally can be memory-intensive [User Query]. The free version of Tabnine offers a more limited set of features compared to its paid tiers.10
Tabnine's evolution exemplifies a broader trend where tools initially centered on Level 1 or Level 2 capabilities are progressively incorporating Level 3 agentic functions. This blurs the strict demarcations between autonomy levels within a single product offering. While Tabnine's core strength remains its advanced code completion 9, the introduction of "AI agents" like the "Code Review Agent," "Jira Implementation and Validation Agents," "Testing Agent," and "Code Fix Agent" 8 clearly indicates a move towards more autonomous, goal-oriented behaviors characteristic of Level 3. For instance, an agent that can "autonomously generate fixes" or create "comprehensive test plans" operates at a higher level of autonomy than mere code suggestion. This strategic direction suggests an ambition to provide a more holistic AI development platform, building upon a successful completion tool by adding features with greater autonomy. Consequently, users need to be cognizant of which specific Tabnine feature they are employing to accurately understand its operational autonomy level.</p>
<p>Other Level 1 Examples:</p>
<p>JetBrains IDEs Autocompletion: Integrated Development Environments from JetBrains, such as IntelliJ IDEA, feature sophisticated context-sensitive code completion. This system suggests names for classes, methods, fields, and keywords by analyzing the current context to propose reachable choices, including live templates.40 Basic completion can be triggered manually (e.g., via Ctrl+Space) or can appear automatically as the developer types.40 A notable feature is "Smart Completion" (Type-Matching Completion), which intelligently filters the suggestion list to display only types that are applicable to the current context.40 These IDEs also support statement completion, which helps create syntactically correct code constructs, and even offer ML-assisted completion ranking that prioritizes suggestions based on the collective choices of other users in similar coding situations.40</p>
<p>Pros: The autocompletion is highly integrated into the IDE, deeply context-aware, and supports a variety of completion types (basic, smart, statement). The ML-ranking feature further refines suggestion relevance over time.40
Cons: These features are primarily rule-based and context-driven within the scope of the current project. They do not generate novel logic beyond established patterns or undertake multi-step tasks.40</p>
<p>Visual Studio IntelliCode: This tool extends Visual Studio's standard IntelliSense by incorporating AI to enhance code suggestions. It provides context-aware code autocompletions, which notably include whole-line completions for C# developers.42 IntelliCode's suggestions are distinguished by a star icon and are prioritized at the top of the completion list, as it predicts the most likely correct API based on the current code context and patterns learned from thousands of high-quality open-source projects.42 It also offers argument completion, suggesting the most probable argument names for method calls.42</p>
<p>Pros: AI-assisted suggestions are often more contextually relevant than simple alphabetical listings. Whole-line completions can significantly speed up coding in C#. IntelliCode can also track repetitive coding actions and offer suggestions to automate them (currently C#-only).42
Cons: Some of its advanced features may still be in preview. Whole-line completion is primarily available for C#. The reliance on patterns from public code repositories might mean that suggestions do not always align perfectly with the specific conventions or needs of private codebases.42
The common characteristic among these advanced Level 1 tools is the application of machine learning to augment traditional static analysis techniques. This makes their suggestions more "intelligent" by considering broader patterns, either learned locally from the user's project or from extensive open-source data. However, they fundamentally remain assistive tools that complete or suggest code based on what the developer has initiated, without independently generating complex logic or undertaking multi-step actions, which are the defining features of higher autonomy levels.</p>
<p>Level 2 – Advanced Code Generation (AI Pair Programmers)Level 2 tools function as AI pair programmers, capable of generating more substantial segments of code in response to prompts or contextual cues, thereby moving beyond the single-line completions typical of Level 1. At this stage, the AI can infer the developer's intent from comments or function signatures and produce code designed to fulfill that intent. Nevertheless, the AI's role remains reactive and confined to single-step operations; it responds to a human prompt with a discrete block of code. The developer continues to drive the overall process, providing instructions for each task, and the AI's output is intended for review and potential modification by the human.Characteristics: These tools exhibit context-aware generation of larger code blocks, such as entire functions or classes, as well as explanations of code. They are frequently powered by large language models (LLMs) extensively trained on code repositories (e.g., OpenAI's Codex family, Code Llama, Anthropic's Claude). While they may integrate into IDEs or operate via chat interfaces, they do not autonomously execute the code they generate or make further changes beyond their immediate response. The typical interaction involves the developer writing a comment or posing a question, to which the AI responds with a code suggestion or an answer.Representative tools at Level 2 include:</p>
<p>GitHub Copilot (Level 2 - Standard Mode):</p>
<p>Autonomy: Level 2. (It is important to note that GitHub Copilot also offers an "Agent Mode," which operates at Level 3 and is discussed in a subsequent section).
Key Characteristics: GitHub Copilot functions as an “AI pair programmer,” seamlessly integrated into popular code editors such as Visual Studio Code and JetBrains IDEs.6 It leverages OpenAI's advanced models (historically Codex, and now more sophisticated GPT versions 6) to generate a wide range of code suggestions, from single lines to complete functions.10 Copilot is adept at inferring developer intent from various contextual cues, including function names, comments, or the surrounding code.6 It offers support for a multitude of programming languages.10
Mode: The primary mode of interaction is through inline completion suggestions that appear as the developer writes code. Additionally, Copilot features "Copilot Chat," an interactive chat interface that allows developers to ask coding-related questions, obtain explanations of code segments, generate unit tests, and receive suggestions for fixes or refactoring. This chat functionality is available within IDEs and directly on the GitHub.com website.10
Use Cases: Common use cases include generating boilerplate code, implementing standard algorithms, and suggesting API calls based on descriptive comments (e.g., a comment like “// function to sort an array of numbers” can prompt Copilot to generate the corresponding sorting function).46 Copilot Chat can be used to explain existing code, offer refactoring ideas, and generate tests.44
Pros: Copilot's seamless IDE integration often makes it feel like an expert assistant is readily available to help.6 It significantly accelerates the process of writing routine code and boilerplate, with some reports indicating productivity increases of up to 40% for experienced developers.6 Its extensive knowledge base, covering numerous frameworks and languages, often saves developers the time they would otherwise spend consulting documentation.10 The Copilot Chat feature further enhances its utility by providing an interactive platform for problem-solving and code explanation.10
Cons: A critical consideration is that the code generated by Copilot can sometimes be inaccurate, insecure, or suboptimal, necessitating careful review and thorough testing by the developer.46 Its understanding of the broader project context is limited, primarily focusing on the currently open files and some related content, which can lead to inconsistencies or non-compiling code if not guided effectively.46 There is also a risk, albeit mitigated by filters, that Copilot might suggest outdated coding patterns or, in rare instances, snippets that resemble publicly available code [User Query]. Furthermore, its reliance on cloud services means it requires a stable internet connection and a subscription (though it is offered free of charge to verified students, teachers, and maintainers of popular open-source projects).10
The success of GitHub Copilot in its standard pair programmer mode has been pivotal in driving broader developer adoption of AI tools and has effectively established a baseline expectation for what an AI pair programmer can offer. It significantly offloads the task of writing common code patterns. However, its inherent limitations, such as a finite context window and the potential for inaccuracies, underscore the indispensable role of human review and validation. This reliance on the developer for verification, integration, and overall architectural direction firmly places its standard operational mode at Level 2. Copilot's core function is to "suggest whole lines or entire functions" [User Query] in a reactive manner to developer actions. The consistent emphasis in both user reviews and official documentation on the necessity of human oversight due to potential errors or security vulnerabilities 6 reinforces its role as an assistant rather than an autonomous decision-maker. The interaction model is single-shot: Copilot provides a suggestion and then awaits the developer's next action—be it acceptance, rejection, modification, or a new prompt.</p>
<p>Amazon CodeWhisperer (Level 2):</p>
<p>Autonomy: Level 2.
Key Characteristics: Amazon CodeWhisperer is AWS’s AI-powered coding companion, designed to provide real-time code recommendations based on the developer's existing code and natural language comments.11 Its suggestions can range from single lines of code to fully formed functions.11 A key distinguishing feature of CodeWhisperer is its deep optimization for Amazon Web Services (AWS). For instance, if a developer writes a comment such as “upload file to S3”, CodeWhisperer is likely to generate the appropriate code snippet utilizing the AWS SDK.11 In addition to code generation, it also offers security scanning capabilities to identify potential vulnerabilities in the code.11 CodeWhisperer supports a variety of programming languages including Python, Java, JavaScript, C#, Go, Rust, PHP, Kotlin, SQL, and Infrastructure as Code (IaC) languages like CloudFormation (JSON/YAML), Terraform (HCL), and AWS CDK (TypeScript, Python).2
Mode: CodeWhisperer is delivered as an IDE plugin, for example, through the AWS Toolkit for popular editors like VS Code and JetBrains IDEs.39 Code suggestions typically appear as the developer types or can be explicitly invoked after writing a comment. Users can navigate through the offered suggestions using arrow keys and accept a chosen suggestion, often with the Tab key.11
Use Cases: It excels in scenarios involving cloud-related code, particularly for AWS operations such as creating S3 buckets, defining DynamoDB tables, or handling AWS authentication.11 It is also used for general code generation in its supported languages, for example, generating a function to convert JSON to CSV based on a descriptive comment [User Query].
Pros: CodeWhisperer can generate “entire functions and logical code blocks at a time” [User Query]. Its strong understanding of AWS APIs means it often automatically includes necessary import statements and SDK calls for AWS-specific tasks.11 The integrated security scanning feature is a valuable addition for detecting potential vulnerabilities early in the development process.11 It is also available free of charge for individual developers.47
Cons: As with other Level 2 tools, the output from CodeWhisperer requires careful review, as it may be incorrect or suboptimal.47 Its pronounced AWS bias makes it less effective for projects that are not hosted on or interacting with the AWS ecosystem.47 Some users have found its suggestions to be more basic or conservative compared to those from competitors, and it initially supported a narrower range of languages, although this support is expanding.47 Utilizing CodeWhisperer requires an AWS account, and it may transmit code context to AWS servers, which can raise privacy and licensing considerations similar to other cloud-based AI tools [User Query]. It's noteworthy that as of May 2025, CodeWhisperer's features are being integrated into the broader Amazon Q Developer service 11, which encompasses Level 3 capabilities discussed later.
CodeWhisperer's specialization in AWS APIs provides a clear advantage for developers working within that particular ecosystem. This focus demonstrates a strategic approach for Level 2 tools: tailoring AI assistance to specific domains to offer more contextually relevant and immediately useful suggestions, even if their general-purpose coding capabilities might be perceived as less broad than some competitors. The ability to generate code for specific AWS SDK calls directly from natural language comments, such as "upload file to S3" [User Query], is a significant productivity enhancer for AWS developers, reducing their reliance on extensive documentation for common tasks. This domain-specific knowledge is a primary differentiator. However, this strength inherently leads to the limitation of an AWS-centric bias.47 Like GitHub Copilot, CodeWhisperer operates on a prompt-and-generate model, where the developer is responsible for validating the output. The ongoing integration of CodeWhisperer into Amazon Q Developer 11 signals Amazon's strategic intent to build more comprehensive, and potentially higher-autonomy, AI assistants for developers.</p>
<p>OpenAI Codex (API) / Successor GPT Models (Level 2 for direct API use):</p>
<p>Autonomy: Level 2, when these models are accessed directly via an API for single-turn code generation tasks.
Key Characteristics: OpenAI Codex was the foundational AI model, a descendant of GPT-3, renowned for its ability to “translates natural language into code”.12 It was the technology that powered the initial versions of GitHub Copilot.12 Although the original Codex API was deprecated as of March 2023 12, its core capabilities have been subsumed and significantly enhanced by newer and more powerful models in the GPT series, such as GPT-4 and particularly GPT-4.1. These advanced models are accessible through OpenAI's API and integrated into tools like ChatGPT.12 They can generate code, provide explanations, assist in debugging, and offer support for a multitude of programming languages, with Python being a notable area of strength.12 For instance, GPT-4.1 demonstrates major improvements in coding proficiency and instruction-following, with an updated knowledge cutoff of June 2024.13 Furthermore, a new AI agent named "Codex" (powered by the codex-1 model), available within ChatGPT for certain users, is a cloud-based software engineering assistant capable of writing features, answering questions about a codebase, fixing bugs, and proposing pull requests, all while operating within isolated cloud sandbox environments.48 However, the fundamental interaction of using the API for straightforward code generation remains a Level 2 activity.
Mode: When used as an API or via a playground interface, the developer provides a prompt—which could be a problem description, a piece of partial code, or a natural language instruction—and the model returns a corresponding code segment.12 For example, a prompt like “Create a Python function to check if a number is prime” would elicit the Python code for that function [User Query]. The newer "Codex" agent within ChatGPT is accessed through the application's sidebar, where users can assign coding tasks via prompts.48
Use Cases: Typical applications include single-step generation of code from detailed specifications (e.g., "compute the moving average of an array for a given window size" 12), answering "how-to" coding questions by providing illustrative code examples, and assisting in script writing by allowing developers to describe the desired outcome in natural language.12 These models are also used for explaining existing code and translating code between different programming languages. The new "Codex" agent can be tasked with more complex objectives like "writing features" or "fixing bugs".48
Pros: These models possess exceptionally powerful language understanding and code generation capabilities across a wide array of programming languages.12 Newer iterations like GPT-4.1 exhibit substantial improvements in coding benchmarks (such as SWE-bench) and in their ability to follow instructions precisely.13 They can generate not only functional code but also accompanying explanations. The new "Codex" agent has the ability to read and edit files, and run commands such as tests and linters.48
Cons: Direct utilization of the API necessitates skill in prompt engineering and often requires iterative refinement of prompts by the developer to achieve desired results [User Query]. The generated outputs invariably require human review for correctness, security, and optimality. All LLMs have context length limitations, which can impede their ability to grasp the nuances of very large codebases unless the context is carefully managed, although these limits are continually increasing (e.g., GPT-4.1 supports up to 1 million tokens 13). The original Codex API is no longer available; access to these capabilities is now through newer, often more general-purpose models or via specific product integrations like ChatGPT.12 API usage also incurs costs.
The trajectory from the original Codex to the more potent GPT-4 series and the introduction of the agentic "Codex" within ChatGPT illustrates the rapid advancement in foundational models tailored for code generation. While the fundamental interaction for a single code generation task via API remains at Level 2, the escalating capabilities—such as file editing and command execution seen in the new "Codex" agent—signal a clear push towards more agentic behaviors. The original Codex was a prime example of Level 2 functionality: translating natural language into code in a single pass [User Query]. Its deprecation and replacement by more advanced GPT models 12 reflect the swift pace of LLM development. The new "Codex" agent 48, with its abilities to "read and edit files, as well as run commands including test harnesses, linters, and type checkers" 48, approaches Level 3 capabilities for that specific implementation. However, if a developer merely uses the OpenAI API to send a prompt like "write a Python function for X" and receives code in return, that interaction itself is a single-step, human-reviewed generation, characteristic of Level 2. The underlying model may be capable of more complex operations, but the specific usage pattern determines the realized level of autonomy in that instance.</p>
<p>Replit Ghostwriter (Level 2):</p>
<p>Autonomy: Level 2.
Key Characteristics: Replit Ghostwriter is the AI developer tool integrated into Replit's online Integrated Development Environment (IDE).49 It offers a suite of AI-powered features including code completions, chat-based assistance via "Ghostwriter Chat," code transformation capabilities, and code explanation functionalities.49 Ghostwriter can suggest single lines of code or entire blocks based on comments or the surrounding code context.49 While optimized for JavaScript and Python, it also supports over 14 other programming languages.51 A key feature of Ghostwriter Chat is its awareness of the project's file context.52
Mode: Ghostwriter provides inline autocomplete suggestions ("Complete Code") and features a chat sidebar ("Ghostwriter Chat") directly within Replit's browser-based IDE or its desktop application.50 Notably, Ghostwriter Chat can proactively offer suggested solutions when errors are detected in the console output.52
Use Cases: It is frequently used for rapidly generating code while working on projects within the Replit environment, such as autocompleting a function in a Python project [User Query]. Developers can ask Ghostwriter Chat questions like "How do I implement a linked list in Java?" and receive code examples and explanations [User Query]. The proactive debugger in the chat can assist in identifying and fixing errors 52, and the tool can also be used to explain existing code snippets.50
Pros: The tight integration with the Replit platform allows for immediate execution and testing of generated code.50 Ghostwriter supports multi-line and function completions, and can refactor or fix code when prompted.52 It serves as a valuable tool for learning from examples and accelerating the development of small projects.50 The project context awareness of Ghostwriter Chat leads to more relevant answers compared to non-integrated AI chat tools.52 The proactive debugger is a unique and beneficial feature.52
Cons: Ghostwriter's full feature set is primarily available within the Replit ecosystem, although a VS Code extension is reportedly in preview.51 The quality of its suggestions is comparable to other LLM-powered tools, meaning it can produce incorrect or non-optimal code that requires careful review [User Query]. Access to the advanced AI features of Ghostwriter is typically not included in Replit's free tier and necessitates a paid subscription, such as Replit Core.49 As a cloud-based tool, it is dependent on an internet connection and may not be ideal for very large codebases due to potential cloud-related limitations.51
Replit Ghostwriter's strength at Level 2 stems from its deep integration within a specific development platform—the Replit online IDE. This provides a cohesive AI-assisted coding experience tailored for that environment, which is particularly beneficial for educational purposes and rapid prototyping. Ghostwriter's core functionality aligns with Level 2: it "suggest[s] single lines or entire blocks of code based on comments or context" 49 and offers chat-based assistance for Q&amp;A and explanations 52, with the developer initiating these interactions. The "proactive debugger" 52, while an interesting feature that initiates contact, is still reacting to a specific event (an error) and offering a single-step solution for the developer to apply; it does not autonomously iterate or manage complex multi-step debugging scenarios without further interaction. The tight coupling with Replit's environment 50 is both a strength, due to ease of use and instant execution, and a limitation, due to platform lock-in.</p>
<p>(Level 2 tools provide significant assistance, but the human developer must prompt each action and integrate the results. They do not autonomously refactor code or verify its correctness beyond what they generate in a single operation. The subsequent section explores tools that exhibit truly agentic behavior, capable of taking initiative in multi-step tasks.)Level 3 – Context-Aware Coding Agents (Supervised Automation)Level 3 tools represent a significant advancement in AI-assisted development, where the AI transitions into an agent capable of autonomously performing multiple steps to accomplish a developer-defined goal, all while under high-level human supervision. Unlike Level 2 tools that output a single code snippet and halt, a Level 3 coding agent can plan a sequence of actions. This may involve reading and modifying several files, compiling or running the project, observing the outcomes (such as test failures), and iteratively adjusting its code to meet the objective. Crucially, at this level, the human is no longer required to prompt every single code modification. Instead, the developer provides one high-level instruction, and the agent undertakes numerous sub-tasks. However, the human typically remains "in the loop," at least to review the final changes or to provide clarification if the agent encounters ambiguity during its process. This operational model is analogous to a capable junior developer who can be tasked with, for example, "Please add this feature," and will then work through the implementation, ask questions if necessary, and finally present a code update for review.Characteristics: These tools are distinguished by their ability to maintain state and memory related to the codebase they are working on. They frequently integrate with the developer's environment (IDE or CLI) to directly apply code edits, execute tests, interact with version control systems, and perform other development-related commands. They possess components for both planning (deconstructing complex tasks into manageable steps) and execution (writing code, running commands). The interaction style is often conversational, where the developer instructs the agent via chat or command, and the agent may respond with questions or confirmations. Alternatively, interaction can be declarative, where the developer issues a "task command," and the agent executes it, reporting back upon completion. The developer's role evolves into that of a product manager or reviewer, focusing on specifying goals and constraints rather than micro-managing implementation details.A growing number of cutting-edge AI developer tools available in 2024–2025 fall into this Level 3 category:</p>
<p>GitHub Copilot (Agent Mode) (Level 3):</p>
<p>Autonomy: Level 3. (It should be noted that "Copilot X" was an earlier umbrella term for GitHub's next-generation AI features; "Agent Mode" and "Copilot coding agent" are the more current and specific terms for these advanced, autonomous capabilities 14).
Key Characteristics: GitHub Copilot's Agent Mode, now available in VS Code Stable after a preview period 14, functions as an "autonomous peer programmer".14 It is designed to execute multi-step coding tasks based on a developer's command. This includes analyzing the existing codebase, identifying and reading relevant files, proposing edits across multiple files, and running terminal commands such as compiling code, installing necessary packages, and executing tests. A key aspect of its operation is its ability to monitor the correctness of its actions, respond to compile errors or test failures, and auto-correct its approach in an iterative loop until the assigned task is successfully completed.14 This agent can be tasked with creating applications from scratch, performing complex multi-file refactorings, writing and running tests, and migrating legacy code to modern frameworks.14 The agent autonomously determines the relevant context and files required for an edit.14 Complementing this, GitHub also offers a "Copilot coding agent" (currently in public preview) which can be assigned a GitHub issue directly. This agent will then work autonomously to implement the required changes and subsequently create a pull request for human review.44
Mode: Interaction is typically initiated via the Copilot Chat interface within the IDE (e.g., VS Code) or by assigning a GitHub issue to the dedicated coding agent.14 The developer provides a high-level instruction, such as "Add a new API endpoint for user profile," or assigns an issue. The agent then autonomously plans and executes the necessary changes, potentially seeking permission for significant steps or iterating its solution based on test failures or other feedback.14
Use Cases: Agent Mode is well-suited for multi-file refactoring, implementing features that span multiple files or layers of an application stack (e.g., creating new applications, migrating legacy code), fixing bugs that involve changes in several code locations, generating documentation, and integrating new libraries.14 An example includes modernizing legacy Python scripts by transforming basic matplotlib histograms into sophisticated, SVG-based animated line charts with minimal guidance from the developer.53
Pros: This mode offers a significant productivity boost for complex or repetitive multi-step tasks by automating much of the "inner development flow".14 It capably handles multi-file edits, command execution, and iterative error remediation.14 Developers are kept in control through an iterative UI and requirements for approval before terminal commands are executed.14 The agent can be guided with explicit context references (e.g., using #file) and custom instructions to adhere to specific coding guidelines.14 Transparency is provided through the UI display of tool invocations.14
Cons: As a relatively new set of features (Agent Mode reached VS Code Stable in early 2025, and the issue-solving Coding Agent is still in public preview 14), its stability and performance may continue to evolve. Its effectiveness can vary, particularly in very large or architecturally complex repositories.14 There is a possibility that it may misinterpret high-level requests or produce unexpected changes, necessitating careful review of the final outputs or pull requests [User Query]. The quality of its work is dependent on the underlying LLMs (references mention GPT-4o and Claude 3.7 for different Copilot features 53). Some reviewers have noted that, in its current state, it might not match the full autonomy of more specialized tools like Roo Code.54
The evolution of GitHub Copilot to include an Agent Mode represents a significant strategic move by a leading provider of Level 2 AI tools into the realm of higher-autonomy (Level 3) capabilities. This development directly addresses the inherent limitations of single-step code generation by incorporating sophisticated planning, tool utilization, and iterative execution. The dual approach—offering an interactive Agent Mode within the IDE alongside an asynchronous, issue-solving Coding Agent—caters to diverse developer workflow preferences. The description of Copilot Agent Mode explicitly details an operational "loop" that involves editing code, running commands and tests, monitoring the output, and remediating any issues identified.14 This iterative, multi-step process, driven by a high-level goal, is the hallmark of Level 3 autonomy. Furthermore, the capability of the "Coding Agent" to take a GitHub issue as input and autonomously work towards producing a pull request 44 is a clear example of supervised automation, where the AI manages the intermediate steps of a larger task. The consistent emphasis on keeping the developer "in control" 14 and the necessity of reviewing the agent's work align with the "supervised" nature of Level 3.</p>
<p>Cursor (AI Code Editor with Agent Mode) (Level 3):</p>
<p>Autonomy: Level 3.
Key Characteristics: Cursor is a comprehensive AI-powered code editor, developed as a fork of Visual Studio Code, and features deeply integrated AI capabilities, most notably its "Agent Mode".55 In Agent Mode, Cursor is designed to "handle complex coding tasks with minimal guidance." It achieves this by autonomously exploring the existing codebase, identifying relevant files, making necessary changes, and utilizing a suite of tools including search, file editing, and terminal command execution.15 A core feature supporting this is its ability to index the entire codebase, which involves computing embeddings for each file to enhance contextual understanding and provide more accurate answers and actions related to the codebase.15 Cursor supports various underlying Large Language Models, such as GPT-4 and Claude, allowing users some flexibility.55 It also includes an "auto command runner" and a feature referred to by users as "YOLO mode" (this is a user community term for a high-autonomy setting, not always an official term in documentation, but its functionality is described 61) where the agent can automatically execute terminal commands and attempt to fix errors based on the output, sometimes without requiring explicit approval for each step if this mode is active and configured.57
Mode: Developers interact with Cursor's AI capabilities directly within the editor environment. High-level instructions can be provided via a chat pane or through the command palette (e.g., instructing the agent to "refactor the authentication logic to use tokens instead of sessions").62 The Agent then autonomously plans and executes the required changes. It may ask for confirmation before performing critical actions, unless a more automatic mode like auto-run or "YOLO mode" is enabled.57
Use Cases: Cursor is well-suited for large-scale refactoring tasks (e.g., "Rename userId to userID everywhere" 56). It can implement features that touch multiple files or layers of an application (e.g., "Add a dark mode toggle to my React application" 15). It also excels at codebase-wide find-and-fix operations and can answer complex questions about the codebase by leveraging its indexed knowledge (using the "@Codebase" command 62). Developers can also generate new code from scratch using shortcuts like Ctrl+K.62 Debugging assistance is also a key use case, where pasting a stack trace can lead to targeted fixes.56
Pros: The deep integration of AI within a familiar VS Code-like IDE provides a natural and intuitive developer experience, with continued compatibility for most VS Code extensions.55 The powerful codebase indexing feature allows for global context understanding, which can overcome the token limits faced by many other AI tools when dealing with large projects.15 The Agent Mode can effectively handle complex multi-file operations and refactorings.15 A "Loops on Errors" feature attempts to iteratively fix bugs until tests pass, particularly when "YOLO mode" is active and a test suite is properly configured.55 The editor also includes a checkpoint system, allowing developers to revert changes made by the agent if necessary, which adds a layer of safety.15
Cons: As a standalone editor (albeit a VS Code fork), it might lack some niche customizations or plugins that developers have in their primary IDE setup, though VS Code compatibility is generally high [User Query]. The "YOLO mode" or fully automatic command execution carries inherent risks if the AI misinterprets the task, potentially leading to unintended or destructive changes; thus, it requires careful handling.57 The performance and quality of suggestions are dependent on the chosen underlying LLM, and API key costs can be a factor for users.55 Some users have reported occasional UI clutter and instances of inconsistent AI behavior, ranging from "brilliant to baffling".56 The agent can also "overreach" or produce undesirable results if given vague prompts.56 Advanced features may have a steeper learning curve.56
Cursor's strategic approach of forking a widely adopted IDE like VS Code and deeply embedding Level 3 agentic AI capabilities, particularly its full codebase indexing and autonomous tool use, represents a significant effort to create a truly "AI-native" development environment rather than merely offering a plugin. "Agent Mode" in Cursor is explicitly designed for "autonomous operation," "multi-step planning," and "full tool access," including terminal commands.15 The codebase indexing feature 58 is critical, as it provides the agent with a comprehensive understanding of the entire project's structure and dependencies.15 This deep context is essential for effectively performing complex, multi-file tasks and refactorings, which is often a challenge for simpler AI plugins. The described workflow—Understand Request → Explore → Plan → Execute → Verify → Task Complete 15—is a clear iterative, agentic loop. The "YOLO mode," as described by users 61, where the agent can "run tests automatically... iterating on the code until the tests pass," is a prime example of supervised automation where the AI handles the iterative fixing loop under high-level directives. This holistic approach aims to make AI a central, rather than peripheral, component of the coding workflow.</p>
<p>Windsurf (formerly Codeium) (Level 3):</p>
<p>Autonomy: Level 3.
Key Characteristics: Windsurf is marketed as an "AI-native IDE" and an "agentic code editor" 63, having evolved from Codeium's earlier autocomplete tool. Its flagship agentic feature is Cascade, an "AI Flow" engine. Cascade enables the AI to generate or modify code, then seek user approval before running it (often in an integrated terminal), observe the results, and subsequently prompt for follow-up actions or make further iterative changes based on the outcome.16 Windsurf is designed to understand the entire project context and can automatically make code changes, run and debug code, and retry operations upon failure.63 It supports advanced features such as image-to-code conversion, web search integration for contextual information, and integration with Model Context Protocol (MCP) servers for extended tool use.16
Mode: Windsurf operates as a custom IDE, which is VS Code-based, with AI capabilities built directly into it [User Query]. Developers primarily interact with the Cascade agent. This can be done in "Write Mode," which functions similarly to AutoGPT by creating files, running scripts, executing tests, and debugging, with user approval checkpoints throughout the process.16 Alternatively, "Chat Mode" provides contextual Q&amp;A, code generation, and instructions, requiring more manual execution by the user.16 Inline edits (triggered by Ctrl+I) and terminal chat are also supported interaction methods.16 A "Turbo mode" can be enabled to allow Cascade to auto-execute terminal commands.63
Use Cases: Windsurf is suited for complex debugging scenarios where the agent can run code, analyze stack traces, propose and apply fixes, and then re-run the code. It's also useful for test-driven development, as it can run tests and autonomously fix failing ones. Implementing features that require modifications across multiple parts of a project is another key use case [User Query]. Users have also reported using it for migrating projects (e.g., from React to Flutter), performing code reviews, and enhancing legacy projects.66
Pros: Windsurf offers powerful automation through Cascade's iterative plan-execute-debug loop. In "Write Mode," it is claimed to automate approximately 90% of the code generation and debugging process.16 The agent has real-time awareness of user actions within the IDE.65 It can automatically detect and fix linting errors that it generates.65 The platform supports a wide range of AI models and can incorporate external context from web pages and documentation.16 Features like "Tab to Jump" and "Supercomplete" provide an enhanced code completion experience.67
Cons: As a newer, all-in-one IDE, it requires users to switch from their existing development environments [User Query]. While the agent is designed to "think 10 steps ahead" 63, it can still produce errors or deviate from the intended path, necessitating human validation and correction.68 Some user reviews for Codeium (its predecessor) mentioned occasional unreliability or issues with customer support.68 A current limitation of Cascade is that its "reverts are currently irreversible" 65, which demands caution from the user. Some advanced features are restricted to paid or enterprise plans.65
Windsurf's Cascade feature, with its explicit workflow of "generate/modify code → seek approval → run code → observe results → iterate," clearly operationalizes the Level 3 agentic paradigm. This system emphasizes a structured, human-supervised process for tackling complex development tasks. The "Write Mode," which functions like AutoGPT to "create multiple files for you, run scripts, test them, and debug them" 16 with checkpoints for user approval, is highly agentic. Features such as "Lint Fixing," where Cascade "will automatically detect and fix lint errors that it generates" 63, further demonstrate its multi-step problem-solving capabilities. This structured approach, while powerful, also highlights the "supervised" aspect of Level 3, as human approval often serves as a key checkpoint, distinguishing it from the more hands-off nature of Level 4 autonomy.</p>
<p>Anthropic Claude Code (Level 3):</p>
<p>Autonomy: Level 3.
Key Characteristics: Claude Code is an agentic coding tool developed by Anthropic. It typically runs in the terminal and leverages Anthropic's Claude models (such as Claude Opus 4 and Sonnet 4), which are known for their large context windows and strong conversational reasoning abilities.19 Key capabilities include understanding the entire codebase (autonomously exploring files as needed without requiring manual file feeding to the context), editing multiple files, fixing bugs, answering complex questions about code architecture and logic, executing tests and linters, searching through Git history, handling merge conflicts, and generating commit messages and pull requests.19 Claude Code also offers native integrations with popular IDEs like VS Code and JetBrains, where it can display proposed edits directly within the developer's files.69
Mode: The primary interaction mode is via a command-line interface (CLI), initiated with the claude command, which starts an interactive Read-Eval-Print Loop (REPL) within a project directory.19 Users interact with Claude Code conversationally, providing natural language instructions (e.g., "Claude, refactor the user authentication logic to use OIDC"). The agent may ask clarifying questions before proposing changes, often presented as diffs in the terminal, which the user can then choose to apply [User Query]. It can utilize a variety of "tools" such as FileEditTool for modifying files, BashTool for executing shell commands, and GrepTool for searching file contents.19
Use Cases: Claude Code is employed for complex codebase refactoring, fixing bugs that span multiple files, automatically generating and running tests, resolving merge conflicts, and answering in-depth questions about code (e.g., "Why is this function recursive?"). It is also useful for onboarding new developers to a project by explaining the existing code structure, for instance, by generating a CLAUDE.md project guide.19
Pros: Claude Code offers a very broad range of capabilities, effectively acting like an expert pair programmer that not only suggests but also applies and verifies changes [User Query]. It benefits from the strong natural language understanding and large context window of the Claude models, enabling better comprehension of complex codebases.68 The agent excels at explaining its reasoning step-by-step, which many users find helpful.72 Operating directly in the terminal allows it to fit seamlessly into existing CLI-centric workflows.19 It is designed with security and privacy as priorities, running locally and requiring explicit user approval before making any modifications to the system.19 An SDK is available for building custom agents.69
Cons: The CLI-first interface might present a learning curve for developers who are more accustomed to graphical user interfaces [User Query]. Claude Code is currently in beta as a research preview, which means access might be limited, and its features may continue to evolve.19 It utilizes paid API calls to the Claude models, which can become costly with extensive use.19 Even with a large context window, extremely large or complex projects might still pose challenges or require careful scoping of prompts [User Query]. Some user reviews for the general Claude models have noted occasional over-cautiousness or a tendency to miss context if queries are not worded with sufficient precision.72
Claude Code's strength lies in its combination of a powerful Large Language Model with direct agency within a developer's local environment. It is particularly effective for tasks that involve extensive codebase understanding and intricate Git operations. Its command-line interface (CLI) nature appeals to a specific segment of developers who prefer terminal-centric workflows and value the control and scriptability such an environment offers. The ability to "edit files and fixing bugs across your codebase," "executing and fixing tests," and "searching through git history, resolving merge conflicts, and creating commits and PRs" 19—all based on high-level natural language instruction—clearly positions Claude Code at Level 3. Its typical workflow, often described as "Explore, plan, code, commit" 70, is inherently multi-step and agentic. The consistent requirement for explicit user approval before any modifications are made 70 ensures that the automation remains supervised, a key characteristic of this autonomy level.</p>
<p>ChatGPT (with Plugins and Advanced Data Analysis/Code Interpreter) (Level 3):</p>
<p>Autonomy: Level 3, when specifically utilized with its Code Interpreter (now often referred to as Advanced Data Analysis) feature or with relevant plugins that enable multi-step, tool-using workflows for coding tasks.
Key Characteristics: OpenAI’s ChatGPT, particularly versions powered by GPT-4 and newer models like GPT-4.1 74, is a highly capable conversational AI. Its utility for coding is significantly amplified by:</p>
<p>Code Interpreter/Advanced Data Analysis: This feature provides ChatGPT with a sandboxed Python execution environment. Within this sandbox, ChatGPT can write and run Python code, upload and download files, perform data analysis, create visualizations, and iteratively debug its own code based on execution errors.74
Plugins (and increasingly, custom GPTs/connectors): These extensions allow ChatGPT to interact with a wide array of external tools and services. For coding, this can include plugins for web browsing (e.g., WebPilot 77) to find solutions or documentation, or integrations for interacting with version control systems like GitHub or GitLab (e.g., via Zapier or dedicated connectors 78).</p>
<p>Mode: Interaction occurs through a conversational interface, typically a web UI or via API. When using the Code Interpreter, users can upload files and instruct ChatGPT to perform analysis or generate and execute code. When using plugins, users first enable the specific plugins they need and then prompt ChatGPT to utilize them for a given task (e.g., "Search the web for a Python library to parse XML, then write a sample script using it and test it with this example XML data").
Use Cases: Performing data analysis and generating visualizations using Code Interpreter (e.g., analyzing data from CSV files, creating charts, converting file formats).75 Prototyping and testing Python code snippets within the sandboxed environment, allowing for iterative refinement.75 Utilizing web browsing plugins to find current documentation, solutions to coding problems, or information about new libraries.77 Interacting with GitHub or GitLab via plugins or connectors for tasks such as summarizing pull requests, creating issues, or potentially (with sufficient permissions and robust plugin capabilities) committing code.78
Pros: ChatGPT offers enormous flexibility due to its broad general knowledge and the extensibility provided by plugins and the Code Interpreter feature.81 It possesses strong natural language understanding and reasoning capabilities, especially with models like GPT-4 and GPT-4.1.74 The Code Interpreter enables a powerful iterative loop of code generation, execution, and debugging directly within the chat interface.75 Plugins allow access to real-time information from the web and integration with various external services, helping to overcome the knowledge cutoff limitations of the base LLM.77
Cons: ChatGPT is not a dedicated IDE tool, so integrating its outputs into a development workflow often involves manual copy-pasting of code, unless specific plugins or integrations can bridge this gap [User Query]. The reliability and capabilities of third-party plugins can vary significantly, and managing multiple plugins for a complex task can be cumbersome [User Query]. Context length limitations, although substantial for models like GPT-4, can still be a constraint when attempting to reason about an entire large codebase [User Query]. Access to advanced models (like GPT-4), Code Interpreter, and plugins typically requires a ChatGPT Plus subscription.82 There are also security and trust considerations when granting plugins permission to execute actions or access external data [User Query]. Responses from ChatGPT can sometimes be verbose or require further refinement to be directly usable [47 (a general con of ChatGPT)].
ChatGPT, when augmented with its Code Interpreter and relevant plugins, transforms from a Level 2 code generator into a versatile Level 3 agent for specific types of development tasks. This transformation is primarily due to its acquired ability to perform actions, use tools, and iterate based on execution results or newly retrieved information. The Code Interpreter's capacity to "write code, run it in a Python environment, and return the results," and critically, "in case of code failure...debugs the code by reading the callback messages...[entering] an automatic loop to fix errors" 75, is a clear demonstration of a Level 3 iterative execution loop. When this is combined with plugins that allow it to, for instance, "interact with your Git repositories directly" 78, it can undertake multi-step, tool-assisted tasks based on a high-level developer request. The developer supervises this entire process through the conversational chat interface, guiding the AI and validating its outputs or actions. This highlights the significant role of "tool use" in elevating the autonomy and practical capabilities of LLMs.</p>
<p>Aider (CLI Chat &amp; Git Integration) (Level 3):</p>
<p>Autonomy: Level 3.
Key Characteristics: Aider is an open-source AI pair programming tool specifically designed for use in the terminal, and it features deep integration with Git repositories.17 It employs various Large Language Models (LLMs)—such as OpenAI's GPT-4/GPT-4o, Anthropic's Claude 3.7 Sonnet, DeepSeek R1, or even locally hosted models—to understand and act upon natural language instructions to edit code within a local Git repository.17 Key distinguishing features include its capability for multi-file editing based on a single prompt, mapping the entire codebase to provide better context for its operations in larger projects, automatically committing changes with sensible, AI-generated messages, and an automated system for running linters and tests after each change, with an ability to attempt auto-fixing any reported issues.17 It also supports adding images and web pages to the chat to provide visual context for coding tasks.17
Mode: Developers typically run the aider command in their terminal from within a Git repository and then interact with it via a chat interface [User Query]. Instructions such as "Implement a function for X feature in these files" will prompt Aider to modify the specified files. It will then usually present a diff of the proposed changes for the developer's approval before committing them.18 Aider can also be configured to "watch" files and respond to specially formatted AI comments added by the developer in their preferred IDE.17
Use Cases: Aider is well-suited for rapid development and iteration on existing codebases. Common tasks include adding new features that span multiple files, fixing bugs that have been described in natural language, and refactoring code across various parts of a project (e.g., "Clean up all API routes to use the new authentication middleware").18 It can also be used for generating documentation for existing code or explaining complex code segments.83
Pros: Aider offers strong automation capabilities, particularly due to its excellent Git integration, which includes auto-commits with meaningful messages. This provides a clear audit trail of AI-generated changes and allows for easy rollbacks if needed.17 The automated linting and testing cycle, coupled with its auto-fix attempts, acts as a valuable safety net for AI-driven modifications.17 The ability to use both cloud-based and local LLMs gives users flexibility regarding cost and data privacy.17 Being open-source, Aider is extensible and benefits from community contributions.18 It is generally considered lightweight and fast in its operation.18 Many users and reviewers have praised its effectiveness for multi-file changes.84
Cons: Its primary interface is command-line based, which may not appeal to all developers, particularly those less comfortable with terminal environments, and it is generally not considered beginner-friendly.18 Initial setup requires configuring API keys for the chosen LLMs and may involve some tuning of models or parameters.18 The effectiveness of Aider's suggestions and modifications is dependent on the clarity of the instructions provided and the capabilities of the selected LLM [User Query]. Furthermore, the success of its auto-fixing feature is contingent on the comprehensiveness and quality of the project's existing linters and test suites [User Query].
Aider's deep integration with Git and its "commit-centric" workflow for managing AI-generated code changes establish a pragmatic approach to Level 3 autonomy. This methodology emphasizes traceability and provides developers with robust control via familiar tools like Git diffs and commits. The automated testing loop further enhances safety. Aider's capability to "edit multiple files at once through a single AI prompt" 18, "automatically commits changes with sensible commit messages," and "automatically lint and test your code every time aider makes changes...[and] fix problems detected" 17 clearly demonstrates a multi-step, iterative workflow that operates under developer supervision (through the approval of changes). This operational model extends well beyond the single-shot generation characteristic of Level 2 tools. The tight coupling with established developer practices like version control and automated testing likely fosters greater trust and facilitates adoption, especially for a CLI-based agent, as the AI's actions are rendered transparent and manageable within existing, well-understood workflows.</p>
<p>Amazon Q Developer CLI (formerly incorporating CodeWhisperer features) (Level 3):</p>
<p>Autonomy: Level 3.
Key Characteristics: The Amazon Q Developer CLI is an enhanced command-line interface agent that represents an evolution from earlier CodeWhisperer CLI functionalities, now incorporating more sophisticated agentic features.85 It is designed to provide a dynamic and interactive coding experience directly within the terminal. This agent leverages powerful foundation models, such as Anthropic's Claude 3.7 Sonnet, accessed via Amazon Bedrock.85 The Q CLI agent is capable of using information from the developer's CLI environment to read and write files locally, query AWS resources, write new code, assist with testing and debugging, and iteratively make adjustments to code based on user feedback and explicit approval.85 A significant aspect of its capability is its ability to make use of tools already installed on the user's system, including compilers, package managers, and the AWS Command Line Interface (AWS CLI).86
Mode: Interaction occurs within an interactive CLI session. Developers engage with Amazon Q Developer using natural language commands and queries (e.g., "Create a new Lambda function in this project that processes S3 events").86 Based on these instructions, the agent can execute necessary shell commands, install dependencies, modify code files, and even deploy resources to AWS. Throughout this process, it may ask for clarification or provide updates on its progress.86
Use Cases: The Q CLI agent is particularly well-suited for cloud development tasks, such as scaffolding new projects, setting up AWS resources (e.g., finding an existing DynamoDB table, installing the relevant SDK using npm, and then updating application files to utilize that table 86), and managing builds or deployments. It can also handle general local development tasks, like initiating a build process, then running automated tests, parsing the test output, and subsequently attempting to fix any reported errors in an agentic loop [User Query].
Pros: This tool helps reduce context switching by enabling developers to remain within their terminal environment while leveraging AI assistance.86 It is designed to complete more tasks autonomously, moving beyond simply providing hints, by employing step-by-step reasoning.85 It features deep integration with AWS services and related development tools.86 The use of powerful underlying models like Claude contributes to its strong natural language understanding and contextual awareness.85
Cons: As a relatively newer tool, its full range of capabilities and robustness across all scenarios are still being established. Early information suggested some OS limitations (e.g., macOS or Linux only for certain Q CLI features, though this is subject to change) [User Query]. The ability to execute commands automatically on the developer's system inherently raises trust and safety considerations, requiring users to exercise caution [User Query]. It primarily benefits developers working within the AWS ecosystem [User Query]. Setup may require an AWS account and specific configurations (e.g., AWS Builder ID) [User Query]. Some user reviews for the broader Amazon Q Developer service (which includes IDE extensions in addition to the CLI) have mentioned instances of misunderstood prompts or the generation of buggy code, and the pricing for Pro tiers can be a concern for some users.87
The Amazon Q Developer CLI's ability to leverage existing tools on the developer's local system—such as compilers, package managers, and the AWS CLI—through natural language commands represents a potent Level 3 agentic pattern. This effectively transforms the AI into an intelligent orchestrator of the developer's own established toolkit. The agent "can make use of tools installed on my system including compilers, package managers, and the AWS CLI".86 This implies that developers do not need to wait for Amazon to build specific, bespoke integrations for every conceivable build system or utility; the AI can utilize what is already present and configured in their environment. For instance, if a developer instructs it to "build the project and run tests," the Q CLI agent can invoke the project's specific make command or npm test script, parse the output generated by these local tools, and then attempt to fix errors based on that output, all by orchestrating the local environment. This "bring your own tools" approach significantly expands the agent's practical capabilities and its adaptability to diverse development environments, which is a key characteristic of effective Level 3 automation.</p>
<p>Other Level 3 Examples:</p>
<p>Cline (Roo): Cline is an open-source autonomous coding assistant designed as a VS Code extension. It uniquely features distinct "Plan" and "Act" modes, allowing the agent to first devise a strategic plan (a sequence of steps to implement a request) and then execute those steps, modifying code accordingly. Cline can read the entire project, search within files, and perform terminal commands. It connects to various LLMs via API, offering flexibility.84 The tool emphasizes a "thoughtful collaborator" approach, not only performing actions but also explaining its steps and suggesting best practices. A significant feature is its integration of the Model Context Protocol (MCP), which enables it to interact with external tools and services, such as running tests, managing Git operations, updating documentation, and even connecting with project management tools.88 This broadens its scope beyond mere code editing to encompass a wider range of development lifecycle tasks. The explicit separation of "Plan" and "Act" modes allows for developer review of the proposed strategy before execution, fitting the "supervised automation" paradigm of Level 3. MCP integration, in particular, significantly enhances its agentic capabilities by allowing it to orchestrate diverse, multi-step tasks.88
QodoAI (formerly CodiumAI): QodoAI positions itself as a quality-first AI coding platform. Its agentic capabilities are primarily focused on testing and code analysis. Key features include "Qodo Gen," an IDE plugin for generating high-quality code and meaningful tests, and "Qodo Merge," a Git agent designed to facilitate smoother pull requests by generating thorough descriptions and providing reviewers with comprehensive walkthroughs to quickly identify potential issues.89 The platform operates on the principle of "Agentic AI for Continuous Quality," aiming to ensure code integrity throughout the development pipeline by automating code reviews and generating comprehensive test suites.89 Users report that it can generate detailed unit tests, sometimes identifying edge cases that human developers might have overlooked.90 QodoAI applies Level 3 agentic principles specifically to the domain of code quality and testing, effectively acting as an autonomous QA assistant that can analyze code, generate appropriate tests, and facilitate the review process. Its functions, such as automating test generation based on code analysis and creating detailed PR descriptions 89, are multi-step processes that assist the developer at a higher level than simple code generation.</p>
<p>Level 4 – Highly Autonomous Coding AgentsLevel 4 tools represent a significant leap towards fully autonomous coding on complex tasks. These agents typically require only minimal human input, often just an initial prompt or a specification document, and are designed to operate with potentially no human review required before deployment (though in current practice, a final review is still common and advisable). Agents at this level can take on a project goal, such as “build me an application that performs X,” or address a very complex bug, and are expected to deliver a completed solution with little to no incremental guidance from the developer. They achieve this through advanced planning capabilities, long-term memory of the project's context, and the ability to integrate and utilize various external tools and information sources. While Level 3 agents still operate under fairly close human supervision for their iterative cycles, Level 4 agents aspire to produce production-ready code that could theoretically be trusted after automated validation processes.As of 2025, Level 4 autonomy is at the cutting edge of AI development, with only a few emerging examples, often proprietary or in limited beta stages, that approach this level of capability. These agents aim to function like seasoned software developers who can be given a broad problem statement and will autonomously clarify requirements (to some extent), write the necessary code across the entire codebase, thoroughly test their work, debug any issues, and ultimately hand over a finished product.Key players and experimental systems in this space include:</p>
<p>Google Jules (Coding Agent) (Level 4):</p>
<p>Autonomy: Level 4.
Key Characteristics: Jules is Google's autonomous coding agent, which entered public beta in May 2025. It is explicitly positioned by Google as "not a co-pilot, not a code-completion sidekick, but an autonomous agent that reads your code, understands your intent, and gets to work".20 Jules operates asynchronously in a secure Google Cloud Virtual Machine (VM). It begins by cloning the target codebase into this VM, allowing it to understand the full project context. It can then perform a range of tasks such as writing tests, building new features, fixing bugs, and managing dependency version updates.20 Jules is powered by Google's Gemini 2.5 Pro model, providing it with advanced coding and reasoning capabilities.20 Upon task completion, Jules presents its plan, the reasoning behind its actions, and a diff of the changes made. It also allows for user steerability, meaning developers can modify the proposed plan before, during, and after execution.20 Unique features include direct GitHub integration for a seamless workflow and "audio changelogs" that provide narrated summaries of recent commits.20
Mode: Jules is accessed via a web interface (Google Labs) or through the Gemini application, and it integrates directly with GitHub.20 Developers assign tasks (e.g., "Jules, add pagination to the blog posts list"). Jules then works on these tasks in the background, eventually returning the results, often in the form of a pull request on GitHub.20
Use Cases: Jules is designed for implementing large features, addressing major bug fixes, writing extensive suites of unit tests, and autonomously upgrading library versions while also resolving any resulting compatibility issues.20 It can also be used to tackle a backlog of bugs or to take an initial pass at building out new features.92
Pros: The agent is highly hands-off, allowing developers to delegate significant coding tasks and focus on other priorities while Jules works autonomously.20 It leverages the powerful Gemini 2.5 Pro model, ensuring access to advanced coding and reasoning capabilities.20 Its integration with GitHub fits naturally into existing developer workflows.20 The asynchronous mode of operation is particularly well-suited for time-consuming tasks.91 Jules provides a visible workflow by presenting its plans and reasoning, and the user steerability feature offers a degree of control over the process.20 During its public beta, a free tier with usage limits is available.92
Cons: As a very new tool (public beta launched May 2025) 20, its real-world performance across a diverse range of complex scenarios is still being evaluated. Its cloud-only operation, while offering security assurances from Google (private code is not used for training and data stays isolated 20), might still raise security or privacy concerns for some organizations and inherently introduces latency [User Query]. The asynchronous nature means there is no real-time interaction during task execution, and developers must wait for Jules to complete its work [User Query]. The high degree of autonomy also means that Jules might make architectural or structural decisions that a human developer would not, necessitating careful review of its pull requests, even if the code is functionally correct [User Query]. Initial language support includes Python and JavaScript, with plans for expansion.92
Jules represents a significant endeavor by a major technology company to realize Level 4 autonomy in coding. Its asynchronous, cloud-based execution model, coupled with its objective to handle entire features or complex bug fixes autonomously, clearly targets this advanced level of operation. It aims to function like a delegated, independent engineering resource. The description of Jules as an "autonomous agent that reads your code, understands your intent, and gets to work" on tasks like "building new features" or "fixing bugs" with "minimal further prompts" 20 aligns squarely with the definition of Level 4. Its workflow—assign task, Jules works independently in the cloud, Jules presents a pull request for review 20—is characteristic of delegating a substantial work package to an AI, with the expectation of a near-complete solution. The ability for users to "modify the presented plan before, during, and after execution" 20 provides a supervisory overlay on an otherwise highly autonomous process.</p>
<p>Cognition’s Devin (AI Software Engineer) (Level 4):</p>
<p>Autonomy: Level 4.
Key Characteristics: Devin, developed by Cognition Labs, is marketed as "the first AI software engineer" and is designed to autonomously perform complex software development tasks from end-to-end.21 It operates within a cloud-based sandboxed environment that is equipped with its own Linux shell, code editor, and web browser, enabling it to utilize a wide range of tools much like a human developer would.21 Devin exhibits advanced capabilities in long-term reasoning and planning, capable of breaking down complex tasks into thousands of discrete decisions and steps. It also features context recall, the ability to learn from feedback over time, and mechanisms for mistake correction.21 The release of Devin 2.0 in April 2025 introduced several enterprise-focused enhancements, including interactive planning (allowing users to review, edit, or approve Devin's proposed plan before execution), Devin Search (enabling users to query their codebase and receive responses with cited code), and Devin Wiki (which automatically generates browsable documentation and architecture diagrams for the codebase).22
Mode: Devin is offered as a service, and users typically interact with it by assigning tasks or GitHub issues via a web dashboard, Slack integration, or a command-line interface.22 Once a task is assigned, Devin works autonomously on the codebase. It reports its progress and can proactively ask for human feedback or design choices if it encounters ambiguity or requires a decision that impacts the project's direction.21
Use Cases: Devin has been demonstrated on ambitious tasks such as building and deploying end-to-end applications based on a natural language description (e.g., a demo showcased Devin creating an interactive "Game of Life" web application from scratch and then deploying it to Netlify).22 It is also used for fixing complex bugs in existing, mature open-source codebases; Devin reportedly solved 13.86% of issues on the SWE-bench benchmark unassisted, a significant improvement over previous state-of-the-art AI models.22 Other use cases include large-scale code migrations and refactoring (as exemplified by its use in the Nubank ETL migration project 21) and even training or fine-tuning machine learning models.22
Pros: Devin demonstrates a high degree of autonomy in planning and executing complex engineering tasks that may require thousands of decisions and maintaining long-term context.22 Its ability to use common developer tools (shell, browser, editor) allows for versatile and realistic task execution, such as reading online documentation or testing web applications.21 Its performance on the SWE-bench benchmark is notably superior to that of raw LLMs like GPT-4 or Claude 2 on the same tasks.22 The system is designed to learn over time and can fix its own mistakes during task execution.22 Features in Devin 2.0, such as interactive planning and Devin Search, enhance transparency and collaboration.22 Real-world enterprise tasks, like large-scale migrations, have seen significant efficiency gains (e.g., 12x improvement reported by Nubank) attributed to Devin.21
Cons: Despite the significant hype and impressive demos, Devin's real-world success rate on very complex or novel coding work is still limited. The 13.86% success rate on SWE-bench, while a notable achievement for an AI, is low compared to human engineer performance and indicates that it is not infallible; many tasks may require multiple attempts or human intervention.23 It can struggle with highly intricate algorithmic challenges or deep architectural design problems [User Query]. Devin is an expensive tool, with initial pricing plans targeting enterprise users (e.g., figures around $500/month were mentioned in early reports, though pricing models may evolve) [User Query]. Utilizing Devin requires granting a third-party cloud service access to the codebase, which raises standard concerns about security and compliance [User Query]. Some early demonstrations have faced scrutiny regarding potential pre-configuration or the exact conditions of the demo environment.22 The system is compute-intensive, which could lead to high operational costs and potentially affect pricing or access to its full capabilities if revenue growth does not keep pace.22
Devin's approach of equipping an LLM-based agent with a comprehensive suite of standard developer tools (shell, editor, browser) within a sandboxed environment is a pivotal architectural decision that underpins its Level 4 aspirations. This setup allows the AI to mimic human developer workflows more completely than agents with more restricted capabilities. Devin has "its own shell... its own code editor... and its own browser".21 This enables it to perform a wide range of actions crucial for real-world software engineering, such as "search the web for documentation, test web applications it builds, and access tools like Notion and Jira".21 The ability to "test its own code, fixing errors until it succeeds" 21 within this integrated environment is fundamental to its autonomous problem-solving loop. This operational model contrasts sharply with agents that are purely API-based or possess limited toolsets, as Devin can interact with the digital world in a manner much closer to that of a human developer to gather information, execute tasks, and validate its work. Its core design is to "autonomously complete software engineering tasks when prompted by a user" 22, encompassing planning, coding, testing, and debugging across multiple files. This aligns with the Level 4 description of handling complex projects end-to-end with minimal human input. The SWE-bench result of 13.86% 23, while far from perfect, represents a significant achievement for unassisted task completion on real-world issues, substantially surpassing previous AI capabilities. The "interactive planning" feature introduced in Devin 2.0 22 still fits within the Level 4 paradigm, as it involves upfront approval of a high-level plan rather than micromanagement of each subsequent step by the AI.</p>
<p>Experimental “One-Shot” Coding Agents (Level 4, experimental):</p>
<p>GPT-Engineer:</p>
<p>Autonomy: Approaches Level 4 in its ambitious goal of "one-shot" codebase generation.
Key Characteristics: GPT-Engineer is an open-source command-line interface (CLI) tool designed with the premise that a single, well-crafted prompt can be used to generate an entire codebase.24 The process typically involves the tool engaging in an interactive clarification dialogue with the user to refine the initial specifications. Once the requirements are sufficiently clear, GPT-Engineer autonomously generates the necessary code files to construct the desired software.24 Its core function is to convert natural language instructions directly into functioning code.24
Mode: The user initiates the process by providing an initial prompt, usually in a dedicated file (e.g., main_prompt). GPT-Engineer is then run from the CLI. Before proceeding with code generation, the tool may ask the user a series of clarifying questions to resolve ambiguities or gather more details. After this clarification phase, it generates the specified software, creating the necessary files and directory structure within a designated workspace.25
Use Cases: GPT-Engineer is primarily used for rapidly bootstrapping entire new projects, especially web applications, from a high-level conceptual idea (e.g., "A website that shows random cat pictures and quotes").25 It can also be employed to generate boilerplate code for common programming tasks or to create basic application structures.25
Pros: It has the potential to significantly speed up the initial phases of project setup and the generation of boilerplate code.24 Its interface can be user-friendly for both non-developers looking to quickly prototype ideas and developers seeking to accelerate initial groundwork.24 Being open-source, it is freely available and customizable, allowing users to experiment with different underlying language models.24
Cons: The quality and completeness of the generated codebase are highly dependent on the clarity and precision of the initial prompt; misinterpretation of complex or ambiguous instructions is a significant risk.24 The code generated by GPT-Engineer often requires substantial review, debugging, and manual refinement to be considered production-ready, especially for applications of moderate to high complexity.96 Some descriptions suggest its primary focus is on web application generation, which may limit its utility for other types of software development.24 It generally lacks the sophisticated iterative feedback and correction loops seen in more advanced agents during the generation process itself; its operation is more akin to a "one-shot" generation after an initial clarification phase [User Query].</p>
<p>AutoGPT-based dev agents:</p>
<p>Autonomy: Approaches Level 4 in its capacity for self-directed task execution.
Key Characteristics: AutoGPT is an experimental, open-source Python program that leverages Large Language Models (like GPT-4) to autonomously achieve user-defined goals. It does this by deconstructing the main goal into a series of smaller, manageable sub-tasks, planning their execution, and utilizing a range of tools—such as internet access for information gathering, file system operations, and code execution—in an automatic loop that does not require continuous human prompting.98 While AutoGPT is a general-purpose autonomous agent framework, it can be specifically tasked with software development goals.98
Mode: The user defines an AI agent by providing it with a name, a role, and a set of high-level goals. Once initiated, AutoGPT autonomously generates its own internal prompts and devises a plan to fulfill these goals. This may involve writing scripts, executing them, debugging any errors encountered, and iterating on this process.99
Use Cases (Coding): AutoGPT can be used experimentally to attempt the construction of simple applications or scripts. For example, if tasked to "create a Python script to collect weather data," it might autonomously plan to create the script, then attempt to fix any errors encountered during execution, and finally add comments to the working code.99 It can also be used to automate specific, well-defined parts of a development workflow if the goal is clearly articulated and achievable with its available tools.
Pros: AutoGPT demonstrates a high degree of autonomy in its ability to independently plan and execute a sequence of diverse actions (such as web searches, file input/output, and code execution) in pursuit of a defined objective.98 It can operate for extended periods without direct human intervention. Its capability to access and utilize external information via web browsing is a significant asset.101
Cons: AutoGPT is often inefficient and can easily get stuck in unproductive loops or pursue incorrect paths if not carefully guided by a very precise initial goal [User Query, 100 (noting high cost per step and limited functions)]. Setting up and running AutoGPT effectively requires a degree of technical skill, including obtaining and configuring an OpenAI API key and operating it via the command line. The cost of operation can also be high due to the large number of LLM calls it typically makes.99 The quality of the code generated can be low, and it may struggle with complex logic or tasks that are not easily broken down into discrete, verifiable steps or that require nuanced understanding beyond its available tools [User Query]. It is fundamentally a general-purpose autonomous agent framework rather than a specialized coding tool, which means it may lack a refined understanding of software development best practices or complex architectural considerations.
Experimental Level 4 agents like GPT-Engineer and AutoGPT highlight the community's strong aspiration for "one-prompt-to-product" solutions or fully autonomous task completion in software development. However, their current limitations vividly underscore the immense challenge in achieving consistently reliable, high-quality output without significant human oversight or more sophisticated built-in mechanisms for iterative refinement and verification. GPT-Engineer's goal to "generate entire websites or apps from a single prompt" 95 clearly reflects a Level 4 ambition. Similarly, AutoGPT aims to "autonomously achieve user-defined goals" by creating its own plans and prompts.98 Despite these aims, practical experience often shows that GPT-Engineer's output "might not be usable as is" and typically requires substantial review 96, with its success heavily reliant on the clarity of the initial prompt.24 AutoGPT, too, "requires some technical skills to set up and use properly" and is prone to getting stuck or becoming costly.99 This demonstrates that while the aspiration for Level 4 capabilities is evident in these tools, their current execution often falls short of producing consistently production-ready results for complex tasks without considerable human intervention, thus keeping them largely in the experimental realm for such applications.</p>
<p>Level 5 – Full Autonomy and Future OutlookLevel 5 autonomy in AI coding tools represents the aspirational endgame: a state where AI agents can manage the entire software development lifecycle independently, extending to the point of autonomously deciding what software needs to be built or which existing systems require modification or improvement, without explicit human requests for each task. This is analogous to a fully self-driving car that not only navigates any environment but also determines its own destinations based on higher-level objectives. In the context of software, a Level 5 AI developer might proactively identify emerging user needs, anticipate system bottlenecks, or respond to security advisories by initiating and completing development projects. As of 2025, this level of autonomy remains largely theoretical and experimental; no current tools can reliably set their own strategic development goals in a controlled and beneficial manner.Characteristics and Theoretical Nature:Level 5 AI agents would transcend the capabilities of Level 4 by not only executing complex tasks autonomously but also initiating these tasks based on their own sophisticated analysis and understanding of various inputs, such as system performance metrics, user feedback trends, evolving market conditions, or newly discovered security vulnerabilities. This implies a capacity for proactive monitoring, opportunity identification, goal formulation, strategic planning, and complete execution of development initiatives. Achieving this would require profound understanding across multiple domains—technical, user experience, business strategy—and the ability to make high-stakes decisions with long-term implications. The concept mirrors that of Level 5 autonomous vehicles, which are described as "currently theoretical for widespread use".5Glimpses in Research &amp; Related Concepts:While no true Level 5 systems exist, several research avenues provide foundational elements and hint at future possibilities:</p>
<p>Self-Improving Coding Agents:The research paper "A Self-Improving Coding Agent (SICA)" (arXiv:2504.15228) presents an agent system capable of autonomously editing its own codebase to enhance its performance on defined benchmarks, such as SWE-Bench Verified, where it improved its score from 17% to 53% on a subset of tasks.26 SICA achieves this through an iterative "Meta Agent Loop," where it analyzes its past performance, identifies potential improvements (e.g., to its prompting schemes or tool usage), and implements these changes to its own Python code.26 This capacity for self-modification is a crucial precursor for any system that could autonomously improve application codebases. Similarly, Anthropic's research with its Claude models, including Claude Code, has explored agents that can "extract and save key facts to maintain continuity and build tacit knowledge over time".69 The availability of the Claude Code SDK for building custom agents 69 also facilitates experimentation with more autonomous behaviors. SICA's ability to modify its own codebase and improve benchmark scores is a concrete step towards self-improving software, a core concept for Level 5. However, it is important to note that SICA is still improving based on predefined benchmarks and human-defined goals; it does not yet set entirely new strategic goals for itself. The agent improves its efficiency and effectiveness at solving given tasks, which is a form of self-optimization. Level 5 implies a higher order of goal-setting, such as deciding which benchmarks to target or what new features would be most valuable for users, capabilities that SICA does not address.</p>
<p>AI for Novel Algorithm Discovery &amp; Code Optimization:DeepMind's AlphaDev project demonstrated an AI system that discovered novel, more efficient sorting algorithms (achieving up to 70% speedup for certain C++ library use cases for short sequences and 1.7% for large ones) and hashing algorithms (up to 30% more efficient for specific byte ranges) by exploring assembly instructions from first principles, in some cases outperforming algorithms designed by humans over decades.28 These AI-discovered algorithms have since been integrated into standard C++ libraries. This showcases AI's potential not merely to write code according to specifications but to innovate at a fundamental algorithmic level, a form of proactive improvement characteristic of Level 5. AlphaDev was not just refactoring existing code; it was inventing new algorithmic approaches. This represents a higher level of "improvement" than simply fixing bugs. However, it operated on a very specific, human-defined goal (i.e., find a faster sorting/hashing algorithm). A true Level 5 system might autonomously decide which algorithms within a larger system require optimization based on broader system performance goals or energy efficiency targets.Concurrently, Meta AI's research into LLM Compilers—using specialized variants of Code Llama pre-trained on assembly codes and compiler Intermediate Representations (IRs)—aims to optimize code and streamline compilation tasks. This includes automatically tuning compiler flags for code size or disassembling compiled code back to LLVM-IR.31 Such research points towards AI systems that could autonomously improve the performance and efficiency of software at a low level, potentially as a continuous background process, aligning with the "continuous improvement" aspect of Level 5.</p>
<p>Multi-Agent Systems &amp; Collaborative AI:Active research into LLM-based Multi-Agent Systems (MASs) is exploring how groups of AI agents can coordinate their actions and collectively solve complex problems.102 Frameworks such as MetaGPT (which mimics a software company structure with role-assigned agents) and AutoGen (which enables sophisticated conversations and collaboration between agents) are investigating how specialized AI agents could collectively manage various aspects of the software development lifecycle.103 Even general-purpose agent frameworks like AutoGPT can be structured to allow for a form of agent collaboration if tasks are designed appropriately.104 This suggests that future Level 5 systems might not be monolithic AIs but rather a collaborative ensemble of specialized AI agents that collectively manage the software lifecycle, potentially even negotiating tasks and strategic goals among themselves. Complex software development often involves diverse roles (architect, coder, tester, DevOps engineer). A Level 5 system might mirror this human team structure with specialized AI agents. The ongoing research in MASs 102 is focused on enabling such collaboration, which could be essential for handling the breadth and complexity of tasks envisioned at Level 5.</p>
<p>Challenges: Trust, Safety, Alignment, and Control:The path to Level 5 autonomy is fraught with significant challenges:
Trust and Reliability: Entrusting an AI with the autonomy to modify and deploy software systems without direct human oversight for each action necessitates an exceptionally high degree of trust in its reliability and correctness. The current performance of even emerging Level 4 tools (e.g., Devin's ~14% success rate on the SWE-bench benchmark 23) indicates that the field is still far from achieving this level of dependability.
Safety and Security: An AI agent possessing full autonomy over a codebase could inadvertently introduce critical bugs, security vulnerabilities, or cause system-wide instabilities. Ensuring the safety of such highly autonomous systems is a monumental challenge.36 As noted in safety research, for systems operating above certain risk or capability thresholds, "the burden of demonstrating such safety guarantees should be on the systems' developers".36
Alignment: Ensuring that an AI agent's self-determined goals remain consistently aligned with human values, ethical considerations, and overarching organizational objectives is crucial and notoriously difficult [User Query]. An AI optimizing for a poorly defined or incomplete metric could lead to highly undesirable or even harmful outcomes. The concept of recursive self-improvement, a potential capability of Level 5 systems, could lead to rapid, unpredictable advancements that surpass human comprehension or control if not impeccably aligned from the outset.34
Interpretability and Debuggability: Understanding why a Level 5 AI decided to make a particular architectural change or pursue a specific development goal would be essential for oversight and trust, yet this could be exceedingly difficult with highly complex, potentially "black box" AI systems.
Ethical Considerations: The prospect of Level 5 autonomy raises profound ethical questions regarding accountability for AI-driven decisions, the potential for large-scale job displacement within the software industry, and the risks associated with the misuse of such powerful autonomous systems.
Future Outlook &amp; Industry Perspectives:While true Level 5 autonomy is not on the immediate horizon, the rapid pace of innovation, particularly the progression from Level 2 to emerging Level 4 tools observed between 2022 and 2025, suggests that AI capabilities in software development will continue to advance significantly. Industry experts predict that AI will increasingly handle more complex tasks. For instance, Outsystems envisions AI generating entire applications, not just code, and anticipates that human-AI interactions will become increasingly multi-modal.106 A survey by IBM and Morning Consult indicated that 99% of developers were exploring or actively developing AI agents in 2025, although experts debate whether current systems described as "agents" possess true autonomy or are more accurately characterized as sophisticated orchestration frameworks.107Sourcegraph, a company developing advanced context-aware AI tools like Cody, currently emphasizes AI's role as an assistant designed to automate "mundane tasks" rather than as a fully autonomous replacement for human programmers.37 Their focus is on "context-aware AI coding assistance" that leverages a deep understanding of the entire codebase.The immediate future will likely see continued improvements in Level 3 and Level 4 tools, making them more reliable and capable of handling complex, developer-defined tasks with decreasing levels of supervision. True Level 5 autonomy, where an AI might act as a "project lead" by setting its own strategic software development goals, remains a longer-term research and development challenge. The evolutionary path towards this ultimate level of autonomy will likely be incremental, perhaps first focusing on "practical applications of geo-fenced Level 4 autonomy" before Level 5 becomes a widespread reality, mirroring the cautious and staged progression observed in the development of autonomous vehicles.4The journey towards Level 5 autonomy is not merely an extrapolation of current code generation capabilities. It necessitates fundamental breakthroughs in several areas, including AI-driven goal formulation, the capacity for self-improvement on complex real-world software systems (not just on its own codebase or isolated benchmarks), and the development of robust, verifiable safety and alignment mechanisms. Current leading-edge agents (Level 3/4) still operate based on goals defined by humans (e.g., "fix this bug," "implement this feature"). Level 5 requires the AI to define these strategic goals independently. While research like SICA 26 demonstrates self-improvement on an agent's own codebase, applying this capability to improve diverse, evolving enterprise applications is a vastly more complex challenge. Similarly, AlphaDev's success 28 lies in discovering specific, albeit fundamental, algorithms; a Level 5 system would need to apply such innovative capabilities broadly and strategically across entire software ecosystems. The significant concerns surrounding safety, trust, and alignment 35 are not just engineering hurdles but profound research questions that must be satisfactorily addressed before Level 5 autonomy can be responsibly achieved. Simply making LLMs incrementally better at coding will not suffice.Consequently, the evolution towards Level 5 will likely involve a hybrid approach. In this model, AI agents might take on increasingly larger and more complex sub-systems or entire lifecycle stages autonomously, but they would operate within a framework that is still ultimately governed, validated, and strategically directed by humans, at least for the foreseeable future. The current progression shows AI tools taking on more sequential steps (Level 2 to Level 3) and larger, more integrated tasks (Level 3 to Level 4). Even optimistic projections for AI agents in 2025 see them primarily as augmenting human developers or automating specific parts of the workflow, rather than achieving complete replacement.107 The concept of "AI generating applications" 106 or AI autonomously handling complex "migrations" 21 suggests AI tackling entire work packages. However, the strategic decision to build that application or perform that migration still rests with human stakeholders. Therefore, a practical path towards something resembling Level 5 might involve highly autonomous AI "teams" responsible for specific software modules or services, but operating under human-defined strategic directives and rigorous quality assurance gates.Comparison Table of Developer AI Tools by Autonomy and CapabilitiesThe following table provides a summarized comparison of the developer AI tools discussed in this report, categorized by their primary autonomy level and highlighting their key characteristics, interaction styles, strengths, and limitations. This table serves as a quick reference to differentiate the capabilities and intended use cases of various tools in the rapidly evolving landscape of AI-assisted software development. Understanding these distinctions is crucial for developers and engineering leaders seeking to effectively integrate AI into their workflows.
Tool NameAutonomy LevelInteraction StyleKey StrengthsNotable LimitationsVS Code IntelliSense1 (Basic Assist)Inline IDE suggestions as you typeFast, context-aware completions for syntax &amp; names based on language semantics and source analysis 7; works offline; reliable for known code patterns; extensible.No true "AI" generation of novel logic; limited to current file/project context; cannot understand higher-level intent [User Query].Tabnine1 (with L3 agent features)IDE plugin, ML-based autocomplete; AI chat; AI agents for specific tasks 8ML-driven suggestions for full lines/functions; supports many languages; local models for privacy 8; team-trained models 10; agents for docs, review, Jira tasks.8Core completion reactive; agent features newer; quality varies with context; large local models can be resource-intensive.10GitHub Copilot (Standard Mode)2 (Pair Programmer)IDE plugin (inline suggestions); Copilot Chat interface 10Generates larger code blocks/functions from comments/context 10; broad framework knowledge; seamless IDE integration; chat for Q&amp;A/explanations.10Can produce incorrect/insecure code requiring review 6; limited full-project context; cloud-dependent; privacy/licensing concerns [User Query].Amazon CodeWhisperer2 (Pair Programmer)IDE plugin (inline suggestions) 11Optimized for AWS services and APIs 11; generates multi-line completions from comments; security scans 11; free for individuals.47 (Becoming part of Amazon Q Developer 11)Strong AWS bias, less effective for non-AWS work 47; suggestions can be basic; output needs review; requires AWS account [User Query].OpenAI Codex (API) / GPT Models2 (Pair Programmer)API/Playground; via ChatGPT (prompt-response); new "Codex" agent in ChatGPT 12Powerful natural language to code generation (esp. GPT-4.1 13); many languages; new "Codex" agent can edit files, run commands.48Original Codex API deprecated 12; API use requires prompt engineering, output needs validation; context limits; advanced access often paid [User Query].Replit Ghostwriter2 (Pair Programmer)Replit IDE integration (inline autocomplete &amp; Ghostwriter Chat) 49Tight Replit integration for instant execution/testing; multi-line/function completion, refactoring, explanation, proactive debugger 50; good for learning/prototyping.Primarily Replit-locked 51; quality similar to other LLM tools (needs review); advanced AI features are paid 49; internet-dependent.51GitHub Copilot (Agent Mode)3 (Agentic – supervised)IDE chat/command (VS Code); GitHub issue assignment 14Plans &amp; executes multi-file edits, runs tests, iterates on errors autonomously based on high-level goals/issues 14; creates PRs 44; developer kept in loop.Newer feature (stable in VS Code 14); can misinterpret complex requests, output needs review; performance varies with repo complexity.14Cursor (AI Editor)3 (Agentic – supervised)Full AI-based code editor (VS Code fork) with chat &amp; agent modes 15Deep codebase indexing for global context 15; multi-file ops, refactoring; Agent Mode explores, plans, executes with tools 15; "YOLO mode" for auto-execution (user term for auto-run) 57; checkpoints for safety.15Standalone editor (though high VS Code compatibility 56); auto-run/YOLO mode is risky 57; performance/cost depends on chosen LLM API keys.55Windsurf (Cascade Agent)3 (Agentic – supervised)AI-driven IDE (VS Code-based) with "Cascade" agent (Write/Chat modes) 63Cascade agent modifies code, runs it, debugs, iterates until success (Write Mode) 16; uses tools (search, terminal, web); auto-fixes lint errors 63; "Turbo mode" for auto-execution.63Newer tool; auto-execution ("Turbo mode") risky; must use its IDE; can make errors requiring human fix 68; some features paid/enterprise.65Anthropic Claude Code3 (Agentic – supervised)Terminal-based chat agent; IDE integrations (VS Code, JetBrains) 19Large context understanding; edits files (presents diffs), runs tests, Git ops (history, merge, commit/PR) 19; explains reasoning; SDK for custom agents.69CLI interface might intimidate some; currently in beta/research preview 19; uses paid API (costly for extensive use) 71; extremely large projects may still pose challenges [User Query].ChatGPT (with Plugins &amp; Advanced Data Analysis)3 (Agentic – supervised)Conversational AI (web UI or API) with Code Interpreter &amp; plugins (tool use) 75Highly flexible multi-tool agent: can fetch docs, run/debug Python code iteratively, use Git (via plugins) 75; strong natural language understanding (GPT-4).74Not an IDE tool by default (copy-paste workflow unless plugins bridge gap) [User Query]; plugin reliability varies; context limits; requires paid subscription for advanced features.82Aider (CLI + Git assistant)3 (Agentic – supervised)CLI chat tool working on local Git repo 17Excellent Git integration (auto-commits, diffs); auto-runs linters/tests &amp; attempts fixes 17; supports various LLMs (cloud/local) 17; open-source.18CLI-based (less visual, steeper curve) 18; requires API key setup; auto-fix quality depends on tests/linters; model errors possible [User Query].Amazon Q Developer CLI3 (Agentic – supervised)Interactive CLI agent (natural language to CLI commands &amp; code edits) 85AI orchestrates local CLI tools (compilers, package managers, AWS CLI) 86; strong for AWS/cloud tasks; uses powerful models (Claude via Bedrock) 85; iterative adjustments based on feedback.85Newer tool; executing commands automatically has inherent risks [User Query]; primarily benefits AWS users; requires AWS setup [User Query].Google Jules4 (High Autonomy)Asynchronous cloud-based agent; GitHub integration (task assignment &amp; PRs) 20Autonomous coding agent: takes high-level goals (features, bugs), works independently in cloud VM, presents PRs 20; uses Gemini 2.5 Pro; provides plan/reasoning; audio changelogs.20Very new (public beta May 2025) 20; cloud-only (code sent to Google VM); asynchronous (no real-time interaction); high autonomy needs careful review of results [User Query].Cognition Devin4 (High Autonomy)Cloud AI service (task assignment via dashboard/Slack/CLI) 22AI Software Engineer: plans &amp; executes complex end-to-end tasks using own shell, editor, browser 21; strong SWE-bench (13.86%) 23; Devin 2.0 has interactive planning, search, wiki.22Expensive enterprise tool [User Query]; success on very complex/novel tasks still limited 23; requires codebase access by third-party; some demo authenticity concerns raised.22GPT-Engineer (open-source)4 (High Autonomy, experimental)CLI tool: single prompt -&gt; clarification dialogue -&gt; codebase generation 24Can bootstrap entire (simple) projects from one prompt; automates initial spec-to-code process 24; open-source and customizable.24Highly prompt-dependent; generated code needs significant review/debugging for complex apps 24; primarily for initial generation, not iterative work on existing code [User Query].AutoGPT (for coding tasks)4 (High Autonomy, experimental)Self-directed AI agent framework (user sets high-level goal, agent plans &amp; executes with tools) 98Fully automated workflow: breaks goal into sub-tasks, uses tools (web search, file I/O, code execution) iteratively without human prompts.98Often inefficient, prone to loops or incorrect paths without precise goals/human feedback 100; not specialized for coding; high API costs; mainly experimental for dev tasks.99
(Note: Tools at Level 5 are omitted from the table as no productized examples currently exist; this level remains theoretical and experimental.)ConclusionThe landscape of AI-powered developer tools is evolving at a remarkable pace, progressing from rudimentary Level 1 code completion assistants to the sophisticated, emerging Level 4 autonomous coding agents. This report has categorized these tools across a five-level autonomy framework, providing a structured understanding of their capabilities, interaction modes, and the shifting role of the human developer in the software creation process.Level 1 tools, such as traditional IDE IntelliSense and early ML-enhanced completers like Tabnine (in its core function), offer essential micro-task assistance, accelerating typing and reducing trivial errors by predicting likely code sequences based on immediate local context. They represent a foundational layer of AI assistance, valued for their responsiveness and low overhead.Level 2 tools, exemplified by AI pair programmers like GitHub Copilot (standard mode), Amazon CodeWhisperer, and direct use of models like OpenAI's GPT series, mark a significant step up. These tools can generate substantial code blocks or entire functions from natural language prompts or broader contextual cues. However, their operation is reactive and single-step, firmly positioning the developer as the architect and quality controller responsible for validating and integrating each piece of AI-generated code. The specialization of tools like CodeWhisperer for specific ecosystems (e.g., AWS) at this level highlights a strategy to provide targeted value.The transition to Level 3 signifies a paradigm shift, with AI becoming a context-aware agent capable of supervised automation. Tools such as GitHub Copilot Agent Mode, Cursor, Windsurf (with Cascade), Anthropic's Claude Code, Aider, and the Amazon Q Developer CLI can autonomously perform multi-step coding tasks. Given a high-level goal, these agents can plan and execute a sequence of actions—editing multiple files, running tests, interacting with version control, and iteratively debugging—while the developer supervises and reviews the outcomes. The deep integration of these agents with the developer's environment (IDE or CLI) and their ability to use external tools are hallmarks of this level. The increasing emphasis on Git integration, automated testing, and even commit/PR generation at this level indicates that these tools are not just writing code but are beginning to automate aspects of the broader development workflow. This trend suggests that as AI tools become more autonomous in code generation, the logical progression is to integrate them more deeply into the surrounding lifecycle processes to maximize productivity gains. This, in turn, implies a growing need for comprehensive and well-maintained test suites, as the AI's ability to "fix until tests pass" is fundamentally constrained by the quality of the tests themselves.Level 4 tools, such as Google's Jules and Cognition's Devin, push the boundaries towards highly autonomous coding. These agents aim to handle complex projects or features end-to-end with minimal human input, aspiring to deliver production-ready code. Their architecture often involves cloud-based execution, access to a full suite of developer tools, advanced planning, and long-term memory. While these systems demonstrate impressive capabilities on benchmarks and specific tasks, their general reliability for all complex scenarios is still evolving, and the "human in the loop" for final validation remains crucial. The emergence of such tools signifies an ambition for AI to transition from a highly capable assistant to a largely independent "software engineer," though the gap between current demonstrations and consistent, trustworthy production of complex software remains substantial.Finally, Level 5—full autonomy where AI agents set their own strategic software development goals—remains largely theoretical and experimental. Research into self-improving AI systems, AI-driven algorithm discovery, and multi-agent collaboration provides glimpses into this future. However, the profound challenges related to trust, safety, alignment, and the sheer complexity of strategic software development mean that Level 5 is a distant prospect. The path towards such advanced autonomy will likely be incremental, building upon the successes and addressing the limitations of Level 3 and 4 tools.In conclusion, the integration of AI into software development is undeniably transformative. As tools progress through the autonomy levels, they offer the potential to significantly enhance developer productivity, automate tedious tasks, and even augment human creativity. However, this progression also necessitates a continuous re-evaluation of developer roles, skills, and workflows. Understanding the specific autonomy level, capabilities, and limitations of each AI tool is paramount for its effective and safe adoption. While the allure of fully autonomous AI is compelling, the current and near-future reality is one of human-AI collaboration, where AI serves as an increasingly powerful and intelligent partner, empowering developers to tackle more complex challenges and innovate at an accelerated pace. The ultimate objective is not necessarily the replacement of human developers but the augmentation of their capabilities, fostering a new era of software engineering.SourcesInformation and claims within this report are supported by inline citations referencing the provided research materials, designated by IDs such as 10 or.7 These references point to official documentation, technical blogs, academic papers, and reputable industry analyses relevant as of early-to-mid 2025.# From Code Completion to Autonomous Coding Agents: AI Developer Tools by Autonomy LevelThe software development landscape is undergoing a significant transformation, largely propelled by the rapid advancements and integration of Artificial Intelligence (AI). This evolution is underscored by substantial growth in private AI investment, with generative AI alone attracting $33.9 billion globally in 2024, marking an 18.7% increase from the previous year.1 AI is no longer confined to research laboratories; it is becoming a pervasive force in daily life and business operations, evidenced by 78% of organizations reporting AI use in 2024, a notable rise from 55% in 2023.1 This trend extends deeply into the realm of developer tooling, where AI-powered solutions are becoming increasingly sophisticated. The discourse itself is shifting from AI as a tool for mere "automation" to AI exhibiting "autonomy," signifying a deeper capacity for independent operation and decision-making.2 Accenture's Technology Vision for 2025 highlights this transition, projecting a future where AI acts autonomously on behalf of individuals, with a reported 75% of knowledge workers already utilizing generative AI.2 This massive investment and widespread adoption signal strong market confidence and a perceived return on investment, suggesting that AI developer tools are being embraced at a pace necessary to remain competitive and enhance productivity, aligning with research indicating AI's potential to boost productivity and narrow skill gaps.1This report provides an expert-level analysis of these AI developer tools, categorizing them based on a five-level autonomy framework. This framework, analogous to the levels established for autonomous driving systems, serves to delineate the evolving capabilities of these tools, from basic code assistants to highly autonomous coding agents. Such a classification is pertinent because both autonomous driving and AI coding involve tasks traditionally performed by humans that demand complex reasoning, contextual understanding, and decision-making. The staged levels in both domains represent milestones in the delegation of these responsibilities to AI, thereby raising similar questions about reliability, accountability, and the evolving role of human oversight.4 The challenges anticipated for Level 5 autonomy in vehicles, such as regulatory hurdles, technological limitations, and development costs 4, are likely to find parallels in the domain of AI coding. This report will explore the capabilities, interaction modes, use cases, and the pros and cons of representative tools at each autonomy level, ensuring factual accuracy through direct references to current research and official documentation. The increasing integration of AI into development workflows promises substantial productivity gains 1 but also introduces new considerations regarding developer roles, code quality, security, and trust. This document aims to navigate these complexities, offering a clear perspective on the current state and future trajectory of AI in software engineering.Autonomy Levels for AI Coding ToolsTo provide a structured understanding of their capabilities, AI coding tools are classified into autonomy levels, drawing an analogy to the widely recognized levels of autonomous driving. This classification helps to differentiate tools based on the degree of human oversight required and the complexity of tasks they can handle.</p>
<p>Level 1 – Basic Code Completion:AI at this level offers single-line or small code suggestions based on the immediate context, such as the current file content and syntax. The developer remains in full control, accepting or rejecting these suggestions for micro-tasks. This functionality is comparable to basic cruise control in automobiles. Tools categorized under Level 1 are designed to accelerate typing and minimize trivial errors but do not generate complex code blocks, understand broader project context, or make independent decisions regarding logic or structure [User Query]. Their primary function is to react to the developer's current input by predicting likely code sequences. The focus is on immediate lexical and syntactic context rather than deep semantic understanding. For instance, Visual Studio Code's IntelliSense provides completions based on "language semantics and an analysis of your source code," but for many languages, this is limited to "word-based completions" without specific extensions 7, reacting to typed input within the current file. Similarly, Tabnine offers "context-aware suggestions based on your code and patterns" but mainly provides "code completions for current line and multiple lines for full-function implementation" 8, which is also a reactive process. The core function is to "predict likely code to follow," not to comprehend higher-level intent [User Query].</p>
<p>Level 2 – Advanced Code Generation (AI Pair Programmer):AI systems at this level can produce larger code snippets or entire functions based on natural language prompts (e.g., comments within the code) or a broader local context. The AI functions akin to a junior pair programmer. However, each output from the AI is typically a single-step operation that is subsequently reviewed and integrated by the human developer. Developers continue to guide the overall software architecture, verify the correctness of the generated code, and explicitly prompt the AI for each significant generation task [User Query]. These tools are often powered by large language models (LLMs) trained on extensive code repositories. The defining characteristic of Level 2 is the AI's capacity to translate natural language intent into substantial code blocks, moving beyond simple completion to actual generation, yet still necessitating explicit human prompting for each distinct operation. For example, GitHub Copilot can suggest "whole lines or entire functions" based on comments or existing code context 10, with the interaction being a prompt-response for each generated piece. Amazon CodeWhisperer generates "suggestions based on your existing code and comments," which can range from "a single line comment to fully formed functions" 11, initiated by the developer's input. OpenAI Codex and its more advanced successors like GPT-4 are designed to "translates natural language into code" 12, representing a direct prompt-to-code generation step. In all these instances, the human developer acts as the driver, prompting for each segment of code and undertaking its review, as the AI does not autonomously chain actions.</p>
<p>Level 3 – Context-Aware Agents (Supervised Automation):</p>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; Contributors to the CyberSecAI Guide.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/CyberSecAI/CyberSecAI.github.io" target="_blank" rel="noopener" title="Guide" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/CyberSecAI/CyberSecAI.github.io/issues/" target="_blank" rel="noopener" title="Report an Issue or Suggest a Change" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 0c53 0 96 43 96 96v3.6c0 15.7-12.7 28.4-28.4 28.4H188.5c-15.7 0-28.4-12.7-28.4-28.4V96c0-53 43-96 96-96zM41.4 105.4c12.5-12.5 32.8-12.5 45.3 0l64 64c.7.7 1.3 1.4 1.9 2.1 14.2-7.3 30.4-11.4 47.5-11.4h112c17.1 0 33.2 4.1 47.5 11.4.6-.7 1.2-1.4 1.9-2.1l64-64c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-64 64c-.7.7-1.4 1.3-2.1 1.9 6.2 12 10.1 25.3 11.1 39.5h64.3c17.7 0 32 14.3 32 32s-14.3 32-32 32h-64c0 24.6-5.5 47.8-15.4 68.6 2.2 1.3 4.2 2.9 6 4.8l64 64c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0l-63.1-63.1c-24.5 21.8-55.8 36.2-90.3 39.6V240c0-8.8-7.2-16-16-16s-16 7.2-16 16v239.2c-34.5-3.4-65.8-17.8-90.3-39.6l-63.1 63c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3l64-64c1.9-1.9 3.9-3.4 6-4.8C101.5 367.8 96 344.6 96 320H32c-17.7 0-32-14.3-32-32s14.3-32 32-32h64.3c1.1-14.1 5-27.5 11.1-39.5-.7-.6-1.4-1.2-2.1-1.9l-64-64c-12.5-12.5-12.5-32.8 0-45.3z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/CyberSecAI/CyberSecAI.github.io" target="_blank" rel="noopener" title="source on Github" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/CyberSecAI/CyberSecAI.github.io/print_page/" target="_blank" rel="noopener" title="Save as HTML or print to PDF / paper" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M288 32c0-17.7-14.3-32-32-32s-32 14.3-32 32v242.7l-73.4-73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l128 128c12.5 12.5 32.8 12.5 45.3 0l128-128c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L288 274.7V32zM64 352c-35.3 0-64 28.7-64 64v32c0 35.3 28.7 64 64 64h384c35.3 0 64-28.7 64-64v-32c0-35.3-28.7-64-64-64H346.5l-45.3 45.3c-25 25-65.5 25-90.5 0L165.5 352H64zm368 56a24 24 0 1 1 0 48 24 24 0 1 1 0-48z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.expand", "navigation.footer", "search.highlight", "search.share", "search.suggest"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.af256bd8.min.js"></script>
      
        <script src="../../../js/print-site.js"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../../../javascripts/tablesort.js"></script>
      
    
  </body>
</html>
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Your guide to using AI for CyberSecurity <p>Why Read this Guide?</p> <p>This guide shows how cybersecurity professionals can get up and running quickly with AI tools to:</p> <ul> <li>augment your skills and amplify your impact</li> <li>reduce your toil</li> </ul> <p>Large Language Models (hosted and open source), and BERT-based language models tools will be covered e.g.</p> <ol> <li>NotebookLM</li> <li>Prompts and Prompt Techniques applied to Cybersecurity </li> <li>BERTopic</li> <li>Embeddings for CyberSecurity</li> <li>AI Agents for orchestrated workflows</li> </ol> <p>Risk Based Prioritization Guide</p> <p>Check out another guide I co-wrote with various thought leaders in vulnerability management  https://riskbasedprioritization.github.io/ </p> <p>The Risk Based Prioritization described in this guide significantly reduces the </p> <ul> <li>cost of vulnerability management </li> <li>risk by reducing the time adversaries have access to vulnerable systems they are trying to exploit.</li> </ul>"},{"location":"CWE_Assignment/NotebookLM_Cwe/","title":"NotebookLM CWE","text":"<p>Overview</p> <p>In this chapter, we'll use NotebookLM to ingest the Common Weakness Enumeration (CWE) list. </p> <ul> <li>In the current version (CWE Version 4.14 2024-02-29 as at June 2024), there are over 1400 CWEs, and 2789 pages in the PDF version of the list. </li> <li>It can be unwieldy to navigate or assimilate this information. </li> </ul> <p>Using NotebookLM, we can chat with the CWE document and ask questions, so that the information comes to us in the form of an answer. </p> <p>For comparison, we will also ask some of the same questions of ChatGPT4o covering 2 cases:</p> <ol> <li>without uploading any documents to it.</li> <li>uploading a document to it.</li> </ol> <p>Success</p> <p>This content and approach was presented to the CWE Root Cause Mapping Working Group mid July.</p> <p>The feedback was very positive!</p>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#data-sources","title":"Data Sources","text":""},{"location":"CWE_Assignment/NotebookLM_Cwe/#mitre-cwe-specification-preparation","title":"MITRE CWE Specification Preparation","text":"<p>Tip</p> <p>cwe_latest.pdf  from https://cwe.mitre.org/data/downloads.html has 2789 pages (version CWE Version 4.14 2024-02-29).</p> <ul> <li>This is too big for NotebookLM to ingest.</li> <li>Use a JSON text version instead of PDF:<ul> <li>ensures all the relevant text we want is fed to the model (e.g. text extraction from PDFs can be lossy for tables)</li> <li>allows us to remove content that is not relevant e.g. \"ContentHistory\" can contain a lot of text that is not relevant to CWE assignment</li> </ul> </li> </ul> <p>Data Sources</p> <p>The files in https://github.com/CyberSecAI/cwe_top25/tree/main/data_out/output_jsonl are imported into NoteBookLM.. They are generated as follows: </p> <ol> <li>Get the MITRE CWE Specification as JSON from https://github.com/CWE-CAPEC/REST-API-wg/blob/main/json_repo/cwe.json</li> <li>Remove the \"ContentHistory\" entries as this is not useful to assign CWEs and is a lot of content<ol> <li>use Claude 3.5 to generate the python code to do this</li> </ol> </li> <li>Split it into smaller files<ol> <li>because the single file is too large to import</li> <li>use Claude 3.5 to generate the python code to do this</li> </ol> </li> <li>The output files are JSONL with a txt extension so NoteBookLM recognizes them: https://github.com/CyberSecAI/cwe_top25/tree/main/data_out/output_jsonl</li> </ol>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#import-mitre-cwe-specification-to-notebooklm","title":"Import MITRE CWE Specification to NoteBookLM","text":"<ol> <li>Download all jsonl.txt files from https://github.com/CyberSecAI/cwe_top25/tree/main/data_out/output_jsonl</li> <li>Open https://notebooklm.google.com/ in a browser</li> <li>Add sources</li> </ol>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#what-cwe-ids-are-associated-with-buffer-overflow","title":"what cwe ids are associated with buffer overflow?","text":"<p>Note</p> <p>https://cwe.mitre.org/data/definitions/121.html contains only 1 Observed Example: </p> <ul> <li>CVE-2021-35395. This is listed under CWE-121: Stack-based Buffer Overflow</li> </ul> <p>The other CVE examples are from the Top25 Mapping Examples included in the JSONL files</p> <ul> <li>CVE-2021-20046, CVE-2021-20048: Stack-based buffer overflows in SonicOS HTTP response headers allow for Denial of Service and potentially remote code execution.</li> <li>CVE-2021-21887: A stack-based buffer overflow in Lantronix PremierWave 2050 8.9.0.0R4 allows remote code execution via a crafted HTTP request.</li> <li>CVE-2021-35395: A stack-based buffer overflow vulnerability exists in the DHCP client of Lantronix PremierWave 2050 8.9.0.0R4.</li> </ul>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#cwe-assignment-prompt","title":"CWE Assignment Prompt","text":"<p>The notebook can be used to assign CWEs to a given CVE Description using a specific prompt: https://github.com/CyberSecAI/CWEMap/blob/main/prompts/assign_cwes/system.md</p> <p>NoteBookLM does not allow the full prompt to be copied into the chat as is - it needs to be copied in 3 smaller parts.</p>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#example-cve-2024-5158","title":"Example CVE-2024-5158","text":"<p>CVE-2024-5158</p> <p>Type Confusion in V8 in Google Chrome prior to 125.0.6422.76 allowed a remote attacker to potentially perform arbitrary read/write via a crafted HTML page. (Chromium security severity: High) https://nvd.nist.gov/vuln/detail/CVE-2024-5158</p> <p></p>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#example-cve-2022-30271","title":"Example CVE-2022-30271","text":"<p>CVE-2022-30271</p> <p>The Motorola ACE1000 RTU through 2022-05-02 ships with a hardcoded SSH private key and initialization scripts (such as /etc/init.d/sshd_service) only generate a new key if no private-key file exists. Thus, this hardcoded key is likely to be used by default. https://nvd.nist.gov/vuln/detail/CVE-2022-30271</p> <p></p> <p>Warning</p> <p>Some of the CWE Abstraction Level values are incorrect; we\u2019re asking for too much here in one prompt with this simple setup.</p> <p>We need to either simplify or decompose into separate prompts.</p>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#sharing-notebooks","title":"Sharing Notebooks","text":"<p>Tip</p> <p>The notebook can be shared with other users via email (using the share button top right) to allow them to use the notebook. See https://support.google.com/notebooklm/answer/14276471?hl=en.</p> <p>Sharing is via a user's email.</p>"},{"location":"CWE_Assignment/NotebookLM_Cwe/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>I found it useful, quick, and convenient to be able to ask questions of the CWE list, as a compliment to the traditional search method I've used in the past.</li> <li>It is also useful to suggest CWEs (and associated info) for a given Vulnerability Description.</li> <li>The Vulnrichment chapter covers a much more detailed use case - and NoteBookLM performed better than other LLMs.</li> </ol>"},{"location":"CWE_Assignment/Vulnrichment/","title":"CISA Vulnrichment","text":"<p>Overview</p> <p>An incorrect CWE assignment caught my eye while reading a post</p> <p>I was reading a post on LinkedIn and the CWE assigned by CISA ADP looked wrong so...</p> <ul> <li>I used my NotebookLM CWE notebook, and other LLMs, to determine the appropriate CWE.</li> <li>I then raised an issue: https://github.com/cisagov/vulnrichment/issues/84.</li> <li>I then decided to dig a bit more into this... specifically the CWEs assigned by CISA ADP.</li> </ul> <p>Using LLMs to find incorrect CWE assignments at scale</p> <p>I used langchain to create a consensus of LLMs to review all CWEs assigned by CISA ADP to find issues:</p> <ul> <li>These issues were found automatically by a consensus of 3 LLMs: (current state-of-the-art) ChatGPT4o, Gemini 1.5 Pro, Claude 3.5 Sonnet who were asked to review CWEs assigned to CVEs by CISA ADP.</li> <li> <p>The consensus output was then reviewed by a human (me).</p> </li> <li> <p>I created 3 Vulnrichment Github Issues initially and these were accepted by CISA Vulnrichment and resolved promptly!</p> </li> <li> <p>I then provided a report to CISA Vulnrichment for all CWEs that were incorrect based on the consensus.</p> </li> </ul> <p>Using LLMs to assign correct CWEs</p> <p>Finally, I showed how NotebookLM can be used for CWE assignment</p> <ul> <li>It avoids the problem of<ul> <li>training language models on bad data (existing CVE CWE assignments)</li> <li>training humans on the detailed CWE standard (though a basic understanding is still required)</li> </ul> </li> <li>NotebookLM did well in recommending a CWE given a CVE Description.... and providing a supporting CVE from the CWE Observed Examples in the CWE standard.<ul> <li>NotebookLM has a large context window which allows it to digest the large CWE standard, and it is source-grounded as described in the NotebookLM chapter.</li> <li>Notebook on CWEs describes how to create this NotebookLM for CWEs</li> </ul> </li> </ul> <p>The different approaches used (and the subscription plan used):</p> <ol> <li>code: <ol> <li>ChatGPT4o OpenAI Batch API (Plus Plan)</li> <li>langchain calling 2 LLMs via APIs: Gemini 1.5 Pro, Claude 3.5 Sonnet (ChatGPT4 or ChatGPT4o is also supported in the code (but commented out) if your plan supports that)</li> </ol> </li> <li>no-code using the browser chat interface is not shown here but it is useful for initial evaluation<ol> <li>Gemini 1.5 Pro (subscription)</li> <li>Claude 3.5 Sonnet (prepay)</li> </ol> </li> </ol> <p>Success</p> <p>The approach described here resulted in successfully identifying incorrectly assigned CWEs, and identifying the correct CWEs, and a report to CISA Vulnrichment.</p>"},{"location":"CWE_Assignment/Vulnrichment/#consensus-of-3-llms","title":"Consensus of 3 LLMs","text":"<p>Source Code</p> <p>https://github.com/CyberSecAI/CWEMap</p>"},{"location":"CWE_Assignment/Vulnrichment/#cisa-vulnrichment_1","title":"CISA Vulnrichment","text":"<p>Quote</p> <p>The CISA Vulnrichment project is the public repository of CISA's enrichment of public CVE records through CISA's ADP (Authorized Data Publisher) container. In this phase of the project, CISA is assessing new and recent CVEs and adding key SSVC decision points. Once scored, some higher-risk CVEs will also receive enrichment of CWE, CVSS, and CPE data points, where possible.</p> <p>https://github.com/cisagov/vulnrichment</p> <p>I have great admiration for CISA and their pragmatic initiatives like CISA KEV and SSVC and have spoken about them and applied them in production.</p> <p>Tip</p> <p>One of the many benefits of this Vulnrichment project is that feedback can be provided as GitHub issues and the team is VERY responsive \u2705  \ud83d\ude4c</p> <ul> <li>The 'Bug' label was assigned the same day to the 3 issues I submitted: https://github.com/cisagov/vulnrichment/issues?q=is%3Aissue+author%3ACrashedmind+is%3Aclosed.</li> <li>The changes were accepted and applied the next working day and a comment was added to the ticket which was then closed.</li> </ul> <p>My overall goal here was to </p> <ul> <li>Show that LLMs could augment human analysts where vulnerability enrichment today is largely done manually.</li> <li>Show how to use them for this purpose.</li> <li>Get people to use LLMs to improve the quality of the CVE data in general, and in this specific example case, the CWE data.</li> <li>Maximize the value of the CISA ADP data and enrichment by improving CWE-assignment quality.</li> </ul>"},{"location":"CWE_Assignment/Vulnrichment/#get-cves-enriched-by-cisa-adp","title":"Get CVEs Enriched by CISA ADP","text":""},{"location":"CWE_Assignment/Vulnrichment/#what-to-ask-the-llms","title":"What to ask the LLMs?","text":"<p>Different approaches are possible when providing the CVE Description to the LLM:</p> <ol> <li>provide the CWE assigned as part of the CVE, and ask the LLM if it agrees or not, and only if not, why<ol> <li>This reduces the output token count/cost by only outputting the verbose rationale in case of disagreement</li> </ol> </li> <li>ask the LLM to assign one or more CWEs, with rationale</li> </ol> <p>The first approach is easier and simpler and cheaper (in terms of token use i.e. shorter response output), and better as a first pass option to get the low hanging fruit.</p> <p>The second approach could be used at the time of CWE assignment to get a second opinion.</p>"},{"location":"CWE_Assignment/Vulnrichment/#consensus","title":"Consensus","text":"<p>To minimize human effort, 3 LLMs are used and the consensus is reviewed</p> <ul> <li>The LLMs are state-of-the-art models from different providers i.e. the best available and reasonably independent.</li> <li>The results are sorted by consensus i.e. 3 models in agreement, then 2 models in agreement,.... and by the LLM's confidence in their responses</li> <li>A Human (me) then reviewed (sorted by consensus and confidence) and made the final decision.</li> </ul>"},{"location":"CWE_Assignment/Vulnrichment/#recipe","title":"Recipe","text":"<ol> <li>Get the Vulnrichment subset of CVEs where CISA ADP assigned a CWE (regardless of whether the CWE was the same or different than that assigned by the CNA) into a sheet/CSV file.<ol> <li>~1.8K (CISA ADP Assigned CWEs) of ~~10K CVEs (in Vulnrichment)</li> </ol> </li> <li>As a dry-run submit e.g. 50 CVE Descriptions, CWEs to each of the 3 LLMs to review via the chat UI in one prompt</li> <li>Ask ChatGPT4o (via Batch API) to Agree (Yes/No) with the assigned CWE (and provide a Confidence score, and rationale if not)<ol> <li>Sort these by Confidence score i.e. start with the highest Confidence ones.</li> </ol> </li> <li>Assign the same task to Gemini and Claude via APIs driven by langchain</li> </ol>"},{"location":"CWE_Assignment/Vulnrichment/#create-a-prompt","title":"Create a Prompt","text":""},{"location":"CWE_Assignment/Vulnrichment/#chat-interface-table-output","title":"Chat Interface - Table Output","text":"<pre><code>caption_system_prompt =\nYou are a cybersecurity expert specializing in identifying Common Weakness Enumeration (CWE) IDs from CVE descriptions.\nYour goal is is to say if you Agree with the assigned CWE ID or not.\nYou will be provided with a CVE ID and description amd a CWE ID that has been assigned to that CVE description.\n\nPlease provide the response in a table 'cve_id', 'CWE_ID', 'Agree'. \"Rationale\", Confidence' where\n1. Agree: string // Yes or No\n2. Rationale: string // Only if you do not Agree, provide a rationale why not\n3. Confidence: string // a confidence score between 0 and 1\n</code></pre> <p>The table output allows copy-and-pasting by a human into a sheet.</p> <p>The prompt consists of these parts:</p> <ol> <li>Role + Task: which is the same for the Chat and API interface</li> <li>Output format: which is different for the Chat and API interface</li> <li>A binary value Agree is requested</li> <li>The rationale only if there is disagreement. This saves on output tokens.</li> <li>A Confidence score to limit impacts of hallucinations, and as a way to assess and prioritize responses by confidence.</li> <li>No (Few-shot) examples are provided. Based on the results, these were not necessary.<ol> <li>If Few-shot examples were required, I'd submit multiple CVEs in a single batch request (because putting the examples in each single CVE request would add a LOT of input tokens)</li> </ol> </li> </ol>"},{"location":"CWE_Assignment/Vulnrichment/#batch-api-interface-json-output","title":"Batch API Interface - JSON Output","text":"<p><pre><code>caption_system_prompt =\nYou are a cybersecurity expert specializing in identifying Common Weakness Enumeration (CWE) IDs from CVE descriptions.\nYour goal is is to say if you Agree with the assigned CWE ID or not.\nYou will be provided with a CVE ID and description amd a CWE ID that has been assigned to that CVE description.\n\nYou will output a json object containing the following information:\n{\n    Agree: string // Yes or No\n    Rationale: string // Only if you do not Agree, provide a rationale why not\n    Confidence: string // a confidence score between 0 and 1\n}\n</code></pre> The JSON output allows processing by machines.</p> <p>Use JSON Mode</p> <p>ChatGPT and Gemini 1.5 support JSON mode that always outputs valid JSON. Use it!</p> <ul> <li>See details of Latest Gemini features support in LangChain4j 0.32.0.</li> </ul> <p>While you can prompt an LLM to output JSON, it may not always output valid JSON and you're left with a cleanup exercise (a friend of mine had that experience when they first tried this \ud83d\ude09)</p> <p>Claude doesn't have a formal \"JSON Mode\" though, in my usage, it always produced valid JSON.</p> <p>Tip</p> <p>It is possible to submit multiple CVEs in one prompt for each batch entry i.e. similar to what is done when using the Chat interface.</p> <ul> <li>This is what is done here. 10 CVE CWE assignments are sent per batch (though the OpenAI Batch API example contains 1 entry only)</li> </ul>"},{"location":"CWE_Assignment/Vulnrichment/#llms","title":"LLMs","text":""},{"location":"CWE_Assignment/Vulnrichment/#gemini-15-pro-api-via-langchain","title":"Gemini 1.5 Pro API via Langchain","text":"<p>The API interface (via Langchain) was used in this example submitting multiple CVEs in one prompt.</p>"},{"location":"CWE_Assignment/Vulnrichment/#claude-35-sonnet-api-via-langchain","title":"Claude 3.5 Sonnet API via Langchain","text":""},{"location":"CWE_Assignment/Vulnrichment/#model","title":"Model","text":"<p>Currently: Claude 3.5 Sonnet was used as it has the best performance vs cost for Claude models.</p> <p> https://docs.anthropic.com/en/docs/welcome </p>"},{"location":"CWE_Assignment/Vulnrichment/#interface","title":"Interface","text":"<p>Currently: Claude does not support a native Batch API interface - though  Amazon Bedrock supports batching of prompts to models including Claude.</p> <p>The API interface (via Langchain) was used in this example submitting multiple CVEs in one prompt.</p>"},{"location":"CWE_Assignment/Vulnrichment/#chatgpt4o-batch-api","title":"ChatGPT4o Batch API","text":""},{"location":"CWE_Assignment/Vulnrichment/#model_1","title":"Model","text":"<p>gpt-4o</p>"},{"location":"CWE_Assignment/Vulnrichment/#plan","title":"Plan","text":"<p>The Plus plan subscription was used.</p> <p>Quote</p> <p>There are some restrictions:</p> <ul> <li>The file can contain up to 50,000 requests.</li> <li>The file cannot be more than 100 MB in size.</li> </ul> <p>Enqueued token limit reached for gpt-4o in organization XYZ. Limit: 90,000 enqueued tokens. Please try again once some in_progress batches have been completed.'</p>"},{"location":"CWE_Assignment/Vulnrichment/#interface_1","title":"Interface","text":"<p>Batch Interface API.</p> <p>The ~1800 ADP CVE-CWE pairs were split into 15 files of 100 CVE-CWE pair prompts to comfortably fit under this token limit.</p> <ul> <li>very little effort was spent to optimize the file size (number of prompts per batch), or the prompt size.</li> <li>The cost to process the ~1800 ADP CVE-CWE pairs: ~$2.</li> </ul>"},{"location":"CWE_Assignment/Vulnrichment/#observations","title":"Observations","text":""},{"location":"CWE_Assignment/Vulnrichment/#leak","title":"Leak","text":"<p>Several CVE Descriptions that include \"leak\" were incorrectly assigned \"CWE-200 Exposure of Sensitive Information to an Unauthorized Actor\".</p> <p>These were actually resource leaks (memory, program objects like handles etc...), not leakage of sensitive data.</p>"},{"location":"CWE_Assignment/Vulnrichment/#gemini-15-pro-hallucinations","title":"Gemini 1.5 Pro Hallucinations","text":"<p>Failure</p> <p>From a sample of 30 assigned CWEs, Gemini 1.5 Pro had 3 hallucinations when asked to provide a rationale for its response (response text shown below, with hallucination in bold).</p> <p>Quote</p> <ol> <li>While CWE-400 (Uncontrolled Resource Consumption) could be a potential consequence, the core issue described in the CVE is about improper handling of the Python crash handler within a chroot environment. This misconfiguration could allow an attacker to potentially escape the chroot and execute code in the context of the Apport process.  A more appropriate CWE might be CWE-247 (Improper Handling of Chroot Environments) or CWE-22 (Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')).<ol> <li>CWE-247: DEPRECATED: Reliance on DNS Lookups in a Security Decision</li> <li>CWE-243: Creation of chroot Jail Without Changing Working Directory is the closest to \"Improper Handling of Chroot Environments\"</li> </ol> </li> <li>The assigned CWE-436 (Unspecified Encoding or Escaping of Output) doesn't fit well. The description argues that the plugin itself doesn't handle file uploads, making it a matter of what WordPress allows to be uploaded. If the core WordPress installation doesn't prevent polyglot files, it's not directly the plugin's fault. A more suitable CWE might be CWE-284 (Improper Access Control) if the plugin fails to restrict access to already uploaded polyglot files, allowing unauthorized viewing or download.<ol> <li>CWE-116 is the correct CWE: CWE-116: Improper Encoding or Escaping of Output</li> </ol> </li> <li>While CWE-57 (Relative Path Traversal) could be a factor in exploiting this vulnerability, the description more directly aligns with CWE-664: Improper Control of File Deletion or Renaming Operations. The vulnerability allows attackers to delete arbitrary files, which is the core characteristic of CWE-664. Relative path traversal might be a means to reach different files, but the primary issue is the improper control over file deletion.<ol> <li>CWE-664 correct title is CWE-664: Improper Control of a Resource Through its Lifetime</li> </ol> </li> </ol> <p>Tip</p> <p>While Gemini 1.5 Pro is the backend for NotebookLM, no hallucinations were detected in NotebookLM (the benefit of a source-grounded closed system).</p>"},{"location":"CWE_Assignment/Vulnrichment/#notebooklm","title":"NotebookLM","text":"<p>NotebookLM works very well for suggesting a relevant CWE for a given CVE Description (or reviewing an assigned CWE)</p> <ul> <li>It suggests related CVEs to support the CWE it suggests (sometimes prompting is required). These CVEs are part of the \"Observed Examples\" CVEs that are listed under a CWE in the CWE standard.</li> <li>The numbered grey circles in the diagram below are links to references from the CWE standard.</li> </ul> <p> CVE-2024-35059</p> <p> CVE-2023-49224</p> <p>Note</p> <p>The NotebookLM prompts above are deliberately not applying prompt engineering principles to show that NotebookLM still provides a useful response.</p>"},{"location":"CWE_Assignment/Vulnrichment/#refinements","title":"Refinements","text":"<p>Note: Some refinements are possible, but were not implemented in this first pass to minimize Time-To-Value</p> <ol> <li>The full CWE standard was used here for illustration purposes (and to take on the harder problem of a large specification)<ol> <li>A subset of CWEs could be used if that is desired. </li> <li>In practice, several hundred CWEs are assigned to CVEs.</li> <li>Remove the 25 \"DEPRECATED:\" entries</li> </ol> </li> <li>The text from the references in the CVE \"References to Advisories, Solutions, and Tools\" was not retrieved and fed to the LLM as part of the CVE Description for CWE review or assignment.<ol> <li>These references were reviewed manually (for the consensus of incorrect CWE assignments)</li> <li>In some cases, this has additional text available that can inform the CWE assignment beyond the CVE Description alone</li> <li>It is relatively easy to retrieve this content automatically but this content varies significantly by type (text, image, pdf), quality, structure.</li> <li>Separately, it is common that these links break because the original website or post is removed, or it's not in the latest version in Github,... - so it would be useful to have the extracted text at the time of CWE assignment.  </li> </ol> </li> <li>Additional known good CVE descriptions and CWE assignments could be incorporated into the corpus, to augment the limited CVE observed examples that are part of the CWE standard.</li> <li>Reducing the set of CWEs to the desired population, or providing guidance in the prompt on what CWEs to use (e.g. \"don't use CWE's marked as Discouraged\")</li> <li>As I was interested in comparing LLM responses, I did not optimize the LLM usage (all LLMs were fed all CVE-CWEs)<ol> <li>This can be done in several ways e.g. <ol> <li>Ask each LLM in turn to review the previous assessments by LLMs</li> <li>Sampling</li> </ol> </li> </ol> </li> </ol>"},{"location":"CWE_Assignment/Vulnrichment/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li> <p>The value of CVE data depends on its quality. </p> <ol> <li>For all published CVEs to date, the quality of CWEs assigned is questionable.</li> <li>A large part of that is that humans can't grok ~~1000 CWEs. LLMs can.</li> </ol> </li> <li> <p>Using LLMs to suggest or validate CWEs can reduce the manual effort and error in CWE assignment.</p> </li> <li>LLMs can validate CWEs at scale e.g. using Batch mode, or multiple CVEs per prompt, or both.</li> <li>LLMs perform well at this task and, given they can be automated, can augment the human manual effort, and improve the quality of assigned CWEs.</li> <li>Langchain makes it easier to have generic code that works across multiple LLMs.</li> <li>LLM JSON Mode should be used where possible to reduce bad JSON output and subsequent cleanup.</li> <li>Based on a manual review of the subset where all 3 LLMs disagreed with the CWE assignment, &gt; 75% of these CWEs were incorrect (and a report with these was submitted to CISA Vulnrichment)<ol> <li>I did not dig into the subset where 2 of 3 LLMs disagreed.</li> </ol> </li> <li>Using LLMs to suggest or validate CWEs can reduce the manual effort and error in CWE assignment.</li> <li>A closed-system that is grounded on the CWE standard only e.g. NotebookLM, performs very well for assigning CWEs, or reviewing assigned CWEs (though it does not have an API so can't do this at scale), and no hallucinations were observed.</li> </ol>"},{"location":"CWE_Assignment/cwe_gpt/","title":"ChatGPT CWE GPT","text":"<p>Overview</p> <p>Here we'll use ChatGPT with the MITRE CWE specification to aid mapping CWEs to vulnerability descriptions.</p> <p>This is a no-code option.</p> <p>We'll attempt to implement a closed grounded system to ensure the accuracy of the data (and mitigate hallucinations)</p> <ol> <li>Grounded: content is provided to inform the ansers</li> <li>Closed system: answers come from only the documents you provide</li> </ol> <p>Result</p> <p>The result is a Grounded Open-System i.e. we can ground - but can't create a closed system - and we still get hallucinations in some cases.</p>"},{"location":"CWE_Assignment/cwe_gpt/#recipe","title":"Recipe","text":"<ol> <li>Use ChatGPT GPTs which are \"custom versions of ChatGPT that combine instructions, extra knowledge, and any combination of skills.\" </li> <li>Grounded: Provide the MITRE CWE specification as the \"extra knowledge\" in JSON format (not PDF).</li> <li>Closed system: Limit the GPT to that knowledge only i.e. disable web search <ol> <li>but we see that the GPT still has knowledge from its training data </li> </ol> </li> </ol>"},{"location":"CWE_Assignment/cwe_gpt/#mitre-cwe-specification","title":"MITRE CWE Specification","text":"<ol> <li>Use a JSON text version instead of PDF <ol> <li>ensures all the relevant text we want is fed to the model (e.g. text extraction from PDFs can be lossy for tables)</li> <li>allows us to remove content that is not relevant e.g. \"ContentHistory\" can contain a lot of text that is not relevant to CWE assignment</li> </ol> </li> <li>Get the MITRE CWE Specification as JSON from https://github.com/CWE-CAPEC/REST-API-wg/blob/main/json_repo/cwe.json</li> <li>Remove the \"ContentHistory\" entries as this is not useful to assign CWEs and is a lot of content<ol> <li>use Claude 3.5 to generate the python code to do this</li> </ol> </li> <li>Split it into smaller files<ol> <li>because the single file is too large to import</li> <li>use Claude 3.5 to generate the python code to do this</li> </ol> </li> </ol>"},{"location":"CWE_Assignment/cwe_gpt/#configure-chatgpt-cwe-gpt","title":"Configure ChatGPT CWE GPT","text":"<ol> <li>Import the MITRE CWE Specification as split JSON files</li> <li>Disable all capabilities:<ol> <li>Web browsing is disabled so the answers come from the imported MITRE CWE Specification</li> </ol> </li> <li>Provide example starter prompts:<ol> <li>what is the best CWE to describe the root cause weakness in CVE \"an issue in the Pickle Python library of some product allows attackers to execute arbitrary commands\". Provide CVEs with the most similar root cause to support your answer.</li> <li>what is the best CWE to describe the root cause weakness in CVE \"ProductX contains a default SSH public key in the authorized_keys file. A remote attacker could use this key to gain root privileges.\". Provide CVEs with the most similar.  root cause to support your answer.</li> <li>what cwe ids are associated with xss. list them all</li> <li>what cwe ids are associated with path or directory traversal. list them all</li> </ol> </li> <li>Publish it so others can use it: https://chatgpt.com/g/g-d8guurMPs-cwe-map </li> </ol>"},{"location":"CWE_Assignment/cwe_gpt/#check-if-the-system-is-closed","title":"Check if the System is Closed","text":"<p>Quote</p> <p>What is a dog?</p> <p>Failure</p> <p> </p> <p>Observations</p> <p>The system is not closed because the GPT can answer the question even though there is no information about dogs in the MITRE CWE specification.</p>"},{"location":"CWE_Assignment/cwe_gpt/#example-usage-cwe-502","title":"Example Usage: CWE-502","text":"<p>Quote</p> <p>what is the best CWE to describe the root cause weakness in CVE \"an issue in the Pickle Python library of some product allows attackers to execute arbitrary commands\". </p> <p>let's think this out step by step. Before you answer, validate your answer by checking that at least one of ObservedExamples for that CWE is similar to the description I provided.</p> <p></p> <p>CWE-502 includes this as part of the ObservedExamples <pre><code>        {\n          \"Reference\": \"CVE-2011-2520\",\n          \"Description\": \"Python script allows local users to execute code via pickled data.\",\n          \"Link\": \"https://www.cve.org/CVERecord?id=CVE-2011-2520\"\n        },\n        {\n          \"Reference\": \"CVE-2012-4406\",\n          \"Description\": \"Unsafe deserialization using pickle in a Python script.\",\n          \"Link\": \"https://www.cve.org/CVERecord?id=CVE-2012-4406\"\n        },\n</code></pre></p>"},{"location":"CWE_Assignment/cwe_gpt/#example-usage-cwe-1394","title":"Example Usage: CWE-1394","text":"<p>Quote</p> <p>what is the best CWE to describe the root cause weakness in CVE \"ProductX contains a default SSH public key in the authorized_keys file. A remote attacker could use this key to gain root privileges.\"</p> <p>let's think this out step by step. Before you answer, validate your answer by checking that at least one of ObservedExamples for that CWE is similar to the description I provided.</p> <p></p> <p>CWE-1394 includes this as part of the ObservedExamples <pre><code>        {\n          \"Reference\": \"CVE-2016-1561\",\n          \"Description\": \"backup storage product has a default SSH public key in the authorized_keys file, allowing root access\",\n          \"Link\": \"https://www.cve.org/CVERecord?id=CVE-2016-1561\"\n        },\n</code></pre></p>"},{"location":"CWE_Assignment/cwe_gpt/#example-usage-cwe-1394-with-example-cves","title":"Example Usage: CWE-1394 with Example CVEs","text":"<p>Failure</p> <p>\"CVE-2013-0138: Network device with a default SSH key that can be exploited to gain unauthorized access.\"</p> <p>This is not the description for https://nvd.nist.gov/vuln/detail/CVE-2013-0138.</p> <p>CVE-2013-0138 is not listed in the MITRE CWE specification.</p> <p>Note: https://nvd.nist.gov/vuln/detail/CVE-2013-0137 relates to a known SSH key.</p> <p>Observations</p> <p>Hallucinations happened for the CVE IDs that the GPT gave as supporting examples whether the \"Web Browsing\" Capability was enabled or not.</p>"},{"location":"CWE_Assignment/cwe_gpt/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Grounding ChatGPT with the MITRE CWE specification helps reduce hallucinations but does not eliminate them.</li> <li>Hallucinations happened for the CVE IDs that the GPT gave as supporting examples whether the \"Web Browsing\" Capability was enabled or not.</li> <li>It is not possible to make ChatGPT GPT a closed system because it has knowledge outside the provided content and will answer from that.</li> </ol>"},{"location":"CWE_Assignment/cwe_overview/","title":"Overview","text":"<p>Overview</p> <p>There are several options to consider when building an LM solution for CWE assignment as described here.</p>"},{"location":"CWE_Assignment/cwe_overview/#approach-to-using-language-models","title":"Approach to using Language Models","text":""},{"location":"CWE_Assignment/cwe_overview/#dont-train-a-model-on-bad-data","title":"Don't Train A Model On Bad Data!","text":"<p>It is possible to train a Language Model as a Classifier to assign CWEs to a CVE Description - and there are several research papers that took that approach e.g.</p> <ul> <li>V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities </li> <li>Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses</li> </ul> <p>The problems with this approach:</p> <ol> <li> <p>It's delusional based on my research and experience of incorrect assigned CWEs in general - Garbage In Garbage Out</p> <ol> <li>Per Steve Christey Coley, CWE tech lead: </li> </ol> <p>Quote</p> <p>There has been significant interest in using AI/ML in various applications to use and/or map to CWE, but in my opinion there are a number of significant hurdles, e.g. you can't train on \"bad mappings\" to learn how to do good mappings.</p> </li> <li> <p>It removes a lot of the context that could be available to an LM by reducing the reference target down to a set of values or classes (for the given input CVE Descriptions)</p> </li> </ol>"},{"location":"CWE_Assignment/cwe_overview/#train-on-good-data-and-the-full-standard","title":"Train on Good Data and the Full Standard","text":"<p>We can \"train\" on \"good mappings\".</p> <ol> <li>The CWE standard includes known \"good mappings\" e.g. CWE-917 Observed Examples includes CVE-2021-44228 and its Description.<ol> <li>The count of these CVE Observed Examples varies significantly per CWE. </li> <li>There's ~3K CVE Observed Examples in the CWE standard.</li> </ol> </li> <li>The Top25 Dataset of known-good mappings contains ~6K CVEs with known-good CWE mappings by MITRE.</li> <li>We can use the full CWE standard and associated known good CWE mappings as the target, allowing an LLM to compare the CVE Description (and other data) to this.<ol> <li>And moreover, prompt the LLM to provide similar CVEs to support its rationale for the CWE assignment</li> </ol> </li> </ol> <p>Tip</p> <p>Rather than train a model on bad data, we can ask a model to assign / validate a CWE based on its understanding of the CWEs available (and its understanding of CWEs assigned to similar CVEs based on the Observed and Top25 Examples for each CWE in the standard).</p> <p>We can ask the model to follow the MITRE CWE Guidance when assigning a CWE.</p>"},{"location":"CWE_Assignment/cwe_overview/#closed-or-open-model","title":"Closed or Open Model","text":"<p>We can use a Closed or Open Model:</p> <ol> <li>a closed-model with access to the CWE specification only (and no other data) e.g. NotebookLM</li> <li>an open-model with access to the CWE specification and other data</li> </ol>"},{"location":"CWE_Assignment/cwe_overview/#rag-corpus","title":"RAG Corpus","text":"<p>Representations of the MITRE CWE Specification:</p> <ol> <li>PDF version of the MITRE CWE Specification <ol> <li>https://cwe.mitre.org/data/downloads.html </li> </ol> </li> <li>JSON version of the MITRE CWE Specification <ol> <li>https://github.com/CWE-CAPEC/REST-API-wg/blob/main/json_repo/cwe.json</li> </ol> </li> <li> <p>JSON version of the modified MITRE CWE Specification to add and remove parts to make it more relevant to CWE assignment for an LLM as described here</p> <ol> <li>https://github.com/CyberSecAI/cwe_top25/tree/main/data_out/output_jsonl </li> </ol> <p>Tip</p> <p>JSON is preferred over PDF as PDF is generally more lossy because it is less structured.</p> </li> </ol>"},{"location":"CWE_Assignment/cwe_overview/#gpt-setups","title":"GPT setups","text":"<p>Different GPT setups e.g.</p> <ol> <li>ChatGPT GPT<ol> <li>Requires you and users to have a paid ChatGPT subscription</li> </ol> </li> <li>NoteBookLM<ol> <li>Anyone with a Google account can get up and running in 5 minutes for free.</li> </ol> </li> <li>VertexAI<ol> <li>This allows the most customization - but there's more effort to set it up and it is not free.</li> </ol> </li> </ol>"},{"location":"CWE_Assignment/cwe_overview/#prompts","title":"Prompts","text":"<ol> <li>Various Prompts, and Prompt Engineering Techniques, can be used depending on what you want.</li> </ol>"},{"location":"CWE_Assignment/cwe_overview/#model-and-environment","title":"Model and Environment","text":"<p>For processing 250K+ CVE Descriptions, speed, latency and cost are important considerations, in addition to accuracy.</p> <p>Based on comparing LLMs as at September 2024, Gemini 1.5 Flash was chosen.</p> <p>There are different Google AI Environments:    - Google AI Studio      - lower learning curve and cost and capability    - Vertex AI Studio or VertexAI in general</p>"},{"location":"CWE_Assignment/vertex_ai/","title":"Google Vertex AI Agent Builder","text":"<p>Overview</p> <p>Here we'll use Vertex AI Agent Builder with the MITRE CWE specification to aid mapping CWEs to vulnerability descriptions.</p> <p>This is a no-code option.</p> <p>We'll implement a closed grounded system to ensure the accuracy of the data (and mitigate hallucinations)</p> <ol> <li>Grounded: content is provided to inform the answers</li> <li>Closed system: answers come from only the documents you provide</li> </ol> <p>Tip</p> <p>In other words, we'll build NotebookLM.</p> <ul> <li>where NotebookLM is basically a combination of Vertex AI Search for Unstructured (PDFs, HTML, etc.), Vertex AI Grounding, and a custom UX/UI.</li> <li>But we'll take advantage of the structured data (JSON) that we have for MITRE CWE list, instead of using the unstructured data from the MITRE CWE list PDF.</li> </ul> <p>Result</p> <p>The result is that we have a grounded closed system (that compares in performance and accuracy to NotebookLM.</p> <p>But we don't have reference links to the source content in the response i.e. I didn't add that part yet but it's standard functionality that is easy in Vertex AI.</p>"},{"location":"CWE_Assignment/vertex_ai/#grounding-confidence","title":"Grounding Confidence","text":"<p>Quote</p> <p>For each response generated from the content of your connected data stores, the agent evaluates a confidence level, which gauges the confidence that all information in the response is supported by information in the data stores. You can customize which responses to allow by selecting the lowest confidence level you are comfortable with. Only responses at or above that confidence level will be shown.</p> <p>There are 5 confidence levels to choose from: VERY_LOW, LOW, MEDIUM, HIGH, and VERY_HIGH.</p> <p>https://cloud.google.com/dialogflow/vertex/docs/concept/tools</p> <p>Quote</p> <p>To create a data store and connect it to your app, you can use the Tools link in the left navigation of the console. Follow the instructions to create a data store.   https://cloud.google.com/dialogflow/vertex/docs/concept/tools</p>"},{"location":"CWE_Assignment/vertex_ai/#recipe","title":"Recipe","text":"<p>Same recipe as before but we'll use Google Vertex AI Agent Builder</p>"},{"location":"CWE_Assignment/vertex_ai/#mitre-cwe-specification","title":"MITRE CWE Specification","text":"<p>Same MITRE CWE Specification as the data source.</p>"},{"location":"CWE_Assignment/vertex_ai/#build-vertex-ai-agent","title":"Build Vertex AI Agent","text":"<p>This link gives the steps with links to the details, summarized here:</p> <ol> <li>Vertex AI Agent Builder </li> <li>Create App </li> <li>Select app type<ol> <li>Agent (preview) \"Built using natural language, agents can answer questions from data, connect with business systems through tools and more\"</li> </ol> </li> <li>Create Data Store<ol> <li>The MITRE CWE JSON data is converted to jsonl format for import</li> <li>It takes ~5 minutes to ingest (create embeddings for) the jsonl file</li> </ol> </li> <li>There are lots of other Settings available like Logging, Git integration to push/pull agents from a Github repo, or just download the JSON file that represents the agent.</li> <li>The built agent supports Interactions with the API .</li> </ol> <p>Tip</p> <p>To create an Grounded Open system, select \"search\" app type.</p> <p>The agent will retrieve information from the local documents you provide and via web search.</p> <p>Note</p> <p>Alternatively these steps can be implemented with code e.g.    https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#generative-ai-gemini-grounding-python_vertex_ai_sdk</p> <p>Quote</p> <p>Note: Conversation history is used as context during tool invocation. Learn more</p>"},{"location":"CWE_Assignment/vertex_ai/#data-preprocessing","title":"Data Preprocessing","text":"<p>Remove unneeded sections from the json</p> <ul> <li>content_history</li> <li>views</li> <li>categories</li> </ul> <pre><code>python3 ./trim_json.py # cwe.json -&gt; cwe_trimmed.json\n</code></pre> <p>Convert to jsonl for import <pre><code>python3 ./json_to_jsonl.py # cwe_trimmed.json -&gt; cwe_trimmed.jsonl\n</code></pre></p>"},{"location":"CWE_Assignment/vertex_ai/#data-import","title":"Data Import","text":""},{"location":"CWE_Assignment/vertex_ai/#check-if-the-system-is-closed","title":"Check if the System is Closed","text":"<p>Quote</p> <p>What is a dog?</p> <p> </p> <p>Success</p> <p>The system is closed because the GPT can't answer the question because there is no information about dogs in the MITRE CWE specification.</p>"},{"location":"CWE_Assignment/vertex_ai/#what-are-the-different-types-of-xss","title":"What are the different types of XSS?","text":""},{"location":"CWE_Assignment/vertex_ai/#what-cwe-ids-relate-to-xss","title":"What CWE IDs Relate To XSS?","text":""},{"location":"CWE_Assignment/vertex_ai/#what-is-the-parent-weakness-or-cwe-for-xss-and-csrf","title":"What Is The Parent Weakness Or CWE For XSS And CSRF?","text":""},{"location":"CWE_Assignment/vertex_ai/#what-cwe-ids-relate-to-path-or-directory-traversal-list-all-cwe-ids-and-their-description-in-a-table","title":"What CWE IDs Relate To Path Or Directory Traversal?  List All CWE IDs And Their Description In A Table","text":""},{"location":"CWE_Assignment/vertex_ai/#what-is-the-cwe-associated-with-cve","title":"What is the CWE Associated With CVE","text":"<p>Quote</p> <p>What is the CWE associated with CVE-2021-27104 \"Accellion FTA OS Command Injection Vulnerability\"</p> <p>What is the CWE associated with \"Cisco Small Business RV320 and RV325 Routers Information Disclosure Vulnerability\"</p> <p></p>"},{"location":"CWE_Assignment/vertex_ai/#example-usage-cwe-1394","title":"Example Usage: CWE-1394","text":"<p>Quote</p> <p>what is the best CWE to describe the root cause weakness in CVE \"ProductX contains a default SSH public key in the authorized_keys file. A remote attacker could use this key to gain root privileges.\". </p> <p></p>"},{"location":"CWE_Assignment/vertex_ai/#other-app-builder-docs","title":"Other App Builder Docs","text":"<p>These were not used or required but listing here as I found them informative.</p> <ol> <li>https://codelabs.developers.google.com/codelabs/vertex-ai-conversation#0</li> <li>https://codelabs.developers.google.com/build-google-quality-rag#0</li> <li>https://github.com/GoogleCloudPlatform/generative-ai/tree/main</li> <li>https://github.com/GoogleCloudPlatform/generative-ai/tree/main/conversation/chat-app</li> <li>https://cloud.google.com/generative-ai-app-builder/docs/samples?language=python</li> <li>https://cloud.google.com/generative-ai-app-builder/docs/samples/genappbuilder-multi-turn-search?hl=en</li> <li>https://cloud.google.com/generative-ai-app-builder/docs/samples/genappbuilder-import-documents?hl=en</li> <li>https://www.googlecloudcommunity.com/gc/Community-Blogs/Building-and-Deploying-AI-Agents-with-LangChain-on-Vertex-AI/bc-p/750793#M415</li> <li>https://medium.com/google-cloud/gen-ai-grounding-with-vertex-ai-llm-3cb1cbe9f9d2</li> <li>https://medium.com/google-cloud/designing-data-store-hybrid-agents-with-dialogflow-cx-vertex-ai-agents-070082f07cb4</li> <li>https://www.cloudskillsboost.google/paths/236/course_templates/978/labs/488165</li> <li>https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/2d22382cf72840dcde313db2f2feb2115f9fbd70/gemini/grounding/intro-grounding-gemini.ipynb?hl=es-419</li> <li>https://docs.llamaindex.ai/en/stable/examples/agent/agentic_rag_using_vertex_ai/</li> <li>https://saschaheyer.medium.com/vertex-ai-grounding-large-language-models-8335f838990f</li> </ol>"},{"location":"CWE_Assignment/vertex_ai/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Google Vertex AI Agent Builder allows/requires more control over the agent than the ChatGPT GPTs currently. </li> <li>Google Vertex AI Agent Builder supports a Closed (or Open) System with Grounding and Grounding Confidence threshold unlike ChatGPT GPTs currently.</li> <li>This comes close to NotebookLM but <ol> <li>does not provide references from the original documents from which the answer was determined.</li> </ol> </li> </ol>"},{"location":"Grounded/grounding/","title":"Grounding","text":"<p>Overview</p> <p>The accurate answers from NotebookLM highlight the benefits of a grounded closed system.</p> <p>NotebookLM also provides links to the content it references in the data sources.</p> <p>There are many tools that can be used to build such a system.</p>"},{"location":"Grounded/grounding/#grounding-overview","title":"Grounding Overview","text":"<p>Quote</p> <p>What is Grounding?</p> <p>Grounding is the process of using large language models (LLMs) with information that is use-case specific, relevant, and not available as part of the LLM's trained knowledge. It is crucial for ensuring the quality, accuracy, and relevance of the generated output. While LLMs come with a vast amount of knowledge already, this knowledge is limited and not tailored to specific use-cases. To obtain accurate and relevant output, we must provide LLMs with the necessary information. In other words, we need to \"ground\" the models in the context of our specific use-case.</p> <p>Motivation for Grounding</p> <p>The primary motivation for grounding is that LLMs are not databases, even if they possess a wealth of knowledge. They are designed to be used as general reasoning and text engines. LLMs have been trained on an extensive corpus of information, some of which has been retained, giving them a broad understanding of language, the world, reasoning, and text manipulation. However, we should use them as engines rather than stores of knowledge.</p> <p>https://techcommunity.microsoft.com/t5/fasttrack-for-azure/grounding-llms/ba-p/3843857</p> <p>Quote</p> <p>In generative AI, grounding is the ability to connect model output to verifiable sources of information. If you provide models with access to specific data sources, then grounding tethers their output to these data and reduces the chances of inventing content. This is particularly important in situations where accuracy and reliability are significant.</p> <p>Grounding provides the following benefits:</p> <ul> <li>Reduces model hallucinations, which are instances where the model generates content that isn't factual.</li> <li>Anchors model responses to specific information.</li> <li>Enhances the trustworthiness and applicability of the generated content.</li> </ul> <p>https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview</p>"},{"location":"Grounded/grounding/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p> https://techcommunity.microsoft.com/t5/fasttrack-for-azure/grounding-llms/ba-p/3843857</p> <p>Quote</p> <p>Retrieval Augmented Generation (RAG) is the primary technique for grounding and the only one I will discuss in detail. RAG is a process for retrieving information relevant to a task, providing it to the language model along with a prompt, and relying on the model to use this specific information when responding. While sometimes used interchangeably with grounding, RAG is a distinct technique, albeit with some overlap. It is a powerful and easy-to-use method, applicable to many use-cases.</p> <p>Fine-tuning, on the other hand, is an \"honourable mention\" when it comes to grounding. It involves orchestrating additional training steps to create a new version of the model that builds on the general training and infuses the model with task-relevant information. In the past, when we had less capable models, fine-tuning was more prevalent. However, it has become less relevant as time-consuming, expensive, and not offering a significant advantage in many scenarios.</p> <p>The general consensus among experts in the field is that fine-tuning typically results in only a 1-2% improvement in accuracy (depending on how accuracy is defined). While there may be specific scenarios where fine-tuning offers more significant gains, it should be considered a last-resort option for optimisation, rather than the starting go-to technique.</p> <p>Info</p> <p>Unlike RAG, fine tuning changes some of the model weights. In some cases, it can lead to reduced performance via catastrophic forgetting. </p>"},{"location":"Grounded/grounding/#vertex-ai-grounding","title":"Vertex AI Grounding","text":"<p> https://youtu.be/RjuKEMVe_Q4?t=303</p> <p>Google announced Grounding in April 2024.</p> <p>Quote</p> <p>You can ground language models to your own text data using Vertex AI Search as a datastore. With Vertex AI Search you integrate your own data, regardless of format, to refine the model output. Supported data types include:</p> <ul> <li>Website data: Directly use content from your website.</li> <li>Unstructured data: Utilize raw, unformatted data.</li> </ul> <p>When you ground to your specific data the model can perform beyond its training data. By linking to designated data stores within Vertex AI Search, the grounded model can produce more accurate and relevant responses, and responses directly related to your use case.</p> <p>https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview#ground-private</p>"},{"location":"Grounded/grounding/#rag-architectures","title":"RAG Architectures","text":"<p> https://www.linkedin.com/posts/weaviate-io_struggling-to-keep-up-with-new-rag-variants-activity-7272294342122192896-iMs1 is the image source, and it contains other useful articles on RAG.</p>"},{"location":"Grounded/grounding/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Where a lot of the information needed is captured in documentation e.g. MITRE CWE specification, Grounding is an effective efficient easy option to improve the quality of responses.</li> </ol>"},{"location":"NotebookLM/NotebookLM/","title":"NotebookLM","text":"<p>Overview</p> <p>LLMs change the information retrieval paradigm. Instead of searching for information where we go to the information, we can chat with our documents and ask questions of them, so that the information comes to us in the form of an answer. </p> <p>In this section, we'll use NotebookLM, and we just need to import our documents to be able to chat with them. </p> <p>ChatGPT4o is also used for comparison to highlight where one is better applied than the other depending on the context.</p> <ul> <li>Both tools use LLMs, but NoteBookLM uses a \"Closed System\" (only the document sources you provide), versus ChatGPT4o which bases it answers on the open internet content at the time it was trained, and additionally the documents you provide.</li> </ul> <p>Tip</p> <p>Your responses from NotebookLM may be different than the examples shown here. LLMs will give different responses to the same question.</p>"},{"location":"NotebookLM/NotebookLM/#notebooklm_1","title":"NotebookLM","text":"<p>Tip</p> <p>Quote</p> <p>NotebookLM lets you read, take notes, ask questions, organize your ideas, and much more -- all with the power of Google AI helping you at every step of the way.</p> <p>Quote</p> <p>Audio Overview, a new way to turn your documents into engaging audio discussions. With one click, two AI hosts start up a lively \u201cdeep dive\u201d discussion based on your sources. They summarize your material, make connections between topics, and banter back and forth. You can even download the conversation and take it on the go.</p> <p>Quote</p> <p>It runs on the company\u2019s Gemini 1.5 Pro model (released Dec 2023), the same AI that powers the Gemini Advanced chatbot.  (ref)</p>"},{"location":"NotebookLM/NotebookLM/#key-features-and-benefits-of-gemini-15-models","title":"Key Features and Benefits of Gemini 1.5 Models","text":"<p>Per Gemini 1.5 Technical Report, the Key Features and Benefits of Gemini 1.5 Models are</p> <ol> <li>Highly Compute-Efficient Multimodal Models<ol> <li>Capable of recalling and reasoning over fine-grained information from millions of tokens of context, including long documents, videos, and audio.</li> </ol> </li> <li>Benchmark Performance<ol> <li>Outperforms other models such as Claude 3.0 (200k tokens) and GPT-4 Turbo (128k tokens) in next-token prediction and retrieval up to 10M tokens (approximately 7M words).</li> </ol> </li> <li>Unprecedented Context Handling<ol> <li>Handles extremely long contexts, up to at least 10M tokens (approximately 7M words).</li> <li>Capable of processing long-form mixed-modality inputs, including entire document collections, multiple hours of video, and almost five days of audio.</li> <li>Near-perfect recall on long-context retrieval tasks across various modalities.</li> </ol> </li> <li>Realistic Multimodal Long-Context Benchmarks<ol> <li>Excels in tasks requiring retrieval and reasoning over multiple parts of the context.</li> <li>Outperforms all competing models across all modalities, even those augmented with external retrieval methods.</li> </ol> </li> </ol> <p>These features make Gemini 1.5 models a generational leap over existing models, offering unparalleled performance in processing and understanding extensive and complex multimodal information.</p> <p>Tip</p> <p>Such systems map document content to vectors (numeric representations of words or tokens in multi-dimensional space).</p> <p>Queries are based on similarity (proximity in vector space).</p>"},{"location":"NotebookLM/NotebookLM/#document-loading","title":"Document Loading","text":"<p>Documents are loaded via GoogleDrive, PDFs, Text files, Copied text, Web page URL.</p> <p>Tip</p> <p>Any sources can be used e.g. Books in PDF format, websites, text files.</p> <p>Using a file of site content (if available) e.g.a PDF, is generally more reliable than using a URL to that site; it ensures all the content is ingested.</p>"},{"location":"NotebookLM/NotebookLM/#closed-system","title":"Closed System","text":"<p>These documents become the corpus where information is retrieved from, with references to the document(s) the information was retrieved from.</p> <p>Quote</p> <p>\u201cNotebookLM is a closed system.\u201d This means the AI won\u2019t perform any web searches beyond what you, the user, give it in a prompt. Every response it generates pertains only to the information it has on hand. (ref)</p> <p>Quote</p> <p>\u201csource-grounded AI\u201d: you define a set of documents that are important to your work\u2014called \u201csources\u201d in the NotebookLM parlance\u2014and from that point on, you can have an open-ended conversation with the language model where its answers will be \u201cgrounded\u201d in the information you\u2019ve selected. It is as if you are giving the AI instant expertise in whatever domain you happen to be working in. (ref)</p>"},{"location":"NotebookLM/NotebookLM/#what-is-a-dog","title":"What is a dog?","text":"<p>This is illustrated with a simple example of asking our Notebook on CWEs \"what is a dog?\":</p> <p>Quote</p> <p>The sources provided do not contain any information about dogs. Therefore, the query \"what is a dog?\" cannot be answered from the given sources.</p> <p></p>"},{"location":"NotebookLM/NotebookLM/#sharing","title":"Sharing","text":"<p>Unlike Google Docs, it is not possible to share a NotebookLM publicly - sharing is done directly via email addresses.</p>"},{"location":"NotebookLM/NotebookLM/#how-to-use-notebooklm","title":"How To Use NotebookLM","text":""},{"location":"NotebookLM/NotebookLM/#references","title":"References","text":"<ol> <li>Introducing NotebookLM, Oct 19, 2023, Steven Johnson who contributed to NotebookLM</li> <li>Getting The Most Out Of Notes In NotebookLM, Mar 18, 2024, Steven Johnson</li> <li>How To Use NotebookLM As A Research Tool, Feb 19, 2024, Steven Johnson</li> <li>Google's NotebookLM is now an even smarter assistant and better fact-checker, June 7, 2024 </li> <li>Using Google\u2019s NotebookLM for Data Science: A Comprehensive Guide, Dec 7, 2023 </li> <li>How to use Google\u2019s genAI-powered note-taking app, Feb 15, 2024 </li> </ol>"},{"location":"NotebookLM/NotebookLM/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>NotebookLM is a powerful free solution from Google that allows users to quickly and easily build a source-grounded AI (where users define the set of documents) and then have an open-ended conversation with the language model where its answers will be \u201cgrounded\u201d in the information users selected.</li> <li>The support for large contexts means that large documents can be processed - as demonstrated in the following sections.</li> <li>I found it a useful tool / companion for the research I was doing on vulnerability management to augment my knowledge and capabilities.</li> </ol>"},{"location":"NotebookLM/NotebookLM_Attack/","title":"NotebookLM ATTACK","text":"<p>Overview</p> <p>As part of Proactive Software Supply Chain Risk Management (P-SSCRM) Framework that I've been collaborating on, we wanted to apply MITRE ATT&amp;CK. It's a detailed specification, so NotebookLM can help us.</p> <p>In this chapter, we'll use NotebookLM to ingest the MITRE ATT&amp;CK Enterprise Tactics. </p> <ul> <li>In the current version, MITRE ATT&amp;CK Matrix for Enterprise consists of 14 tactics, 559 Total Attack Patterns. </li> <li>It can be unwieldy to navigate or assimilate this information. </li> </ul> <p>Using NotebookLM, we can chat with the MITRE ATT&amp;CK Matrix and ask questions, so that the information comes to us in the form of an answer. </p> <ol> <li>without uploading any documents to it.</li> <li>uploading a document to it.</li> </ol>"},{"location":"NotebookLM/NotebookLM_Attack/#data-sources","title":"Data Sources","text":"<p>The MITRE ATTACK Tactics and Techniques are available online at https://attack.mitre.org/tactics/enterprise/ as 1 webpage for each of the 14 Tactics.</p> <ul> <li>However, loading these webpages (or the \"Version Permalink\" pages) into NotebookLM did not work.</li> </ul> <p>MITRE ATTACK is also available as an Excel file from https://attack.mitre.org/resources/attack-data-and-tools/</p> <ul> <li>https://attack.mitre.org/docs/enterprise-attack-v15.1/enterprise-attack-v15.1.xlsx</li> <li>Note: The data is also available as JSON. </li> </ul> <p>So we can convert that to a text file and load those as follows:</p> <ol> <li>Open the Excel file</li> <li>For the tabs we want (e.g. Tactics, Techniques, Mitigations), delete the columns we don't want </li> <li>Export each tab as a csv file</li> <li>Transpose the data to a text file</li> <li>Import the text files</li> </ol> <p>Data Sources</p> <p>The MITRE ATTACK csv and text files, and script to convert, are available in https://github.com/CyberSecAI/CyberSecAI.github.io/tree/main/data/Notebook_Attack</p>"},{"location":"NotebookLM/NotebookLM_Attack/#convert-mitre-attack-to-a-text-file-for-import","title":"Convert MITRE ATTACK to a Text File for Import","text":"<p>Claude 3.5 Sonnet Prompt: <pre><code>i have a csv file with these columns ID    name description    tactics    detection    \"data sources\"    \"is sub-technique\"    \"sub-technique of\"\nI want to extract the content into a document to read. \nEach row should be extracted as follows: the name of the first column, then the ccontent for the first column. Then the second column name and content.\nAnd so on for each column.\nAnd do this for all rows\n\nI want python code to do this\n</code></pre></p> <p>The LLM generated generic code that was applied to convert each csv file to a text file.</p>"},{"location":"NotebookLM/NotebookLM_Attack/#load-each-text-file","title":"Load each text file","text":"<p>So we can just load each one to NoteBookLM as a text file:</p> <ul> <li>MITRE_ATTACK_mitigations.txt</li> <li>MITRE_ATTACK_tactics.txt</li> <li>MITRE_ATTACK_techniques.txt</li> </ul>"},{"location":"NotebookLM/NotebookLM_Attack/#tell-me-about-mitre-techniques-related-to-supply-chain","title":"Tell me about MITRE techniques related to supply chain","text":""},{"location":"NotebookLM/NotebookLM_Attack/#notebooklm-answer","title":"NotebookLM Answer","text":""},{"location":"NotebookLM/NotebookLM_Attack/#claude-sonnet-35-answer","title":"Claude Sonnet 3.5 Answer","text":""},{"location":"NotebookLM/NotebookLM_Attack/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Any data or document in text format can be converted to a format suitable for import to an LM.</li> </ol>"},{"location":"NotebookLM/NotebookLM_Capec/","title":"NotebookLM CAPEC","text":"<p>Overview</p> <p>In this chapter, we'll use NotebookLM to ingest the Common Attack Pattern Enumeration and Classification (CAPEC) list. </p> <ul> <li>In the current version (CAPEC List Version 3.9 as at June 2024), there are 559 Total Attack Patterns. </li> <li>It can be unwieldy to navigate or assimilate this information. </li> </ul> <p>Using NotebookLM, we can chat with the CAPEC document and ask questions, so that the information comes to us in the form of an answer. </p> <p>For comparison, we will also ask some of the same questions of ChatGPT4o covering 2 cases:</p> <ol> <li>without uploading any documents to it.</li> <li>uploading a document to it.</li> </ol> <p>Note that CAPEC appears a lot less than CWE on the internet so an LLM will have a lot less context for CAPEC.</p>"},{"location":"NotebookLM/NotebookLM_Capec/#data-sources","title":"Data Sources","text":"<p>https://capec.mitre.org/data/downloads.html offers various formats (XML, CSV, HTML), and views:</p> <ul> <li>Mechanisms of Attack: https://capec.mitre.org/data/slices/1000.html</li> <li>Domains of Attack: https://capec.mitre.org/data/slices/3000.html<ul> <li>We'll use this one.</li> </ul> </li> </ul> <p>NotebookLM - Upload from - Web page URL - Paste website link - https://capec.mitre.org/data/slices/3000.html</p>"},{"location":"NotebookLM/NotebookLM_Capec/#what-is-the-capec-id-that-related-to-xss","title":"what is the CAPEC ID that related to xss","text":""},{"location":"NotebookLM/NotebookLM_Capec/#chatgpt4o-answer","title":"ChatGPT4o Answer","text":"<p>ChatGPT4o did not provide all the CWEs that NotebookLM did.</p> <p></p>"},{"location":"NotebookLM/NotebookLM_Capec/#chatgpt4o-answer-with-hallucination","title":"ChatGPT4o Answer with Hallucination","text":"<p>Failure</p> <p>Hallucination</p> <p>\"CAPEC-633: Client-Side XSS using Content-Security-Policy Bypass\" is not valid https://capec.mitre.org/data/definitions/633.html</p>"},{"location":"NotebookLM/NotebookLM_Capec/#chatgpt4o-answer-with-hallucination-with-uploaded-capec-file","title":"ChatGPT4o Answer with Hallucination with Uploaded CAPEC File","text":"<p>ChatGPT4o UI did not process the CAPEC HTML file, and the UI does not accept URLs, so the CSV file was uploaded instead.</p> <p></p>"},{"location":"NotebookLM/NotebookLM_Capec/#chatgpt4o-validate-the-hallucination","title":"ChatGPT4o Validate the Hallucination","text":"<p>In a different ChatGPT4o session (new context to avoid the hallucination), we ask ChatGPT4o to validate the CAPEC. </p> <p></p>"},{"location":"NotebookLM/NotebookLM_Capec/#chatgpt4o-review-and-rate-the-responses-from-notebooklm-and-chatgpt4o","title":"ChatGPT4o Review and Rate the Responses from NotebookLM and ChatGPT4o","text":"<p>ChatGPT4o was used to review and rate both responses (copied directly from the NoteBookLM and ChatGPT4o responses)</p> <p></p> <p></p> <p></p>"},{"location":"NotebookLM/NotebookLM_Capec/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>For less common or referenced text like CAPEC, ChatGPT4o can hallucinate even when the CAPEC list is uploaded to it.</li> <li>NotebookLM did not hallucinate (a benefit of the \"Closed System\" approach).</li> </ol>"},{"location":"NotebookLM/NotebookLM_Config/","title":"NotebookLM Config","text":"<p>Overview</p> <p>I came across this via https://tldrsec.com/p/tldr-sec-237 (an excellent newsletter) in the \"AI + Security\" section, and it piqued my interest! </p> <p>One area of research is using LLMs for infrastructure configuration as detailed in https://www.coguard.io/post/coguard-uses-openai-cybersecurity-grant-to-automate-infrastructure-security and the associated repo.</p> <p>Here we take on the first task [C1]: Extraction of security and uptime-relevant configuration parameters from manuals; for the example provided in the repo: Apache Spark</p>"},{"location":"NotebookLM/NotebookLM_Config/#details","title":"Details","text":""},{"location":"NotebookLM/NotebookLM_Config/#task","title":"Task","text":"<p>Quote</p> <p>[C1] Extraction of security and uptime-relevant configuration parameters from manuals. The goal of this component is simple to describe, but hard to accomplish. Given a manual for a software component, extract the configuration parameters and define the security relevant ones from it.</p> <p>Example: For Apache Spark, the manual is provided on the general configuration page online, i.e. in HTML format, and there is also a specific security page. The expectation would be to at least extract the parameters from the security page, as well as some log-related items from the general page. In total, when manually examining the configuration parameters, it totals approximately 80 parameters that are security relevant. You can find these in the Appendix A.</p>"},{"location":"NotebookLM/NotebookLM_Config/#original-prompt","title":"Original Prompt","text":"<p>The original prompt used is per https://github.com/coguardio/coguard_openai_rule_auto_generation_research/tree/master?tab=readme-ov-file#extraction-of-security-relevant-parameters-in-c1</p> <p>Quote</p> <p>You are an assistant that is a cybersecurity expert and knows their configuration files. Your response here should be a JSON list of strings Using the documentation of Apache Spark, which configuration parameters are relevant from a security perspective?.</p>"},{"location":"NotebookLM/NotebookLM_Config/#expected-answer","title":"Expected Answer","text":"<p>The expected answer is per https://github.com/coguardio/coguard_openai_rule_auto_generation_research/tree/master?tab=readme-ov-file#appendix-a. </p> <p>Quote</p> <p>The following parameters were identified by the CoGuard team by hand as relevant from a security point of view.</p> <pre><code>spark.yarn.shuffle.server.recovery.disabled\nspark.authenticate\nspark.authenticate.secret\nspark.authenticate.secret.file\nspark.authenticate.secret.driver.file\nspark.authenticate.secret.executor.file\nspark.network.crypto.enabled\nspark.network.crypto.config.*\nspark.network.crypto.saslFallback\nspark.authenticate.enableSaslEncryption\nspark.network.sasl.serverAlwaysEncrypt\nspark.io.encryption.enabled\nspark.io.encryption.keySizeBits\nspark.io.encryption.keygen.algorithm\nspark.io.encryption.commons.config.*\nspark.ui.allowFramingFrom\nspark.ui.filters\nspark.acls.enable\nspark.admin.acls\nspark.admin.acls.groups\nspark.modify.acls\nspark.modify.acls.groups\nspark.ui.view.acls\nspark.ui.view.acls.groups\nspark.user.groups.mapping\nspark.history.ui.acls.enable\nspark.history.ui.admin.acls\nspark.history.ui.admin.acls.groups\nspark.ssl.enabled\nspark.ssl.port\nspark.ssl.enabledAlgorithms\nspark.ssl.keyPassword\nspark.ssl.keyStore\nspark.ssl.keyStorePassword\nspark.ssl.keyStoreType\nspark.ssl.protocol\nspark.ssl.needClientAuth\nspark.ssl.trustStore\nspark.ssl.trustStorePassword\nspark.ssl.trustStoreType\nspark.ssl.ui.enabled\nspark.ssl.ui.port\nspark.ssl.ui.enabledAlgorithms\nspark.ssl.ui.keyPassword\nspark.ssl.ui.keyStore\nspark.ssl.ui.keyStorePassword\nspark.ssl.ui.keyStoreType\nspark.ssl.ui.protocol\nspark.ssl.ui.needClientAuth\nspark.ssl.ui.trustStore\nspark.ssl.ui.trustStorePassword\nspark.ssl.ui.trustStoreType\nspark.ssl.standalone.enabled\nspark.ssl.standalone.port\nspark.ssl.standalone.enabledAlgorithms\nspark.ssl.standalone.keyPassword\nspark.ssl.standalone.keyStore\nspark.ssl.standalone.keyStorePassword\nspark.ssl.standalone.keyStoreType\nspark.ssl.standalone.protocol\nspark.ssl.standalone.needClientAuth\nspark.ssl.standalone.trustStore\nspark.ssl.standalone.trustStorePassword\nspark.ssl.standalone.trustStoreType\nspark.ssl.historyServer.enabled\nspark.ssl.historyServer.port\nspark.ssl.historyServer.enabledAlgorithms\nspark.ssl.historyServer.keyPassword\nspark.ssl.historyServer.keyStore\nspark.ssl.historyServer.keyStorePassword\nspark.ssl.historyServer.keyStoreType\nspark.ssl.historyServer.protocol\nspark.ssl.historyServer.needClientAuth\nspark.ssl.historyServer.trustStore\nspark.ssl.historyServer.trustStorePassword\nspark.ssl.historyServer.trustStoreType\nspark.ui.xXssProtection\nspark.ui.xContentTypeOptions.enabled\nspark.ui.strictTransportSecurity\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#data-sources","title":"Data Sources","text":"<p>The data sources are per above:</p> <ol> <li>https://spark.apache.org/docs/latest/configuration.html</li> <li>https://spark.apache.org/docs/latest/security.html</li> </ol> <p>Data Sources</p> <p>Copies of the html files are available in https://github.com/CyberSecAI/CyberSecAI.github.io/tree/main/data/NotebookLM_Config</p>"},{"location":"NotebookLM/NotebookLM_Config/#setup","title":"Setup","text":""},{"location":"NotebookLM/NotebookLM_Config/#prepare-validation-file","title":"Prepare Validation File","text":"<ol> <li>CopyNPaste the Expected answer to a text file ./data/NotebookLM_Config/security_parameters_manual.txt.</li> <li>Sort alphabetically to allow diff comparison with answer from NotebookLM.</li> </ol> <pre><code>    cat ./data/NotebookLM_Config/security_parameters_manual.txt | sort &gt; ./data/NotebookLM_Config/security_parameters_manual_sorted.txt\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#attempt-1-use-the-provided-prompt","title":"Attempt 1: Use the Provided Prompt","text":""},{"location":"NotebookLM/NotebookLM_Config/#create-a-new-notebooklm-with-the-2-data-sources-only","title":"Create A New Notebooklm With The 2 Data Sources Only","text":"<p>New NotebookLM. Sources - Upload from - Web page URL for the 2 Data Sources listed above.</p> <p></p>"},{"location":"NotebookLM/NotebookLM_Config/#submit-the-prompt","title":"Submit the prompt","text":"<p>Quote</p> <p>You are an assistant that is a cybersecurity expert and knows their configuration files. Your response here should be a JSON list of strings Using the documentation of Apache Spark, which configuration parameters are relevant from a security perspective?.</p>"},{"location":"NotebookLM/NotebookLM_Config/#save-the-result","title":"Save The Result","text":"<ol> <li>Click the Copy button.</li> <li>Create a new file security_parameters.json and save the result, then remove the ```` part at the beginning and end of the file so the file contains JSON only.</li> <li>Sort the answer</li> </ol> <pre><code>jq -r '.[]' ./data/NotebookLM_Config/security_parameters.json | sort &gt; ./data/NotebookLM_Config/security_parameters.txt\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#compare-the-answer-with-the-expected-answer","title":"Compare The Answer With The Expected Answer","text":"<ol> <li>60 parameters were retrieved</li> <li>We can see that the main difference relates to \"spark.ssl.\" parameters.</li> <li>Searching manually in the 2 Data Sources above reveals that these config strings are not actually listed in the documentation e.g. \"spark.ssl.ui.needClientAuth\" directly - but using placeholders.</li> <li>The LLM didn't understand that - so we'll let it know - then ask it again.</li> <li>diff data/NotebookLM_Config/security_parameters_manual.txt data/NotebookLM_Config/security_parameters.txt   </li> </ol> <pre><code>diff data/NotebookLM_Config/security_parameters_manual.txt data/NotebookLM_Config/security_parameters.txt                 \n1c1,4\n&lt; spark.yarn.shuffle.server.recovery.disabled\n---\n&gt; hadoop.security.credential.provider.path\n&gt; spark.acls.enable\n&gt; spark.admin.acls\n&gt; spark.admin.acls.groups\n2a6\n&gt; spark.authenticate.enableSaslEncryption\n4d7\n&lt; spark.authenticate.secret.file\n7,11c10,14\n&lt; spark.network.crypto.enabled\n&lt; spark.network.crypto.config.*\n&lt; spark.network.crypto.saslFallback\n&lt; spark.authenticate.enableSaslEncryption\n&lt; spark.network.sasl.serverAlwaysEncrypt\n---\n&gt; spark.authenticate.secret.file\n&gt; spark.history.ui.acls.enable\n&gt; spark.history.ui.admin.acls\n&gt; spark.history.ui.admin.acls.groups\n&gt; spark.io.encryption.commons.config.*\n15,20c18,29\n&lt; spark.io.encryption.commons.config.*\n&lt; spark.ui.allowFramingFrom\n&lt; spark.ui.filters\n&lt; spark.acls.enable\n&lt; spark.admin.acls\n&lt; spark.admin.acls.groups\n---\n&gt; spark.kerberos.access.hadoopFileSystems\n&gt; spark.kerberos.keytab\n&gt; spark.kerberos.principal\n&gt; spark.kubernetes.hadoop.configMapName\n&gt; spark.kubernetes.kerberos.krb5.configMapName\n&gt; spark.kubernetes.kerberos.krb5.path\n&gt; spark.kubernetes.kerberos.tokenSecret.itemKey\n&gt; spark.kubernetes.kerberos.tokenSecret.name\n&gt; spark.mesos.driver.secret.envkeys\n&gt; spark.mesos.driver.secret.filenames\n&gt; spark.mesos.driver.secret.names\n&gt; spark.mesos.driver.secret.values\n23,28c32,39\n&lt; spark.ui.view.acls\n&lt; spark.ui.view.acls.groups\n&lt; spark.user.groups.mapping\n&lt; spark.history.ui.acls.enable\n&lt; spark.history.ui.admin.acls\n&lt; spark.history.ui.admin.acls.groups\n---\n&gt; spark.network.crypto.config.*\n&gt; spark.network.crypto.enabled\n&gt; spark.network.crypto.saslFallback\n&gt; spark.network.sasl.serverAlwaysEncrypt\n&gt; spark.redaction.regex\n&gt; spark.redaction.string.regex\n&gt; spark.security.credentials.${service}.enabled\n&gt; spark.sql.redaction.options.regex\n30d40\n&lt; spark.ssl.port\n36d45\n&lt; spark.ssl.protocol\n37a47,48\n&gt; spark.ssl.port\n&gt; spark.ssl.protocol\n41,77c52,57\n&lt; spark.ssl.ui.enabled\n&lt; spark.ssl.ui.port\n&lt; spark.ssl.ui.enabledAlgorithms\n&lt; spark.ssl.ui.keyPassword\n&lt; spark.ssl.ui.keyStore\n&lt; spark.ssl.ui.keyStorePassword\n&lt; spark.ssl.ui.keyStoreType\n&lt; spark.ssl.ui.protocol\n&lt; spark.ssl.ui.needClientAuth\n&lt; spark.ssl.ui.trustStore\n&lt; spark.ssl.ui.trustStorePassword\n&lt; spark.ssl.ui.trustStoreType\n&lt; spark.ssl.standalone.enabled\n&lt; spark.ssl.standalone.port\n&lt; spark.ssl.standalone.enabledAlgorithms\n&lt; spark.ssl.standalone.keyPassword\n&lt; spark.ssl.standalone.keyStore\n&lt; spark.ssl.standalone.keyStorePassword\n&lt; spark.ssl.standalone.keyStoreType\n&lt; spark.ssl.standalone.protocol\n&lt; spark.ssl.standalone.needClientAuth\n&lt; spark.ssl.standalone.trustStore\n&lt; spark.ssl.standalone.trustStorePassword\n&lt; spark.ssl.standalone.trustStoreType\n&lt; spark.ssl.historyServer.enabled\n&lt; spark.ssl.historyServer.port\n&lt; spark.ssl.historyServer.enabledAlgorithms\n&lt; spark.ssl.historyServer.keyPassword\n&lt; spark.ssl.historyServer.keyStore\n&lt; spark.ssl.historyServer.keyStorePassword\n&lt; spark.ssl.historyServer.keyStoreType\n&lt; spark.ssl.historyServer.protocol\n&lt; spark.ssl.historyServer.needClientAuth\n&lt; spark.ssl.historyServer.trustStore\n&lt; spark.ssl.historyServer.trustStorePassword\n&lt; spark.ssl.historyServer.trustStoreType\n&lt; spark.ui.xXssProtection\n---\n&gt; spark.ssl.useNodeLocalConf\n&gt; spark.ui.allowFramingFrom\n&gt; spark.ui.filters\n&gt; spark.ui.strictTransportSecurity\n&gt; spark.ui.view.acls\n&gt; spark.ui.view.acls.groups\n79c59,60\n&lt; spark.ui.strictTransportSecurity\n\\ No newline at end of file\n---\n&gt; spark.ui.xXssProtection\n&gt; spark.user.groups.mapping\n</code></pre> <p>Note</p> <p>In the next section, we'll use an LLM to do the comparison.</p> <p>Here we used traditional methods i.e. diff.</p>"},{"location":"NotebookLM/NotebookLM_Config/#attempt-2-explain-about-config-namespace-placeholders","title":"Attempt 2: Explain about Config Namespace Placeholders","text":"<p>The LLM did not understand from the documents about Config Namespace placeholders:</p> <p></p> <p>So this time, we explain as part of the prompt about Config Namespace placeholders.</p>"},{"location":"NotebookLM/NotebookLM_Config/#submit-the-prompt_1","title":"Submit The Prompt","text":"<p>Quote</p> <p>Note that Config Namespace placeholders are used for some security configurations. These are placeholders \"\\({ns}\" that can take values listed under Config Namespace. For these security configurations placeholders list all possible security configurations for each Config Namespace e.g. For Property Name: \"\\).enabled\", \"spark.ssl.enabled\" and \"spark.ssl.historyServer\" would be some possible configurations now that you understand namespace placeholders, list all the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values. Your response here should be a JSON list of strings only. list the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values and repeated patterns</p> <p>Note</p> <p>Note the duplication in the prompt to emphasize what we want</p> <p>Quote</p> <p>\"list the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values and repeated patterns\" in the prompt.</p> <p>security_parameters_ns.json is the resulting file that has 96 config parameters - more than the expected answer config parameters as generated by humans.</p>"},{"location":"NotebookLM/NotebookLM_Config/#save-the-result_1","title":"Save The Result","text":"<ol> <li>Click the Copy button.</li> <li>Create a new file security_parameters_ns.json and save the result, then remove the ```` part at the beginning and end of the file so the file contains JSON only.</li> <li>Sort the answer and ensure there's no duplicates.</li> </ol> <pre><code>jq -r '.[]' ./data/NotebookLM_Config/security_parameters_ns.json | sort | uniq &gt; ./data/NotebookLM_Config/security_parameters_ns.txt\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#compare-the-answer-with-the-expected-answer_1","title":"Compare The Answer With The Expected Answer","text":"<p>In this case, we use ChatGPT4o to do the diff, copy and pasting the values from each file:</p> <ul> <li>./data/NotebookLM_Config/security_parameters_manual.txt: the expected answers</li> <li>./data/NotebookLM_Config/security_parameters_ns.txt: the actual answers</li> </ul>"},{"location":"NotebookLM/NotebookLM_Config/#prompt","title":"Prompt","text":"<pre><code>List the values that are in ===manual===  but not in ===notebooklm===\nList the values that are in ===notebooklm===  but not in ===manual===\n\n===manual=== \nspark.yarn.shuffle.server.recovery.disabled\nspark.authenticate\nspark.authenticate.secret\nspark.authenticate.secret.file\nspark.authenticate.secret.driver.file\nspark.authenticate.secret.executor.file\nspark.network.crypto.enabled\nspark.network.crypto.config.*\nspark.network.crypto.saslFallback\nspark.authenticate.enableSaslEncryption\nspark.network.sasl.serverAlwaysEncrypt\nspark.io.encryption.enabled\nspark.io.encryption.keySizeBits\nspark.io.encryption.keygen.algorithm\nspark.io.encryption.commons.config.*\nspark.ui.allowFramingFrom\nspark.ui.filters\nspark.acls.enable\nspark.admin.acls\nspark.admin.acls.groups\nspark.modify.acls\nspark.modify.acls.groups\nspark.ui.view.acls\nspark.ui.view.acls.groups\nspark.user.groups.mapping\nspark.history.ui.acls.enable\nspark.history.ui.admin.acls\nspark.history.ui.admin.acls.groups\nspark.ssl.enabled\nspark.ssl.port\nspark.ssl.enabledAlgorithms\nspark.ssl.keyPassword\nspark.ssl.keyStore\nspark.ssl.keyStorePassword\nspark.ssl.keyStoreType\nspark.ssl.protocol\nspark.ssl.needClientAuth\nspark.ssl.trustStore\nspark.ssl.trustStorePassword\nspark.ssl.trustStoreType\nspark.ssl.ui.enabled\nspark.ssl.ui.port\nspark.ssl.ui.enabledAlgorithms\nspark.ssl.ui.keyPassword\nspark.ssl.ui.keyStore\nspark.ssl.ui.keyStorePassword\nspark.ssl.ui.keyStoreType\nspark.ssl.ui.protocol\nspark.ssl.ui.needClientAuth\nspark.ssl.ui.trustStore\nspark.ssl.ui.trustStorePassword\nspark.ssl.ui.trustStoreType\nspark.ssl.standalone.enabled\nspark.ssl.standalone.port\nspark.ssl.standalone.enabledAlgorithms\nspark.ssl.standalone.keyPassword\nspark.ssl.standalone.keyStore\nspark.ssl.standalone.keyStorePassword\nspark.ssl.standalone.keyStoreType\nspark.ssl.standalone.protocol\nspark.ssl.standalone.needClientAuth\nspark.ssl.standalone.trustStore\nspark.ssl.standalone.trustStorePassword\nspark.ssl.standalone.trustStoreType\nspark.ssl.historyServer.enabled\nspark.ssl.historyServer.port\nspark.ssl.historyServer.enabledAlgorithms\nspark.ssl.historyServer.keyPassword\nspark.ssl.historyServer.keyStore\nspark.ssl.historyServer.keyStorePassword\nspark.ssl.historyServer.keyStoreType\nspark.ssl.historyServer.protocol\nspark.ssl.historyServer.needClientAuth\nspark.ssl.historyServer.trustStore\nspark.ssl.historyServer.trustStorePassword\nspark.ssl.historyServer.trustStoreType\nspark.ui.xXssProtection\nspark.ui.xContentTypeOptions.enabled\nspark.ui.strictTransportSecurity\n\n\n===notebooklm===\nhadoop.security.credential.provider.path\nspark.acls.enable\nspark.admin.acls\nspark.admin.acls.groups\nspark.authenticate\nspark.authenticate.enableSaslEncryption\nspark.authenticate.secret\nspark.authenticate.secret.driver.file\nspark.authenticate.secret.executor.file\nspark.authenticate.secret.file\nspark.history.ui.acls.enable\nspark.history.ui.admin.acls\nspark.history.ui.admin.acls.groups\nspark.io.encryption.commons.config.*\nspark.io.encryption.enabled\nspark.io.encryption.keySizeBits\nspark.io.encryption.keygen.algorithm\nspark.kerberos.access.hadoopFileSystems\nspark.kerberos.keytab\nspark.kerberos.principal\nspark.kubernetes.hadoop.configMapName\nspark.kubernetes.kerberos.krb5.configMapName\nspark.kubernetes.kerberos.krb5.path\nspark.kubernetes.kerberos.tokenSecret.itemKey\nspark.kubernetes.kerberos.tokenSecret.name\nspark.mesos.driver.secret.envkeys\nspark.mesos.driver.secret.filenames\nspark.mesos.driver.secret.names\nspark.mesos.driver.secret.values\nspark.modify.acls\nspark.modify.acls.groups\nspark.network.crypto.config.*\nspark.network.crypto.enabled\nspark.network.crypto.saslFallback\nspark.network.sasl.serverAlwaysEncrypt\nspark.redaction.regex\nspark.redaction.string.regex\nspark.security.credentials.${service}.enabled\nspark.sql.redaction.options.regex\nspark.ssl.enabled\nspark.ssl.enabledAlgorithms\nspark.ssl.historyServer.enabled\nspark.ssl.historyServer.enabledAlgorithms\nspark.ssl.historyServer.keyPassword\nspark.ssl.historyServer.keyStore\nspark.ssl.historyServer.keyStorePassword\nspark.ssl.historyServer.keyStoreType\nspark.ssl.historyServer.needClientAuth\nspark.ssl.historyServer.port\nspark.ssl.historyServer.protocol\nspark.ssl.historyServer.trustStore\nspark.ssl.historyServer.trustStorePassword\nspark.ssl.historyServer.trustStoreType\nspark.ssl.keyPassword\nspark.ssl.keyStore\nspark.ssl.keyStorePassword\nspark.ssl.keyStoreType\nspark.ssl.needClientAuth\nspark.ssl.port\nspark.ssl.protocol\nspark.ssl.standalone.enabled\nspark.ssl.standalone.enabledAlgorithms\nspark.ssl.standalone.keyPassword\nspark.ssl.standalone.keyStore\nspark.ssl.standalone.keyStorePassword\nspark.ssl.standalone.keyStoreType\nspark.ssl.standalone.needClientAuth\nspark.ssl.standalone.port\nspark.ssl.standalone.protocol\nspark.ssl.standalone.trustStore\nspark.ssl.standalone.trustStorePassword\nspark.ssl.standalone.trustStoreType\nspark.ssl.trustStore\nspark.ssl.trustStorePassword\nspark.ssl.trustStoreType\nspark.ssl.ui.enabled\nspark.ssl.ui.enabledAlgorithms\nspark.ssl.ui.keyPassword\nspark.ssl.ui.keyStore\nspark.ssl.ui.keyStorePassword\nspark.ssl.ui.keyStoreType\nspark.ssl.ui.needClientAuth\nspark.ssl.ui.port\nspark.ssl.ui.protocol\nspark.ssl.ui.trustStore\nspark.ssl.ui.trustStorePassword\nspark.ssl.ui.trustStoreType\nspark.ssl.useNodeLocalConf\nspark.ui.allowFramingFrom\nspark.ui.filters\nspark.ui.strictTransportSecurity\nspark.ui.view.acls\nspark.ui.view.acls.groups\nspark.ui.xContentTypeOptions.enabled\nspark.ui.xXssProtection\nspark.user.groups.mapping\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#response","title":"Response","text":"<pre><code>Values in ===manual=== but not in ===notebooklm===\nspark.yarn.shuffle.server.recovery.disabled\nValues in ===notebooklm=== but not in ===manual===\nhadoop.security.credential.provider.path\nspark.kerberos.access.hadoopFileSystems\nspark.kerberos.keytab\nspark.kerberos.principal\nspark.kubernetes.hadoop.configMapName\nspark.kubernetes.kerberos.krb5.configMapName\nspark.kubernetes.kerberos.krb5.path\nspark.kubernetes.kerberos.tokenSecret.itemKey\nspark.kubernetes.kerberos.tokenSecret.name\nspark.mesos.driver.secret.envkeys\nspark.mesos.driver.secret.filenames\nspark.mesos.driver.secret.names\nspark.mesos.driver.secret.values\nspark.redaction.regex\nspark.redaction.string.regex\nspark.security.credentials.${service}.enabled\nspark.sql.redaction.options.regex\nspark.ssl.useNodeLocalConf\n</code></pre> <p>Observation</p> <ol> <li>NotebookLM did not understand the concept of placeholder config parameters from the source documents. Since there are several config parameters per placeholder, it missed many config parameters (Attempt 1).</li> <li>When the prompt explained about placeholder config parameters, NotebookLM found all but 1 of the 79 configuration parameters related to security (Attempt 2).<ol> <li>NotebookLM found 18 config parameters related to security that were not found by humans as part of the expected answers and these appear to be valid configuration parameters related to security based on my review.</li> </ol> </li> <li>1 config parameter was not found by NotebookLM, and it's not obvious why: <code>spark.yarn.shuffle.server.recovery.disabled</code> </li> </ol>"},{"location":"NotebookLM/NotebookLM_Config/#chatgpt4o","title":"ChatGPT4o","text":""},{"location":"NotebookLM/NotebookLM_Config/#original-prompt_1","title":"Original Prompt","text":"<pre><code>You are an assistant that is a cybersecurity expert and knows their configuration files. Your response here should be a JSON list of strings\nUsing only the documentation of Apache Spark from the URLs below (and no other source), which configuration parameters are relevant from a security perspective?\n\nThe data sources are :\n1. https://spark.apache.org/docs/latest/configuration.html\n2. https://spark.apache.org/docs/latest/security.html\n</code></pre> <pre><code>[\n    \"spark.authenticate\",\n    \"spark.authenticate.secret\",\n    \"spark.authenticate.secret.file\",\n    \"spark.authenticate.secret.driver.file\",\n    \"spark.authenticate.secret.executor.file\",\n    \"spark.network.crypto.enabled\",\n    \"spark.network.crypto.config.*\",\n    \"spark.network.crypto.saslFallback\",\n    \"spark.authenticate.enableSaslEncryption\",\n    \"spark.network.sasl.serverAlwaysEncrypt\",\n    \"spark.io.encryption.enabled\",\n    \"spark.io.encryption.keySizeBits\",\n    \"spark.io.encryption.keygen.algorithm\",\n    \"spark.io.encryption.commons.config.*\",\n    \"spark.ui.allowFramingFrom\",\n    \"spark.ui.filters\",\n    \"spark.acls.enable\",\n    \"spark.admin.acls\",\n    \"spark.admin.acls.groups\",\n    \"spark.modify.acls\",\n    \"spark.modify.acls.groups\",\n    \"spark.ui.view.acls\",\n    \"spark.ui.view.acls.groups\",\n    \"spark.user.groups.mapping\",\n    \"spark.history.ui.acls.enable\",\n    \"spark.history.ui.admin.acls\",\n    \"spark.history.ui.admin.acls.groups\",\n    \"spark.ssl.enabled\",\n    \"spark.ssl.port\",\n    \"spark.ssl.enabledAlgorithms\",\n    \"spark.ssl.keyPassword\",\n    \"spark.ssl.keyStore\",\n    \"spark.ssl.keyStorePassword\",\n    \"spark.ssl.keyStoreType\",\n    \"spark.ssl.protocol\",\n    \"spark.ssl.needClientAuth\",\n    \"spark.ssl.trustStore\",\n    \"spark.ssl.trustStorePassword\",\n    \"spark.ssl.trustStoreType\",\n    \"spark.ui.xXssProtection\",\n    \"spark.ui.xContentTypeOptions.enabled\",\n    \"spark.ui.strictTransportSecurity\",\n    \"spark.kerberos.access.hadoopFileSystems\",\n    \"spark.security.credentials.${service}.enabled\"\n]\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#prompt-explaining-config-namespace-placeholders","title":"Prompt Explaining Config Namespace placeholders","text":"<pre><code>Note that Config Namespace placeholders are used for some security configurations. These are placeholders \"${ns}\" that can take values listed under Config Namespace. For these security configurations placeholders list all possible security configurations for each Config Namespace e.g. For Property Name: \"${ns}.enabled\", \"spark.ssl.enabled\" and \"spark.ssl.historyServer\" would be some possible configurations now that you understand namespace placeholders, list all the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values. Your response here should be a JSON list of strings only. list the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values and repeated patterns\n</code></pre> <pre><code>[\n    \"spark.ssl.enabled\",\n    \"spark.ssl.historyServer.enabled\",\n    \"spark.security.credentials.enabled\",\n    \"spark.ssl.port\",\n    \"spark.ssl.historyServer.port\",\n    \"spark.security.credentials.port\",\n    \"spark.ssl.enabledAlgorithms\",\n    \"spark.ssl.historyServer.enabledAlgorithms\",\n    \"spark.security.credentials.enabledAlgorithms\",\n    \"spark.ssl.keyPassword\",\n    \"spark.ssl.historyServer.keyPassword\",\n    \"spark.security.credentials.keyPassword\",\n    \"spark.ssl.keyStore\",\n    \"spark.ssl.historyServer.keyStore\",\n    \"spark.security.credentials.keyStore\",\n    \"spark.ssl.keyStorePassword\",\n    \"spark.ssl.historyServer.keyStorePassword\",\n    \"spark.security.credentials.keyStorePassword\",\n    \"spark.ssl.keyStoreType\",\n    \"spark.ssl.historyServer.keyStoreType\",\n    \"spark.security.credentials.keyStoreType\",\n    \"spark.ssl.protocol\",\n    \"spark.ssl.historyServer.protocol\",\n    \"spark.security.credentials.protocol\",\n    \"spark.ssl.needClientAuth\",\n    \"spark.ssl.historyServer.needClientAuth\",\n    \"spark.security.credentials.needClientAuth\",\n    \"spark.ssl.trustStore\",\n    \"spark.ssl.historyServer.trustStore\",\n    \"spark.security.credentials.trustStore\",\n    \"spark.ssl.trustStorePassword\",\n    \"spark.ssl.historyServer.trustStorePassword\",\n    \"spark.security.credentials.trustStorePassword\",\n    \"spark.ssl.trustStoreType\",\n    \"spark.ssl.historyServer.trustStoreType\",\n    \"spark.security.credentials.trustStoreType\",\n    \"spark.ssl.historyServer\",\n    \"spark.ssl.historyServer.historyServer\",\n    \"spark.security.credentials.historyServer\",\n    \"spark.authenticate\",\n    \"spark.authenticate.secret\",\n    \"spark.authenticate.secret.file\",\n    \"spark.authenticate.secret.driver.file\",\n    \"spark.authenticate.secret.executor.file\",\n    \"spark.network.crypto.enabled\",\n    \"spark.network.crypto.config.*\",\n    \"spark.network.crypto.saslFallback\",\n    \"spark.authenticate.enableSaslEncryption\",\n    \"spark.network.sasl.serverAlwaysEncrypt\",\n    \"spark.io.encryption.enabled\",\n    \"spark.io.encryption.keySizeBits\",\n    \"spark.io.encryption.keygen.algorithm\",\n    \"spark.io.encryption.commons.config.*\",\n    \"spark.ui.allowFramingFrom\",\n    \"spark.ui.filters\",\n    \"spark.acls.enable\",\n    \"spark.admin.acls\",\n    \"spark.admin.acls.groups\",\n    \"spark.modify.acls\",\n    \"spark.modify.acls.groups\",\n    \"spark.ui.view.acls\",\n    \"spark.ui.view.acls.groups\",\n    \"spark.user.groups.mapping\",\n    \"spark.history.ui.acls.enable\",\n    \"spark.history.ui.admin.acls\",\n    \"spark.history.ui.admin.acls.groups\",\n    \"spark.ui.xXssProtection\",\n    \"spark.ui.xContentTypeOptions.enabled\",\n    \"spark.ui.strictTransportSecurity\",\n    \"spark.kerberos.access.hadoopFileSystems\",\n    \"spark.security.credentials.${service}.enabled\"\n]\n``` &amp;#8203;:citation[oaicite:0]{index=0}&amp;#8203;\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#compare-the-answer-with-the-expected-answer_2","title":"Compare The Answer With The Expected Answer","text":"<ol> <li>ChatGPT4o missed these 2 config placeholders (these were not provided in the example in the prompt - only the first and last one were provided)<ol> <li>spark.ssl.ui</li> <li>spark.ssl.standalone   </li> </ol> </li> <li>ChatGPT4o wrongly interpreted spark.security.credentials.${service}.enabled as being the same as ${ns} placeholder, and appended the parameters for ${ns} placeholder to spark.security.credentials.</li> </ol> <pre><code>{\n    \"manual_not_in_chatgpt4o\": [\n        \"spark.yarn.shuffle.server.recovery.disabled\",\n        \"spark.ssl.ui.enabled\",\n        \"spark.ssl.ui.port\",\n        \"spark.ssl.ui.enabledAlgorithms\",\n        \"spark.ssl.ui.keyPassword\",\n        \"spark.ssl.ui.keyStore\",\n        \"spark.ssl.ui.keyStorePassword\",\n        \"spark.ssl.ui.keyStoreType\",\n        \"spark.ssl.ui.protocol\",\n        \"spark.ssl.ui.needClientAuth\",\n        \"spark.ssl.ui.trustStore\",\n        \"spark.ssl.ui.trustStorePassword\",\n        \"spark.ssl.ui.trustStoreType\",\n        \"spark.ssl.standalone.enabled\",\n        \"spark.ssl.standalone.port\",\n        \"spark.ssl.standalone.enabledAlgorithms\",\n        \"spark.ssl.standalone.keyPassword\",\n        \"spark.ssl.standalone.keyStore\",\n        \"spark.ssl.standalone.keyStorePassword\",\n        \"spark.ssl.standalone.keyStoreType\",\n        \"spark.ssl.standalone.protocol\",\n        \"spark.ssl.standalone.needClientAuth\",\n        \"spark.ssl.standalone.trustStore\",\n        \"spark.ssl.standalone.trustStorePassword\",\n        \"spark.ssl.standalone.trustStoreType\"\n    ],\n    \"chatgpt4o_not_in_manual\": [\n        \"spark.security.credentials.enabled\",\n        \"spark.security.credentials.port\",\n        \"spark.security.credentials.enabledAlgorithms\",\n        \"spark.security.credentials.keyPassword\",\n        \"spark.security.credentials.keyStore\",\n        \"spark.security.credentials.keyStorePassword\",\n        \"spark.security.credentials.keyStoreType\",\n        \"spark.security.credentials.protocol\",\n        \"spark.security.credentials.needClientAuth\",\n        \"spark.security.credentials.trustStore\",\n        \"spark.security.credentials.trustStorePassword\",\n        \"spark.security.credentials.trustStoreType\",\n        \"spark.security.credentials.historyServer\",\n        \"spark.security.credentials.${service}.enabled\"\n    ]\n}\n</code></pre> <p>Observation</p> <ol> <li><code>spark.yarn.shuffle.server.recovery.disabled</code> was not found by ChatGPT4o (or NotebookLM).     </li> <li>ChatGPT4o found 44 parameters in the first prompt.</li> <li>When the concept of placeholder config parameters was explained in the second prompt, ChatGPT made 2 mistakes and thereby got many config parameters wrong<ol> <li>it did not extend the example to all the placeholder config parameters</li> <li>it appended the {ns} placeholder parameters to a different placeholder {service}</li> </ol> </li> </ol>"},{"location":"NotebookLM/NotebookLM_Config/#claude-sonnet-35","title":"Claude Sonnet 3.5","text":"<p>Note</p> <p>Claude Sonnet 3.5 Pro was used as both documents were too large for the free version.</p>"},{"location":"NotebookLM/NotebookLM_Config/#original-prompt_2","title":"Original Prompt","text":"<pre><code>You are an assistant that is a cybersecurity expert and knows their configuration files. Your response here should be a JSON list of strings\nUsing only the documentation of Apache Spark from the URLs below (and no other source), which configuration parameters are relevant from a security perspective?\n\nThe data sources are :\n1. https://spark.apache.org/docs/latest/configuration.html\n2. https://spark.apache.org/docs/latest/security.html\n</code></pre> <pre><code>Based on my analysis of the Apache Spark documentation, here is a JSON list of configuration parameters that are relevant from a security perspective:\n[\n\"spark.authenticate\",\n\"spark.authenticate.secret\",\n\"spark.network.crypto.enabled\",\n\"spark.network.crypto.saslFallback\",\n\"spark.authenticate.enableSaslEncryption\",\n\"spark.network.sasl.serverAlwaysEncrypt\",\n\"spark.io.encryption.enabled\",\n\"spark.io.encryption.keySizeBits\",\n\"spark.io.encryption.keygen.algorithm\",\n\"spark.ssl.enabled\",\n\"spark.ssl.keyStore\",\n\"spark.ssl.keyStorePassword\",\n\"spark.ssl.keyPassword\",\n\"spark.ssl.trustStore\",\n\"spark.ssl.trustStorePassword\",\n\"spark.ssl.protocol\",\n\"spark.ssl.enabledAlgorithms\",\n\"spark.ui.filters\",\n\"spark.acls.enable\",\n\"spark.admin.acls\",\n\"spark.admin.acls.groups\",\n\"spark.modify.acls\",\n\"spark.modify.acls.groups\",\n\"spark.ui.view.acls\",\n\"spark.ui.view.acls.groups\",\n\"spark.eventLog.enabled\",\n\"spark.eventLog.dir\",\n\"spark.history.kerberos.enabled\",\n\"spark.history.kerberos.principal\",\n\"spark.history.kerberos.keytab\",\n\"spark.kerberos.principal\",\n\"spark.kerberos.keytab\",\n\"spark.hadoop.security.authentication\",\n\"spark.yarn.security.tokens.hbase.enabled\",\n\"spark.yarn.security.tokens.hive.enabled\",\n\"spark.yarn.security.credentials.hadoop.enabled\",\n\"spark.redaction.regex\",\n\"spark.ui.proxyRedirectUri\",\n\"spark.driver.log.dfsDir\"\n]\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#prompt-explaining-config-namespace-placeholders_1","title":"Prompt Explaining Config Namespace placeholders","text":"<pre><code>Note that Config Namespace placeholders are used for some security configurations. These are placeholders \"${ns}\" that can take values listed under Config Namespace. For these security configurations placeholders list all possible security configurations for each Config Namespace e.g. For Property Name: \"${ns}.enabled\", \"spark.ssl.enabled\" and \"spark.ssl.historyServer\" would be some possible configurations now that you understand namespace placeholders, list all the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values. Your response here should be a JSON list of strings only. list the configuration parameters that are relevant from a security perspective replacing all placeholders with possible config namespace values and repeated patterns\n</code></pre> <pre><code>I understand. Here's a revised JSON list of configuration parameters relevant from a security perspective, with namespace placeholders expanded and repeated patterns included:\n[\n\"spark.authenticate\",\n\"spark.authenticate.secret\",\n\"spark.network.crypto.enabled\",\n\"spark.network.crypto.saslFallback\",\n\"spark.authenticate.enableSaslEncryption\",\n\"spark.network.sasl.serverAlwaysEncrypt\",\n\"spark.io.encryption.enabled\",\n\"spark.io.encryption.keySizeBits\",\n\"spark.io.encryption.keygen.algorithm\",\n\"spark.ssl.enabled\",\n\"spark.ssl.ui.enabled\",\n\"spark.ssl.standalone.enabled\",\n\"spark.ssl.historyServer.enabled\",\n\"spark.ssl.keyStore\",\n\"spark.ssl.ui.keyStore\",\n\"spark.ssl.standalone.keyStore\",\n\"spark.ssl.historyServer.keyStore\",\n\"spark.ssl.keyStorePassword\",\n\"spark.ssl.ui.keyStorePassword\",\n\"spark.ssl.standalone.keyStorePassword\",\n\"spark.ssl.historyServer.keyStorePassword\",\n\"spark.ssl.keyPassword\",\n\"spark.ssl.ui.keyPassword\",\n\"spark.ssl.standalone.keyPassword\",\n\"spark.ssl.historyServer.keyPassword\",\n\"spark.ssl.trustStore\",\n\"spark.ssl.ui.trustStore\",\n\"spark.ssl.standalone.trustStore\",\n\"spark.ssl.historyServer.trustStore\",\n\"spark.ssl.trustStorePassword\",\n\"spark.ssl.ui.trustStorePassword\",\n\"spark.ssl.standalone.trustStorePassword\",\n\"spark.ssl.historyServer.trustStorePassword\",\n\"spark.ssl.protocol\",\n\"spark.ssl.ui.protocol\",\n\"spark.ssl.standalone.protocol\",\n\"spark.ssl.historyServer.protocol\",\n\"spark.ssl.enabledAlgorithms\",\n\"spark.ssl.ui.enabledAlgorithms\",\n\"spark.ssl.standalone.enabledAlgorithms\",\n\"spark.ssl.historyServer.enabledAlgorithms\",\n\"spark.ui.filters\",\n\"spark.acls.enable\",\n\"spark.admin.acls\",\n\"spark.admin.acls.groups\",\n\"spark.modify.acls\",\n\"spark.modify.acls.groups\",\n\"spark.ui.view.acls\",\n\"spark.ui.view.acls.groups\",\n\"spark.eventLog.enabled\",\n\"spark.eventLog.dir\",\n\"spark.history.kerberos.enabled\",\n\"spark.history.kerberos.principal\",\n\"spark.history.kerberos.keytab\",\n\"spark.kerberos.principal\",\n\"spark.kerberos.keytab\",\n\"spark.hadoop.security.authentication\",\n\"spark.yarn.security.tokens.hbase.enabled\",\n\"spark.yarn.security.tokens.hive.enabled\",\n\"spark.yarn.security.credentials.hadoop.enabled\",\n\"spark.redaction.regex\",\n\"spark.ui.proxyRedirectUri\",\n\"spark.driver.log.dfsDir\",\n\"spark.ssl.keyStoreType\",\n\"spark.ssl.ui.keyStoreType\",\n\"spark.ssl.standalone.keyStoreType\",\n\"spark.ssl.historyServer.keyStoreType\",\n\"spark.ssl.trustStoreType\",\n\"spark.ssl.ui.trustStoreType\",\n\"spark.ssl.standalone.trustStoreType\",\n\"spark.ssl.historyServer.trustStoreType\",\n\"spark.ssl.needClientAuth\",\n\"spark.ssl.ui.needClientAuth\",\n\"spark.ssl.standalone.needClientAuth\",\n\"spark.ssl.historyServer.needClientAuth\",\n\"spark.ui.allowFramingFrom\",\n\"spark.ui.xXssProtection\",\n\"spark.ui.xContentTypeOptions.enabled\",\n\"spark.ui.strictTransportSecurity\"\n]\n</code></pre>"},{"location":"NotebookLM/NotebookLM_Config/#compare-the-answer-with-the-expected-answer_3","title":"Compare The Answer With The Expected Answer","text":"<pre><code>Values in ===manual=== but not in ===claude35===:\n\nspark.yarn.shuffle.server.recovery.disabled\nspark.authenticate.secret.file\nspark.authenticate.secret.driver.file\nspark.authenticate.secret.executor.file\nspark.network.crypto.config.*\nspark.io.encryption.commons.config.*\nspark.user.groups.mapping\nspark.history.ui.acls.enable\nspark.history.ui.admin.acls\nspark.history.ui.admin.acls.groups\nspark.ssl.port\nspark.ssl.ui.port\nspark.ssl.standalone.port\nspark.ssl.historyServer.port\n\nValues in ===claude35=== but not in ===manual===:\n\nspark.eventLog.enabled\nspark.eventLog.dir\nspark.history.kerberos.enabled\nspark.history.kerberos.principal\nspark.history.kerberos.keytab\nspark.kerberos.principal\nspark.kerberos.keytab\nspark.hadoop.security.authentication\nspark.yarn.security.tokens.hbase.enabled\nspark.yarn.security.tokens.hive.enabled\nspark.yarn.security.credentials.hadoop.enabled\nspark.redaction.regex\nspark.ui.proxyRedirectUri\nspark.driver.log.dfsDir\n</code></pre> <p>Observation</p> <ol> <li><code>spark.yarn.shuffle.server.recovery.disabled</code> was not found by Claude3.5 (or ChatGPT4o or NotebookLM).     </li> <li>Claude3.5 found 39 parameters in the first prompt.</li> <li>When the concept of placeholder config parameters was explained in the second prompt, Claude3.5 understood it.</li> <li>Claude3.5 found 14 parameters more than the human-generated answer, including several parameters that the other LLMs did not find.</li> </ol>"},{"location":"NotebookLM/NotebookLM_Config/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>NotebookLM, ChatGPT4o and Claude3.5 Pro did reasonably well at extracting the config parameters related to security</li> <li>NotebookLM performed best, missing just 1, and finding 18 more than the human-generated answer.</li> <li>Claude3.5 found 79 parameters, missed 14, and found 14 more than the human-generated answer.</li> <li>ChatGPT 4o found 71 parameters.</li> <li>Overall, with everything-as-code (infrastructure, policy, LLM answers, ....), and LLMs being able to process code, there's a lot of benefit and promise in applying LLMs.</li> <li>The feedback to CoGuard is via https://github.com/coguardio/coguard_openai_rule_auto_generation_research/issues/2 </li> </ol>"},{"location":"NotebookLM/NotebookLM_Secure_Code/","title":"NotebookLM Secure Code","text":"<p>Overview</p> <p>In two separate conversations recently, the topic of using LLMs for secure coding came up. One of the concerns that is often raised is that GenAI Code is not secure because GenAI is trained on arbitrary code on the internet.</p> <p>I was curious how NotebookLM would work for generating or reviewing secure code i.e. a closed system that has been provided a lot of guidance on secure code (and not arbitrary examples).</p> <p>Claude Sonnet 3.5 was also used for comparison.</p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#vulnerability-types","title":"Vulnerability Types","text":"<p>Secure Programming with Static Analysis classifies vulnerability types as follows:</p> <p></p> <p>LLMs go beyond understanding syntax to understanding semantics and may be effective in the 3 quadrants that traditional static analysis isn't.</p> <p>But in this simple test case below, the focus is on Generic defects visible in the code, as an initial proof of concept.</p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#data-sources","title":"Data Sources","text":"<p>Two books I had on Java were loaded to NotebookLM:</p> <ol> <li>The CERT Oracle Secure Coding Standard for Java<ol> <li>The same material is available on https://wiki.sei.cmu.edu/confluence/display/java/SEI+CERT+Oracle+Coding+Standard+for+Java</li> </ol> </li> <li>Java Coding Guidelines: 75 Recommendations for Reliable and Secure Programs</li> </ol>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#test-data","title":"Test Data","text":"<p>NIST Software Assurance Reference Dataset (SARD) was used as the test dataset.</p> <p>Quote</p> <p>The Software Assurance Reference Dataset (SARD) is a growing collection of test programs with documented weaknesses. Test cases vary from small synthetic programs to large applications. The programs are in C, C++, Java, PHP, and C#, and cover over 150 classes of weaknesses.</p> <p>e.g. CWE: 191 Integer Underflow https://samate.nist.gov/SARD/test-cases/252126/versions/1.0.0#4</p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#setup","title":"Setup","text":"<ol> <li>Import both PDFs into a new NotebookLM.</li> </ol>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#test","title":"Test","text":""},{"location":"NotebookLM/NotebookLM_Secure_Code/#test-code-cwe-191-integer-underflow","title":"Test Code CWE: 191 Integer Underflow","text":"<p> https://samate.nist.gov/SARD/test-cases/252126/versions/1.0.0#4</p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#review-test-code","title":"Review Test Code","text":"<p>Comments are removed from https://samate.nist.gov/SARD/test-cases/252126/versions/1.0.0#4 so the code fits in the prompt window.</p> <p></p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#claude-35","title":"Claude 3.5","text":""},{"location":"NotebookLM/NotebookLM_Secure_Code/#generate-code-write-secure-code-to-multiply-2-numbers","title":"Generate Code: Write Secure Code to Multiply 2 numbers","text":""},{"location":"NotebookLM/NotebookLM_Secure_Code/#use-biginteger-instead","title":"Use BigInteger Instead","text":""},{"location":"NotebookLM/NotebookLM_Secure_Code/#claude-35_1","title":"Claude 3.5","text":""},{"location":"NotebookLM/NotebookLM_Secure_Code/#llama-31-405b-code-training","title":"Llama 3.1 405B Code Training","text":"<p>Llama 3.1 405B was released July 2024.</p> <p>The training process to generate good code is described in https://www.deeplearning.ai/the-batch/issue-260/.</p> <p>Quote</p> <p>The pretrained model was fine-tuned to perform seven tasks, including coding and reasoning, via supervised learning and direct preference optimization (DPO). Most of the fine-tuning data was generated by the model itself and curated using a variety of methods including agentic workflows. For instance,</p> <p>To generate good code to learn from, the team: </p> <ol> <li>Generated programming problems from random code snippets. </li> <li>Generated a solution to each problem, prompting the model to follow good programming practices and explain its thought process in comments. </li> <li>Ran the generated code through a parser and linter to check for issues like syntax errors, style issues, and uninitialized variables. </li> <li>Generated unit tests. </li> <li>Tested the code on the unit tests. </li> <li>If there were any issues, regenerated the code, giving the model the original question, code, and feedback. </li> <li>If the code passed all tests, added it to the dataset. </li> <li>Fine-tuned the model. </li> <li>Repeated this process several times.</li> </ol> <p>https://www.deeplearning.ai/the-batch/issue-260/</p>"},{"location":"NotebookLM/NotebookLM_Secure_Code/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>NotebookLM with 2 Secure Code Java references performed well in these simple test cases.</li> <li>LLMs in conjunction with traditional code assurance tools can be used to \"generate good code\".</li> </ol>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/","title":"NotebookLM Vulnerability Standards","text":"<p>Overview</p> <p>In this chapter, we'll feed the main vulnerability standards to NotebookLM.</p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#data-sources","title":"Data Sources","text":"<p>These files are available in docs/NotebookLM/InputDocs for convenience.</p> Content Page with link to content Direct Link Risk Based Prioritization Guide https://riskbasedprioritization.github.io/print_page/ Print - Save to PDF CVSS 3.1 User Guide https://www.first.org/cvss/v3.1/user-guide https://www.first.org/cvss/v3-1/cvss-v31-user-guide_r1.pdf CVSS 3.1 Specification https://www.first.org/cvss/v3.1/specification-document https://www.first.org/cvss/v3-1/cvss-v31-specification_r1.pdf CVSS 4.0 User Guide https://www.first.org/cvss/v4.0/user-guide https://www.first.org/cvss/v4-0/cvss-v40-user-guide.pdf CVSS 4.0 Specification Document https://www.first.org/cvss/v4.0/specification-document https://www.first.org/cvss/v4-0/cvss-v40-specification.pdf Exploit Prediction Scoring System (EPSS) FAQ https://www.first.org/epss/faq https://www.first.org/epss/faq Exploit Prediction Scoring System (EPSS) latest published paper https://www.first.org/epss/model https://arxiv.org/pdf/2302.14172 Stakeholder-Specific Vulnerability Categorization (SSVC) latest published paper https://github.com/CERTCC/SSVC/tree/main/pdfs https://github.com/CERTCC/SSVC/blob/main/pdfs/ssvc_2_1_draft.pdf CISA KEV https://www.cisa.gov/known-exploited-vulnerabilities copy and paste text to a file cisakev.txt because NotebookLM cannot pull content directly from the page <p>Data Sources</p> <p>The vulnerability standards PDFs are available in https://github.com/CyberSecAI/CyberSecAI.github.io/tree/main/data/NotebookLM_VulnerabilityStandards</p> <p></p> The initial screen after the documents are loaded.CISA KEV was then loaded as a text file. <p>Quote</p> <p>write an introductory article in less than 250 words that gives an overview of all these sources and how they fit together</p> <p></p> <p>Each circled-number is a clickable link to the reference text in the original sources.</p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#i-want-to-rank-vulnerabilities-by-exploitation","title":"I want to rank vulnerabilities by Exploitation","text":"<p>Quote</p> <p>I am currently using CVSS to rank vulnerabilities. I also want to rank based on exploitation as I heard only a small percentage of CVEs are actually exploited. What standard should I use. How should I use them. Provide an answer with rationale and examples.</p> <p></p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#explain-risk-like-im-a-5-year-old","title":"Explain risk like I'm a 5 year old","text":"<p>Quote</p> <p>I want to prioritize vulnerabilities by risk. But I don't understand risk and the components or factors of risk. Can you explain that to me like I'm a 5 year old. Then explain how I can use that for my risk based prioritization approach. Please provide a rationale</p> <p></p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#cvss-base-parameters-for-cvss-v3-and-v4","title":"CVSS Base Parameters for CVSS v3 and v4","text":"<p>Quote</p> <p>what parameters contribute to the CVSS base parameter score? How do these parameters differ in CVSS 3 versus CVSS 4</p> <p></p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#chatgpt4o-response","title":"ChatGPT4o Response","text":"<p>Failure</p> <p>Hallucination Remediation Level (RL) is NOT part of the Exploitability Metrics</p> <p>Quote</p> <p>Remediation Level (RL): New metric considering how available and effective remediation methods are (Not Defined, Official Fix, Temporary Fix, Workaround, Unavailable).</p>"},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#chatgpt4o-check-response","title":"ChatGPT4o Check Response","text":""},{"location":"NotebookLM/NotebookLM_VulnerabilityStandards/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>NotebookLM does a good job assimilating these verbose standards and was competently able to answer specific factual questions, and broader opinionated questions.</li> <li>Lots of references to the source material were provided in the answers which help understand and validate the answers.</li> </ol>"},{"location":"TopicModeling/BertTopic/","title":"BERTopic","text":"<p>Overview</p> <p>Topic modeling can be used to discover the underlying themes and patterns in a collection of documents (e.g. CVE Descriptions, or CWE Descriptions,...).</p> <p>It is an unsupervised machine learning technique that\u2019s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize the set of documents.</p> <p>The output is collections of documents that the algorithm has grouped together, as well as clusters of words and expressions that it used to infer these relations.</p>"},{"location":"TopicModeling/BertTopic/#bertopic_1","title":"BERTopic","text":"<p>BERTopic is a topic modeling technique that leverages \ud83e\udd17 transformers and c-TF-IDF. In general, it outperforms other methods (statistical vs Language Model based) e.g. LSA, LDA. </p> <p>Quote</p> <p>Before diving into a classification task, text clustering allows for getting an intuitive understanding of the task but also of its complexity.</p> <p>BERTopic is a topic modeling technique that assumes that clusters of semantically similar documents are a powerful way of generating and describing clusters. The documents in each cluster are expected to describe a major theme and combined they might represent a topic.</p> <p>Extracts from book co-authored by BertTopic creator: https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/</p>"},{"location":"TopicModeling/BertTopic/#topic-model-cwes","title":"Topic Model CWEs","text":"<p>I've used BERTopic to Topic Model lots of datasets to quickly get an intuition for the data.</p> <p>In this example, we'll look at CWEs. </p> <ul> <li>Specifically, generate a Topic Model of the ~1000 CWEs in 1400 View, so we can better understand the underlying patterns and themes.</li> </ul> <p>I did this initially as part of a LinkedIn Thread, to illustrate Topic Modeling as a solution, but also the ease and speed at which it could be done.</p>"},{"location":"TopicModeling/BertTopic/#code","title":"Code","text":"<p>Source Code</p> <p>The starting point code is deliberately chosen as unrelated to CyberSecurity: https://github.com/splevine/harnessing-nlp-cx-demo Mar 2024 (where this repo gives an excellent overview of NLP and BERTopic and SetFit).</p> <p>The main changes I made were:</p> <ol> <li>Don't use wandb (Weights and Balances) service as it was not needed for the demo</li> <li>Use the 1400 CWE view 1400.csv as the input Data</li> <li>Use SentenceTransformer(\"basel/ATTACK-BERT\") as the embedding_model so that CyberSecurity embeddings are used.</li> </ol> <p>The code and visualizations are available here: https://colab.research.google.com/drive/168i3oHekL4Cva2s2w01KMq6mnffDgxIS?usp=sharing No effort was made to optimize the topic modeling i.e. it was completely unsupervised.</p>"},{"location":"TopicModeling/BertTopic/#topic-model-of-the-1000-cwes-in-1400-view","title":"Topic Model of the ~1000 CWEs in 1400 View","text":"<p> The top level categorizations of CVE 1400 per https://cwe.mitre.org/data/definitions/1400.html.</p> <p> The Topic Model of 1400 CWE view 1400.csv with the top level categorizations overlayed in bold to allow comparison of the top level groupings assigned by MITRE and the Topic Modeling.</p> <p>Other visualizations are available in the Colab Notebook.</p>"},{"location":"TopicModeling/BertTopic/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Topic Modeling with BERTopic is a powerful unsupervised learning approach to quickly and easily get an intuition for a set of documents.</li> </ol>"},{"location":"agents/Agents/","title":"Agents","text":""},{"location":"agents/Agents/#why-agents","title":"Why Agents?","text":"<p>Why agents are the next frontier of generative AI, July 2024</p> <p>Tip</p> <p>\"If one agent can't work well, then why is multi-agent useful?\"</p> <ul> <li>Grouping tools/responsibilities can give better results. An agent is more likely to succeed on a focused task than if it has to select from dozens of tools.</li> <li>Separate prompts can give better results. Each prompt can have its own instructions and few-shot examples. Each agent could even be powered by a separate fine-tuned LLM!</li> <li>Helpful conceptual model to develop. You can evaluate and improve each agent individually without breaking the larger application.</li> </ul> <p>Multi-agent designs allow you to divide complicated problems into tractable units of work that can be targeted by specialized agents and LLM programs.</p> <p>https://blog.langchain.dev/langgraph-multi-agent-workflows/</p> <p>Using Agents can significantly improve performance</p> <p> </p> <p>GPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot) does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%.  https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance</p> <p>An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/</p> <p>Agentic AI Design Patterns</p> <ol> <li>Reflection: The LLM examines its own work to come up with ways to improve it. </li> <li>Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.</li> <li>Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).</li> <li>Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.</li> </ol> <p>https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance</p> <p>Reflection using a multi-agent framework</p> <p>Further, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.</p> <p>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/</p> <p>Like the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of Reflection and Tool Use are more reliable.</p> <p>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/</p>"},{"location":"agents/Agents/#llms-in-production","title":"LLMs in Production","text":"<ol> <li>Building LLMs for Production: Enhancing LLM Abilities and Reliability with Prompting, Fine-Tuning, and RAG, October 2024, Louis-Francois Bouchard and Louie Peters</li> <li>LLMs in Production From language models to successful products, December 2024, Christopher Brousseau and Matthew Sharp</li> <li>The Shift from Models to Compound AI Systems, Feb 18, 2024</li> <li>LLM Engineer's Handbook, by Paul Iusztin and Maxime Labonne, Oct 2024</li> <li>What We\u2019ve Learned From A Year of Building with LLMs. A practical guide to building successful LLM products, covering the tactical, operational, and strategic., June 2024</li> </ol>"},{"location":"agents/Agents/#agents-in-production","title":"Agents in Production","text":"<ol> <li>Building effective agents (Anthropic), Dec 19, 2024</li> <li>Pitfalls and Best Practices \u2014 5 lessons from LLMs in Production, Raza Habib, LLMs in Prod Con 2 https://www.youtube.com/watch?v=TRAUNcPj8KU </li> <li>Generative AI Agents in Production: Best Practices and Lessons Learned, Patrick Marlow https://www.youtube.com/watch?v=3y17VO3C8jo and whitepaper https://www.kaggle.com/whitepaper-agents</li> <li>Fine-Tuning LLMs: Best Practices and When to Go Small, Mark Kim-Huang, MLOps Meetup https://www.youtube.com/watch?v=1WSUfWojoe0 </li> <li>Everything you ever wanted to know about LLMOps Maturity Models, Alex Strick van Linschoten Nov 26, 2024 https://www.zenml.io/blog/everything-you-ever-wanted-to-know-about-llmops-maturity-models</li> <li>A comprehensive collection of over 300 real-world generative AI implementations that showcases how organizations are successfully deploying LLMs in production https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations</li> <li>Stanford Webinar - Agentic AI: A Progression of Language Model Usage and a list of 40 takeaways </li> </ol>"},{"location":"agents/Agents/#references","title":"References","text":"<ol> <li>https://huggingface.co/spaces/galileo-ai/agent-leaderboard \"How do top LLMs perform in real-world agentic scenarios?\"</li> <li>https://github.com/Jason2Brownlee/awesome-llm-books a list of \"Awesome LLM Books\"</li> <li>https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance<ol> <li>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection</li> <li>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use</li> <li>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning</li> <li>https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration</li> </ol> </li> <li>https://www.linkedin.com/pulse/potential-impact-future-ai-agents-agentic-series-part-44-techwards-84h3f</li> <li>https://www.resilientcyber.io/p/agentic-ais-intersection-with-cybersecurity</li> <li>https://blog.langchain.dev/langgraph-multi-agent-workflows/</li> <li>Mixture-of-Agents Enhances Large Language Model Capabilities, June 2024, https://arxiv.org/abs/2406.04692</li> <li>Data Interpreter: An LLM Agent For Data Science, Mar 2024, https://arxiv.org/abs/2402.18679</li> <li>AgentCoder: Multi-Agent Code Generation with Effective Testing and Self-optimisation, May 2024, https://arxiv.org/html/2312.13010v3 </li> <li>The COMPLETE TRUTH About AI Agents (2024) https://www.youtube.com/watch?v=HGS5LfyxLUU</li> <li>Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents, Apr 2025, https://arxiv.org/html/2504.19956v1</li> </ol>"},{"location":"agents/Build_agents/","title":"Building Multi Agent Systems","text":"<p>Overview</p> <p>Here we will discuss how to build a multi agent system using LangChain and LangGraph.</p> <p>We will illustrate the architecture of the system using a Mermaid diagram (generated via a GPT).</p> <p>We will then convert the diagram to LangChain and LangGraph code (using a GPT)</p> <p>No code solutions to building Multi Agent Systems are also available</p> <p>Some examples:</p> <ul> <li>LangGraph Studio </li> <li>Flowise </li> </ul> <p>The building/coding of these agents was AI-assisted</p> <ol> <li>Architecture Diagram: Bootstrap Mermaid diagram was created (using Claude 3.5 Sonnet)</li> <li>Code: Bootstrap code was generated from the Architecture Diagram (using Claude 3.5 Sonnet)</li> <li>Code Editor: Cursor AI was used (with Claude 3.5 Sonnet)</li> <li>Documentation: CrewAI.com Chat with our docs link (ChatGPT agent). This was also used to generate code snippets as an answer to a question.</li> </ol>"},{"location":"agents/Build_agents/#multi-agent-systems-problem-and-solution-space","title":"Multi Agent Systems Problem and Solution Space","text":"<p>For open-ended problems, ReAct is appropriate.</p> <p>But for smaller Problem and Solution Spaces (more bounded), agents and their tools can be more defined and deterministic, and this allows for more optimization.</p>"},{"location":"agents/Build_agents/#multi-agent-systems-frameworks","title":"Multi Agent Systems Frameworks","text":"<p>There are several frameworks for building Multi Agent Systems (MAS), and many more being developed.</p> <p>Example popular frameworks:</p> <ul> <li>CrewAI (built on LangChain)</li> <li>AutoGen (by Microsoft)</li> <li>Swarms</li> </ul> <p>Some frameworks are best suited to more complex multi agent use cases - where autonomy and conversation is required.      * Chatty can be expensive in time and money as its more LLM calls. * they can also be useful for rapid prototyping and development of Multi Agent Systems - getting a working system quickly, that can then be optimized and customized with a lower level framework.</p> <p>For simpler use cases, where we want to prescribe/control how agents interact with each other, we can use LangChain and LangGraph (or no-code solution based on LangGraph).</p> <ul> <li>LangChain does have LCEL (LangChain Expression Language) but LangGraph on LangChain may be a better option.  </li> <li>Another option again is to write (and maintain) your own plumbing to get a simpler Multi Agent System. And it seems like lots of people are rolling their own.</li> </ul> <p>My requirements here are:</p> <ol> <li>Minimal Time-To-Value and Time-To-Learning from the initial solution</li> <li>Support by tools</li> <li>including Observability tooling e.g. LangFuse, LangSmith, LangTrace </li> <li>Representation and support for production Deployment on Cloud platforms (AWS, GCP)</li> </ol>"},{"location":"agents/Build_agents/#architecture","title":"Architecture","text":"<pre><code>\ngraph TD\n    Start((Start)) --&gt; A[Get Vulnerability Description]\n    A --&gt; |Vulnerability Description| C[Extract vulnerability keyphrases from Vulnerability Description]\n    C --&gt; |Vulnerability KeyPhrases| D[Get CVEs with similar weakness keyphrases \\nfrom CWE Observed Examples]\n    C --&gt; |Vulnerability KeyPhrases| E[Get CVEs with similar weakness keyphrases \\nfrom Top25 CWE Mappings]\n    C --&gt; |Vulnerability KeyPhrases| I[Get CVEs with similar weakness keyphrases \\nfrom NVD]\n    A-. Vulnerability Description .-&gt; R\n\n    D --&gt; R[Create report]\n    E --&gt; R[Create report]\n    I --&gt; R[Create report]\n    R --&gt; |Draft Report|G[Review Report]\n    G --&gt;|Human feedback| G\n    G --&gt; |Final Report|End((End))\n</code></pre> <p>Note</p> <p>Text from the NVD References could also be retrieved to augment the report input, as sometimes they contain information that is not in the CVE Description. </p> <p>That has not been done here (yet) and is not shown in the diagram.</p> <p>Diagram Code <pre><code>graph TD\n    Start((Start)) --&gt; A[Vulnerability Description]\n    A --&gt; |Vulnerability Description| C[Extract vulnerability keyphrases from Vulnerability Description]\n    C --&gt; |Vulnerability Description\\nVulnerability KeyPhrases| D[Get CVEs with similar weakness keyphrases \\nfrom CWE Observed Examples]\n    C --&gt; |Vulnerability Description\\nVulnerability KeyPhrases| E[Get CVEs with similar weakness keyphrases \\nfrom Top25 CWE Mappings]\n    C --&gt; |Vulnerability Description\\nVulnerability KeyPhrases| I[Get CVEs with similar weakness keyphrases \\nfrom NVD]\n    D --&gt; R[Create report]\n    E --&gt; R[Create report]\n    I --&gt; R[Create report]\n    R --&gt; |Draft Report|G[Review Report]\n    G --&gt;|Human feedback| G\n    G --&gt; |Final Report|End((End))\n</code></pre></p>"},{"location":"agents/Build_agents/#architecture-to-code","title":"Architecture to Code","text":"<pre><code>I want to create a LangGraph multi agent system based on this diagram\n</code></pre> <p><pre><code>I want to create a CrewAI multi agent system based on this diagram\n</code></pre> Claude 3.5 Sonnet</p>"},{"location":"agents/Build_agents/#multi-agent-system-skeleton-code","title":"Multi Agent System Skeleton Code","text":"<pre><code>#from typing import TypedDict, Annotated, Sequence\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph import StateGraph, Graph\nfrom langchain_core.runnables import RunnableBinding\nfrom langchain.pydantic_v1 import BaseModel, Field\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nimport asyncio\n\n# Define the state\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], \"The messages in the conversation\"]\n    vulnerability_description: Annotated[str, \"The original vulnerability description\"]\n    vulnerability_keyphrases: Annotated[list, \"Extracted keyphrases from the vulnerability description\"]\n    cwe_observed_cves: Annotated[list, \"CVEs from CWE Observed Examples\"]\n    top25_cwe_cves: Annotated[list, \"CVEs from Top25 CWE Mappings\"]\n    nvd_cves: Annotated[list, \"CVEs from NVD\"]\n    draft_report: Annotated[str, \"The draft report\"]\n    final_report: Annotated[str, \"The final report\"]\n\n# Initialize the language model\nllm = ChatOpenAI(temperature=0)\n\n# Define the agents\n\ndef extract_keyphrases(state):\n    prompt = ChatPromptTemplate.from_template(\n        \"Extract key phrases related to the vulnerability from the following description: {vulnerability_description}\"\n    )\n    chain = prompt | llm\n    response = chain.invoke({\"vulnerability_description\": state[\"vulnerability_description\"]})\n    keyphrases = response.content.split(\", \")\n    return {\"vulnerability_keyphrases\": keyphrases}\n\nasync def get_cwe_observed_cves(state):\n    # Simulating an asynchronous API call\n    await asyncio.sleep(1)\n    # In a real scenario, you would query a CVE database here\n    return {\"cwe_observed_cves\": [\"CVE-2021-1234\", \"CVE-2022-5678\"]}\n\nasync def get_top25_cwe_cves(state):\n    await asyncio.sleep(1)\n    return {\"top25_cwe_cves\": [\"CVE-2020-9876\", \"CVE-2023-4321\"]}\n\nasync def get_nvd_cves(state):\n    await asyncio.sleep(1)\n    return {\"nvd_cves\": [\"CVE-2019-8765\", \"CVE-2024-1111\"]}\n\ndef create_report(state):\n    prompt = ChatPromptTemplate.from_template(\n        \"Create a comprehensive report based on the following information:\\n\"\n        \"Vulnerability Description: {vulnerability_description}\\n\"\n        \"Key Phrases: {vulnerability_keyphrases}\\n\"\n        \"CVEs from CWE Observed Examples: {cwe_observed_cves}\\n\"\n        \"CVEs from Top25 CWE Mappings: {top25_cwe_cves}\\n\"\n        \"CVEs from NVD: {nvd_cves}\\n\"\n        \"Provide a detailed analysis and recommendations.\"\n    )\n    chain = prompt | llm\n    response = chain.invoke(state)\n    return {\"draft_report\": response.content}\n\ndef review_report(state):\n    prompt = ChatPromptTemplate.from_template(\n        \"Review and improve the following report:\\n{draft_report}\\n\"\n        \"Provide any necessary corrections or additions.\"\n    )\n    chain = prompt | llm\n    response = chain.invoke(state)\n    return {\"final_report\": response.content}\n\n# Define the graph\nworkflow = StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"extract_keyphrases\", extract_keyphrases)\nworkflow.add_node(\"get_cwe_observed_cves\", get_cwe_observed_cves)\nworkflow.add_node(\"get_top25_cwe_cves\", get_top25_cwe_cves)\nworkflow.add_node(\"get_nvd_cves\", get_nvd_cves)\nworkflow.add_node(\"create_report\", create_report)\nworkflow.add_node(\"review_report\", review_report)\n\n# Add edges\nworkflow.add_edge(\"extract_keyphrases\", \"get_cwe_observed_cves\")\nworkflow.add_edge(\"extract_keyphrases\", \"get_top25_cwe_cves\")\nworkflow.add_edge(\"extract_keyphrases\", \"get_nvd_cves\")\nworkflow.add_edge(\"get_cwe_observed_cves\", \"create_report\")\nworkflow.add_edge(\"get_top25_cwe_cves\", \"create_report\")\nworkflow.add_edge(\"get_nvd_cves\", \"create_report\")\nworkflow.add_edge(\"create_report\", \"review_report\")\n\n# Set entry point\nworkflow.set_entry_point(\"extract_keyphrases\")\n\n# Compile the graph\napp = workflow.compile()\n\n# Run the graph\nconfig = {\"vulnerability_description\": \"A buffer overflow vulnerability in the XYZ software allows remote attackers to execute arbitrary code.\"}\nresult = app.invoke(config)\n\nprint(result[\"final_report\"])\n</code></pre>"},{"location":"agents/Crewai/","title":"Building Multi Agent Systems","text":"<p>Overview</p> <p>Here we will walk through building a Multi Agent System using CrewAI.</p> <p>The system architecture Mermaid diagram was converted to CrewAI code (using a GPT).</p> <p>Tip</p> <p>CrewAI.com has a Chat with our docs link (ChatGPT agent) which is useful to ask questions e.g.</p> <p>Quote</p> <p>explain the different agent parts and what's the difference between system_prompt backstory and goal</p> <p>This is a small extract from the response</p> <p>When defining an agent, each of these parts works together to create a fully functional and contextually aware agent:</p> <p>The role is the title or the agent\u2019s job.</p> <p>The goal is what the agent is aiming to achieve in a broad sense.</p> <p>The backstory adds narrative depth, giving the agent more personality and context.</p> <p>The system_prompt provides the specific instructions to execute tasks in real time.</p>"},{"location":"agents/Crewai/#multi-agent-systems","title":"Multi Agent Systems","text":""},{"location":"annex/Glossary/","title":"Glossary","text":"<p>~: shorthand for \"approximately\"</p> <p>~~: shorthand for \"very approximately\"</p>"},{"location":"annex/Learning/","title":"Learning","text":""},{"location":"annex/Learning/#books","title":"Books","text":"<ol> <li>Prompt Engineering for Generative AI James Phoenix, Mike Taylor</li> <li>Hands-On Large Language Models   Jay Alammar, Maarten Grootendorst </li> <li>AI Agents in Action, Publication in Early 2025 (estimated)</li> <li>Prompt Engineering in Practice, Publication in Spring 2025 (estimated)</li> <li>LLMs in Production From language models to successful products</li> <li>AI Apps with LangChain </li> <li>Natural Language Processing in Action, 2nd ed.</li> <li>Data Storytelling with Generative AI using Altair and Python</li> <li>langchain in your pocket</li> </ol>"},{"location":"annex/Learning/#newsletters","title":"Newsletters","text":"<ol> <li>https://www.deeplearning.ai/the-batch/issue-260/</li> </ol>"},{"location":"annex/Learning/#lists","title":"Lists","text":"<ol> <li>Applying LLMs &amp; GenAI to Cyber Security (list of resources) Dylan Williams </li> <li>https://joelbooks.medium.com/top-8-books-about-generative-ai-2024-llms-gpts-diffusion-models-b70663f72686</li> <li>https://emt.gartnerweb.com/ngw/globalassets/en/cybersecurity/documents/predicts-2024-cybersecurity-ai.pdf</li> <li>https://www.wardsauto.com/industry/gartner-predicts-highs-and-lows-of-ai-in-cybersecurity</li> </ol>"},{"location":"annex/Learning/#courses","title":"Courses","text":"<ol> <li>AI and GenAI Courses     |  Andrew Ng and others| </li> <li>HugginFace courses</li> </ol>"},{"location":"cti_models/cyber_security_models/","title":"Overview","text":"<p>Overview</p> <p>CyberSecurity Models are those that are optimized and configured specifically for the CyberSecurity domain via knowledge.</p>"},{"location":"cti_models/cyber_security_models/#benchmark","title":"Benchmark","text":"<p>CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence is a suite of benchmark tasks and datasets to evaluate LLMs in Cyber Threat Intelligence (CTI). </p> <p></p> <p>It consists of:</p> <ul> <li>(a) CTI-MCQ, a knowledge evaluation dataset comprising multiple-choice questions aimed at testing LLMs\u2019 understanding of crucial CTI concepts, including standards, threat identification, detection strategies, mitigation techniques, and best practices</li> <li>(b) CTI-RCM, which involves mapping Common Vulnerabilities and Exposures (CVE) descriptions to Common Weakness Enumeration (CWE) categories</li> <li>(c) CTI-VSP, which requires calculating Common Vulnerability Scoring System (CVSS) scores </li> <li>(d) CTI-ATE, which focuses on extracting MITRE ATT&amp;CK techniques from threat descriptions </li> <li>(e) CTI-TAA, where LLMs are tasked with analyzing publicly available threat reports and attributing them to specific threat actors or malware families</li> </ul>"},{"location":"cti_models/cyber_security_models/#benchmark-usage","title":"Benchmark Usage","text":"<p>CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence is being used by some vendors for their CTI models e.g.</p> <ol> <li>Google https://security.googleblog.com/2025/04/google-launches-sec-gemini-v1-new.html </li> <li>Cisco https://huggingface.co/fdtn-ai/Foundation-Sec-8B</li> <li>Trend Micro https://huggingface.co/trend-cybertron</li> </ol>"},{"location":"cti_models/cyber_security_models/#cti-rcm","title":"CTI-RCM","text":"<p>The Cyber Threat Intelligence Root Cause Mapping (CTI-RCM) benchmark evaluates an LLM's ability to understand the nuances of vulnerability descriptions, identify vulnerabilities underlying root causes, and accurately classify them according to the CWE taxonomy.</p> <p>CTI-RCM includes 1000 CVE --&gt; CWE Mappings using the CVE Description only (not CVE Reference link content). </p> <ul> <li>The 1000 is a random sample of vulnerabilities reported in the year 2024: https://github.com/xashru/cti-bench/blob/main/data/cti-rcm.tsv </li> <li>It also includes 1000 CVES from 2021 https://github.com/xashru/cti-bench/blob/main/data/cti-rcm-2021.tsv</li> </ul>"},{"location":"cti_models/cyber_security_models/#cti-rcm-exploratory-data-analysis","title":"CTI-RCM Exploratory Data Analysis","text":"<p>Exploratory Data Analysis of the CTI-RCM dataset with code</p> <p>An Exploratory Data Analysis of the dataset is available in https://github.com/CyberSecAI/cti_bench_rcm.</p>"},{"location":"cti_models/cyber_security_models/#limitations","title":"Limitations","text":"<ol> <li>The dataset is not \"known-good\" i.e. how do we know the CVE-CWE mappings are correct?<ol> <li>For comparison, the MITRE CWE Top25 2023 identified a significant number of CWEs that were incorrect </li> </ol> </li> <li>The dataset contains a limited number of CWEs per https://github.com/CyberSecAI/cti_bench_rcm<ol> <li>For comparison, the MITRE CWE Top25 2023 contains ~400 unique CWEs</li> </ol> </li> <li>The dataset contains a limited number of CVEs per https://github.com/CyberSecAI/cti_bench_rcm<ol> <li>For comparison, the MITRE CWE Top25 2022, 2023 include ~~7K CVE-CWE mappings each</li> </ol> </li> <li>The dataset contains 1 CWE per CVE <ol> <li>For comparison, the MITRE CWE Top25 2022, 2023 datasets contain many CWEs with more than 1 CWE per CVE</li> </ol> </li> <li>The dataset is highly imbalanced per https://github.com/CyberSecAI/cti_bench_rcm<ol> <li>Some CWEs appear a lot more than others per https://github.com/CyberSecAI/cti_bench_rcm</li> </ol> </li> <li>The dataset contains the CVE Description only i.e. not the CVE Reference Information in the CVE links<ol> <li>This generally contains more information than the CVE Description alone.</li> <li>A benchmark dataset should use the full CVE info available in the CVE including the Reference Content.</li> <li>For comparison, the MITRE CWE Top25 2022, 2023 mappings relied on the Reference content in many cases because the CVE Description was not sufficient.</li> </ol> </li> <li>The evaluation is a binary equals per https://github.com/xashru/cti-bench/blob/main/evaluation/evaluation.ipynb i.e. correct if the CWE assigned by the model equals the CWE in the benchmark dataset. <ol> <li>Based on my work with the MITRE CWE-RCMWG, a model may assign CWEs that are (correctly) more specific (childOf) than the CWE in the CVE. Does this make it \"wrong\" or \"more right\"?</li> <li>An evaluation metric should support this e.g. use distance between benchmark and assigned CWEs, where exact match is distance 0, parent/child is distance 1, grandparent/grandchild or cousin is distance 2.</li> <li>An evaluation metric should support the case where there is more that one CWE for a CVE.</li> </ol> </li> </ol>"},{"location":"introduction/Introduction/","title":"Introduction","text":"<p>About this Guide</p> <p>This guide is in an initial early access state currently and is written to organize my play time as I play with, and apply, these tools.</p> <p>The content is about solving real problems e.g. how to </p> <ul> <li>view the main topics in a set of documents</li> <li>validate assigned CWEs, and suggest CWEs to assign </li> <li>chat with large documents </li> <li>extract configuration parameters from user manuals.</li> </ul> <p>These examples were driven by a user need. </p> <p>While the examples focus on specific areas, they can be applied in general to many areas.</p> <p>After reading this guide you should be able to</p> <ol> <li>Apply Language Models to augment and amplify your skills.</li> <li>Understand the types of problems that suit Language Models, and those that don't</li> </ol>"},{"location":"introduction/Introduction/#overview","title":"Overview","text":""},{"location":"introduction/Introduction/#intended-audience","title":"Intended Audience","text":"<p>The intended audience is people wanting to go beyond the hype and basics of Large Language Models.</p> <p>No prior knowledge is assumed to read the guide - it provides just enough information to understand the advanced topics covered.</p> <p>A basic knowledge of Jupyter Python is required to run the code (with the data provided or on your data).</p>"},{"location":"introduction/Introduction/#how-to-use-this-guide","title":"How to Use This Guide","text":""},{"location":"introduction/Introduction/#how-to-contribute-to-this-guide","title":"How to Contribute to This Guide","text":"<p>You can contribute content or suggest changes:</p> <ul> <li>\ud83d\udcac Discuss content</li> <li>\ud83d\udca1 Suggest content</li> <li>\ud83e\udeb2 Report Errors, typos</li> </ul>"},{"location":"introduction/Introduction/#writing-style","title":"Writing Style","text":"<p>The \"writing style\" in this guide is succinct, and leads with an opinion, with data and code to back it up i.e. data analysis plots (with source code where possible) and observations and takeaways that you can assess - and apply to your data and environment. This allows the reader to assess the opinion and the code/data and rationale behind it.</p> <p>Different, and especially opposite, opinions with the data to back them up, are especially welcome! - and will help shape this guide.</p> <p>Quote</p> <p>If we have data, let\u2019s look at data. If all we have are opinions, let\u2019s go with mine.</p> <p>Jim Barksdale, former CEO of Netscape </p>"},{"location":"introduction/Introduction/#notes","title":"Notes","text":"<p>Notes</p> <ol> <li>This guide is not affiliated with any Tool/Company/Vendor/Standard/Forum/Data source. <ol> <li>Mention of a vendor in this guide is not a recommendation or endorsement of that vendor. </li> </ol> </li> <li>This guide is a living document i.e. it will change and grow over time - with your input.</li> </ol> <p>This guide is not about which tool is better than the other</p> <p>\"Don't fall in love with models: they're expendable. Fall in love with data!\" </p> <p>Julien Simon, Chief Evangelist, Hugging Face</p> <p>Warning</p> <p>This space is rapidly evolving so the content in this guide may become no longer current or accurate.</p> <p>Warning</p> <p>You are responsible for your data and where it goes. </p> <p>If you don't understand where your data goes, and what happens to it for a given model or tool, then find out before you use private or personal data.</p> <p>To evaluate models and tools, you can start with public data.</p>"},{"location":"introduction/Preface/","title":"Preface","text":"<p>Language Models are powerful tools that can be applied to CyberSecurity.</p> <p>I enjoy learning about, playing with, and applying these tools to better solve problems:</p> <ul> <li>I wrote this guide for me, to organize my thoughts and my play time as I play with, and apply, Language Models.</li> <li>I've found by putting something out there, you get something back.</li> <li>It's the guide I wish existed already.</li> </ul> <p>This approach worked well for the Risk-Based Prioritization guide...</p> <p>You may find it useful.</p> <p>Chris Madden</p>"},{"location":"introduction/cybsersecurity/","title":"LLMs for CyberSecurity","text":""},{"location":"introduction/cybsersecurity/#llms-for-cybersecurity-users-and-use-cases","title":"LLMs for CyberSecurity Users and Use Cases","text":"<p>Image from Generative AI and Large Language Models for Cyber Security: All Insights You Need. </p>"},{"location":"introduction/cybsersecurity/#empathy-map","title":"Empathy Map","text":"<p>See Original Post.</p> <p>Tip</p> <p>See also MITRE\u2019s Innovation Toolkit https://itk.mitre.org/toolkit/tools-at-a-glance/ a collection of proven and repeatable problem-solving methods to help you and your team do something different that makes a difference.</p>"},{"location":"introduction/cybsersecurity/#targeted-premortem-for-trustworthy-ai","title":"Targeted PreMortem for Trustworthy AI","text":"<p>In general, it is good practice to start with the end in mind ala \"Destination Postcard\" from the book Switch, Dan and Chip Heath which looks at the aspirational positive outcomes.</p> <p>This is also useful for Premortems to proactively identify failures so they can be avoided, to ensure the positive outcomes.</p> <p>Quote</p> <p>The Targeted Premortem (TPM) is a variant of Klein's Premortem Technique, which uses prospective hindsight to proactively identify failures. This variant targets brainstorming on reasons for losing trust in AI in the context of the sociotechnical system into which it is integrated. That is, the prompts are targeted to specific evidence-based focus areas where trust has been lost in AI. This tool comes with instructions, brainstorming prompts, and additional guidance on how to analyze the outcomes of a TPM session with users, developers, and other stakeholders.</p> <p>MITRE Targeted PreMortem for Trustworthy AI</p>"},{"location":"introduction/cybsersecurity/#references","title":"References","text":""},{"location":"introduction/cybsersecurity/#llms-for-cybersecurity-references","title":"LLMs for CyberSecurity References","text":"<ol> <li>Generative AI and Large Language Models for Cyber Security: All Insights You Need, May 2024</li> <li>A Comprehensive Review of Large Language Models in Cyber Security, September 2024</li> <li>Large Language Models in Cybersecurity: State-of-the-Art, January 2024</li> <li>How Large Language Models Are Reshaping the Cybersecurity Landscape | Global AI Symposium talk, September 2024</li> <li>Large Language Models for Cyber Security: A Systematic Literature Review, July 2024</li> <li>Using AI for Offensive Security, June 2024</li> </ol>"},{"location":"introduction/cybsersecurity/#agents-for-cybersecurity-references","title":"Agents for CyberSecurity References","text":"<ol> <li>Blueprint for AI Agents in Cybersecurity - Leveraging AI Agents to Evolve Cybersecurity Practices</li> <li>Building AI Agents: Lessons Learned over the past Year</li> </ol>"},{"location":"introduction/cybsersecurity/#comparing-llms","title":"Comparing LLMs","text":"<p>There are several sites that allow comparisons of LLMs e.g.</p> <ol> <li>https://winston-bosan.github.io/llm-pareto-frontier/<ol> <li>LLM Arena Pareto Frontier: Performance vs Cost</li> </ol> </li> <li>https://artificialanalysis.ai/<ol> <li>Independent analysis of AI models and API providers. Understand the AI landscape to choose the best model and provider for your use-case</li> </ol> </li> <li>https://llmpricecheck.com/<ol> <li>Compare and calculate the latest prices for LLM (Large Language Models) APIs from leading providers such as OpenAI GPT-4, Anthropic Claude, Google Gemini, Mate Llama 3, and more. Use our streamlined LLM Price Check tool to start optimizing your AI budget efficiently today!</li> </ol> </li> <li>https://openrouter.ai/rankings?view=day <ol> <li>Compare models used via OpenRouter</li> </ol> </li> <li>https://github.com/vectara/hallucination-leaderboard <ol> <li>LLM Hallucination Rate leaderboard</li> </ol> </li> <li>https://lmarena.ai/?leaderboard<ol> <li>Chatbot Arena is an open platform for crowdsourced AI benchmarking</li> </ol> </li> <li>https://aider.chat/docs/leaderboards/<ol> <li>Benchmark to evaluate an LLM\u2019s ability to follow instructions and edit code successfully without human intervention</li> </ol> </li> <li>https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro<ol> <li>Benchmark to evaluate language understanding models across broader and more challenging tasks</li> </ol> </li> </ol> <p>See also Economics of LLMs: Evaluations vs Pricing - Looking at which model to use for which task</p>"},{"location":"introduction/cybsersecurity/#books","title":"Books","text":"<ol> <li>Build a Large Language Model (from Scratch) by Sebastian Raschka, PhD </li> <li>LLM Engineer's Handbook by Paul Iusztin and Maxime Labonne </li> <li>AI Engineering by Chip Huyen </li> <li>Hands-On Large Language Models: Language Understanding and Generation, Oct 2024, Jay Alammar and Maarten Grootendorst</li> <li>Building LLMs for Production: Enhancing LLM Abilities and Reliability with Prompting, Fine-Tuning, and RAG, October 2024, Louis-Francois Bouchard and Louie Peters</li> <li>LLMs in Production From language models to successful products, December 2024, Christopher Brousseau and Matthew Sharp</li> <li>Fundamentals of Secure AI Systems with Personal Data, June 2025, Enrico Glerean </li> </ol>"},{"location":"introduction/models/","title":"Model Types","text":"<p>Overview</p> <p>This section gives an overview of different model types.</p>"},{"location":"introduction/models/#introduction","title":"Introduction","text":"<p>Different types of text models are designed with varying architectures, training data, and optimization goals, leading to distinct capabilities and best-fit use cases. </p> <p>While many models focus primarily on text, an increasing number are becoming multimodal, capable of processing and generating information across different types of data, such as text, images, audio, and even video.</p> <p>Today, these different types of models are generally accessed by selecting the specific model or API endpoint provided by developers (versus the user accessing the same interface that figures out the best type of model).</p>"},{"location":"introduction/models/#deep-research","title":"Deep Research","text":"<p>Models in this category are typically designed for in-depth information retrieval, synthesis, and report generation from vast amounts of data, often involving Browse and analyzing multiple sources. They aim to provide comprehensive and well-supported answers to complex queries.</p> <p>Key Insights:</p> <ul> <li>Extensive Information Gathering: Excel at searching and processing information from large and diverse datasets, including the web or private document repositories.</li> <li>Synthesis and Structuring: Capable of synthesizing information from various sources into coherent and structured reports or summaries.</li> <li>Handling Complexity: Designed to tackle complex, multi-faceted research questions that require connecting information across different domains.</li> <li>Citation and Verification: Often include features for citing sources, allowing users to verify the information presented.</li> </ul> <p>Use Cases:</p> <ul> <li>Generating detailed reports on niche or complex topics.</li> <li>Performing market research by analyzing industry trends and competitor information.</li> <li>Assisting academics and researchers in literature reviews and synthesizing findings.</li> <li>Providing comprehensive answers to complex legal, medical, or scientific questions.</li> <li>Analyzing large volumes of internal documents to extract insights.</li> </ul> <p>Examples: Perplexity Deep Research, ChatGPT Deep Research, Gemini Deep Research, HuggingFace Open Deep Research, Claude 3 Opus</p>"},{"location":"introduction/models/#reasoning","title":"Reasoning","text":"<p>Reasoning-focused models are optimized to perform complex logical deductions, solve problems requiring multiple steps, and understand intricate relationships between concepts. They are built to \"think\" through problems rather than just retrieving information or generating text based on patterns.</p> <p>Key Insights:</p> <ul> <li>Logical Deduction: Strong capabilities in applying logical rules and deriving conclusions from given premises.</li> <li>Multi-Step Problem Solving: Can break down complex problems into smaller, manageable steps and follow a chain of thought to reach a solution.</li> <li>Mathematical and Scientific Reasoning: Often perform well on mathematical problems, coding challenges, and scientific inquiries that require step-by-step analysis.</li> <li>Reduced Hallucination in Complex Tasks: While still a challenge for all LLMs, models focused on reasoning aim to reduce the likelihood of generating false or inconsistent information in complex scenarios by showing their work or using techniques like self-correction.</li> </ul> <p>Use Cases:</p> <ul> <li>Solving complex mathematical equations or proofs.</li> <li>Debugging code and suggesting logical fixes.</li> <li>Analyzing data and drawing reasoned conclusions.</li> <li>Assisting in strategic planning by evaluating scenarios and predicting outcomes.</li> <li>Providing step-by-step explanations for complex concepts or solutions.</li> <li>Excelling at benchmarks requiring logical inference and problem-solving.</li> </ul> <p>Examples: DeepSeek-R1, OpenAI\u2019s GPT-4, Google\u2019s Gemini Ultra, Anthropic\u2019s Claude 3 Sonnet, Meta\u2019s Llama 3</p>"},{"location":"introduction/models/#general-purpose","title":"General Purpose","text":"<p>General-purpose LLMs are designed to be versatile and handle a wide array of natural language tasks. They are trained on broad datasets to provide a good balance of capabilities across different domains without being specifically optimized for one.</p> <p>Key Insights:</p> <ul> <li>Versatility: Capable of performing a wide range of tasks, including text generation, summarization, translation, question answering, and creative writing.</li> <li>Broad Knowledge: Possess a vast amount of general knowledge from their diverse training data.</li> <li>Adaptability: Can often adapt to different styles and formats based on the prompt.</li> <li>Accessibility: Typically the most widely available and accessible models for everyday use.</li> </ul> <p>Use Cases:</p> <ul> <li>Drafting emails, articles, and other written content.</li> <li>Summarizing documents or long texts.</li> <li>Translating text between languages.</li> <li>Answering general knowledge questions.</li> <li>Brainstorming ideas and assisting in creative writing.</li> <li>Powering chatbots and virtual assistants for a variety of inquiries.</li> </ul> <p>Examples: GPT-4 Turbo, Gemini Pro, Claude 3 Haiku, Mistral Large, Cohere Command R+</p>"},{"location":"introduction/models/#code","title":"Code","text":"<p>Code-focused models are specifically trained on large datasets of code from various programming languages and sources. They are designed to understand, generate, and assist with programming tasks.</p> <p>Key Insights:</p> <ul> <li>Code Generation: Can generate code snippets, functions, or even entire programs based on natural language descriptions or prompts.</li> <li>Code Completion: Provide intelligent suggestions for completing code as developers type.</li> <li>Code Explanation and Documentation: Can explain how code works and generate documentation.</li> <li>Debugging and Error Detection: Assist in identifying potential errors and suggesting fixes in code.</li> <li>Code Translation: Translate code between different programming languages.</li> <li>Support for Multiple Languages: Trained on a wide variety of programming languages.</li> </ul> <p>Use Cases:</p> <ul> <li>Speeding up software development by generating boilerplate code.</li> <li>Assisting developers in learning new programming languages or frameworks.</li> <li>Automating repetitive coding tasks.</li> <li>Improving code quality through suggestions and error detection.</li> <li>Generating test cases for software.</li> <li>Helping non-programmers understand or modify code.</li> </ul> <p>Examples: CodeLlama (Meta), StarCoder (ServiceNow), Codex (OpenAI), DeepSeek Coder, Google\u2019s Codey</p> <p>Tip</p> <p>This blog gives a good overview of different models' capabilities for code and echoes my experience.</p> <p>Tip</p> <p>Code Generation is a small part of Software Engineering.</p> <p>Different model types are suitable for different Software Engineering Artifacts</p>"},{"location":"prompt_engineering/Fabric/","title":"Fabric","text":"<p>Overview</p> <p>One of Fabric's primary features is helping people collect and integrate prompts</p>"},{"location":"prompt_engineering/Fabric/#fabric-prompt-collection-and-framework","title":"Fabric Prompt Collection and Framework","text":"<p>https://github.com/danielmiessler/fabric/tree/main </p> <p>Quote</p> <p>Since the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but it's not easy to integrate this functionality into our lives.</p> <p>In other words, AI doesn't have a capabilities problem\u2014it has an integration problem. Fabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.</p> <p>One of fabric's primary features is helping people collect and integrate prompts, which we call Patterns, into various parts of their lives.</p> <p>The prompts (aka Patterns) are available at https://github.com/danielmiessler/fabric/tree/main/patterns</p>"},{"location":"prompt_engineering/Fabric/#fabric-prompts-for-cybersecurity","title":"Fabric Prompts for CyberSecurity","text":"<p>Fabric has Prompts that are specific to CyberSecurity e.g.</p> <ul> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/create_threat_model/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/analyze_logs/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/analyze_malware/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/analyze_threat_report/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/analyze_threat_report_trends/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/write_hackerone_report/system.md</li> <li>https://github.com/danielmiessler/fabric/blob/main/patterns/write_semgrep_rule/system.md </li> </ul>"},{"location":"prompt_engineering/Fabric/#fabric-prompt-to-improve-prompts","title":"Fabric Prompt to Improve Prompts","text":"<p>https://github.com/danielmiessler/fabric/blob/main/patterns/improve_prompt/system.md</p>"},{"location":"prompt_engineering/Fabric/#fabric-extract-wisdom","title":"Fabric Extract Wisdom","text":"<p>We'll take Goldman Sachs, Top of Mind, Gen AI: Too Much Spend, Too Little Benefit?, June 2024 as the example content to extract wisdom from. It's a 30 page report that is dense and varied.</p> <p>Below the summary by Claude 3.5 Sonnet using the extract_article_wisdom prompt.   </p> <pre><code>SUMMARY\nGoldman Sachs analyzes AI's economic impact, discussing potential benefits, challenges, and investment implications in their \"Top of Mind\" report.\n\nIDEAS\n\nAI could increase US productivity by 9% and GDP growth by 6.1% over the next decade.\nThe current AI investment boom may be unsustainable, with potential for a market correction.\nPower demand from AI data centers could lead to a significant energy crunch in the coming years.\nAI's economic impact may be more limited than many expect, with only 4.6% of tasks potentially automated within a decade.\nThe semiconductor industry faces supply constraints that could limit AI growth in the near term.\nUtilities companies are emerging as unexpected beneficiaries of the AI boom due to increased power demand.\nAI technology may struggle to replicate human cognitive abilities, limiting its transformative potential.\nThe cost of AI infrastructure is currently high, raising questions about return on investment.\nAI's impact on corporate profitability will be crucial for long-term equity returns.\nThe AI theme in the stock market may have room to run despite recent gains.\nData centers could consume up to 8% of total US power demand by 2030.\nAI development is outpacing the ability of power infrastructure to support it.\nThe US may struggle to build the necessary infrastructure to support AI growth.\nAI spending by tech giants is not seen as irrational exuberance by some analysts.\nThe AI investment cycle is still in the infrastructure buildout phase, with applications yet to emerge.\nEuropean power demand could increase by 40-50% over the next decade due to AI and electrification.\nAI data centers can consume up to 10 times more energy than traditional data centers.\nThe US utility industry has not experienced significant load growth in almost two decades.\nAI chip demand is outstripping supply, particularly for High-Bandwidth Memory technology.\nThe pace of AI model improvements may be slower than many anticipate.\n\nQUOTES\n\n\"Given the focus and architecture of generative AI technology today... truly transformative changes won't happen quickly and few\u2014if any\u2014will likely occur within the next 10 years.\" - Daron Acemoglu\n\"AI technology is exceptionally expensive, and to justify those costs, the technology must be able to solve complex problems, which it isn't designed to do.\" - Jim Covello\n\"Spending is certainly high today in absolute dollar terms. But this capex cycle seems more promising than even previous capex cycles.\" - Kash Rangan\n\"The devil is ultimately in the details. So, I don't have a strong prior as to how much of the current investment boom will be wasted vs. productive. But I expect both will happen.\" - Daron Acemoglu\n\"Until we reach a level of saturation in terms of human capacity to consume data, any amount of efficiency gains will undoubtedly be gobbled up by even more demand.\" - Brian Janous\n\"The risk that our children or grandchildren in 2074 accuse us of moving too slowly in 2024 at the expense of growth seems far lower than the risk that we end up moving too quickly and destroy institutions, democracy, and beyond in the process.\" - Daron Acemoglu\n\"I'm loathe to use the word 'bubble' because I don't believe that AI is a bubble, but most bubbles in history ended either because the cost of capital changed dramatically or end-demand deteriorated and affected companies' ability to deploy capital, not because companies retreated from investing in a technology where the payoff was taking longer than expected.\" - Eric Sheridan\n\"Utilities have not experienced a period of load growth in almost two decades and are not prepared for\u2014or even capable of matching\u2014the speed at which AI technology is developing.\" - Brian Janous\n\"The US has unfortunately lost the ability to build large infrastructure projects\u2014this is a task better suited for 1930s America, not 2030s America.\" - Brian Janous\n\"Every human invention should be celebrated, and generative AI is a true human invention. But too much optimism and hype may lead to the premature use of technologies that are not yet ready for prime time.\" - Daron Acemoglu\n\nFACTS\n\nData centers currently account for only about 1% of global power demand.\nThe US utility industry has not experienced significant load growth in almost two decades.\n60% of workers today are employed in occupations that did not exist in 1940.\nThe US corporate profit share of GDP is near its highest levels since World War II.\nNorthern Virginia is home to the most data centers in the US.\nCommercial power consumption in Virginia rose 37% from 2016 to 2023.\nAI data centers can consume up to 10 times more energy than traditional data centers.\nThe average power usage effectiveness (PUE) of cloud data centers declined by only 0.2 over the course of a decade.\nWait times for power projects to connect to the grid currently range from 40-70 months.\nThe High-Bandwidth Memory market is expected to grow at a ~100% compound annual growth rate over the next few years.\nThe S&amp;P 500 Shiller P/E ratio is currently in the 97th percentile since 1900.\nUtilities returned 16% between March and May, making them the best-performing sector in the S&amp;P 500.\nEuropean electricity consumption has declined by around 10% from its 2008 peak.\nTSMC's Chip-on-Wafer-on-Substrate packaging capacity is expected to more than double in 2024.\nThe US is expected to add nearly 800 gigawatts of wind and solar over the coming 10-15 years.\n\nREFERENCES\n\nChatGPT\nGlobal Macro Research by Goldman Sachs\nMIT's Daron Acemoglu's research\nJim Covello's analysis\nJoseph Briggs' economic forecasts\nKash Rangan and Eric Sheridan's market analysis\nBrian Janous' insights on power infrastructure\nCarly Davenport's utility sector analysis\nToshiya Hari, Anmol Makkar, and David Balaban's semiconductor industry research\nRyan Hammond's equity market analysis\nChristian Mueller-Glissmann's long-term return forecasting framework\nAlberto Gandolfi's European utilities analysis\nHongcen Wei's power consumption analysis\nDavid Autor's research on technological innovation and employment\nRobert Shiller's P/E ratio data\nEIA (Energy Information Administration) data\nCensus Bureau data\nFactSet financial data\nTSMC's capacity projections\nREPowerEU Plan\n\nRECOMMENDATIONS\n\nInvestors should remain invested in AI infrastructure providers despite high valuations.\nUtilities companies should find ways to extract more efficiencies from the current power system.\nPolicymakers should focus on investing in America's transmission infrastructure.\nCompanies should tie AI spending to visible revenue growth to maintain investor confidence.\nInvestors should consider \"Electrification Compounders\" in the utilities sector as potential beneficiaries of AI growth.\nThe US should prioritize expanding long-duration energy storage to meet growing power demand.\nTech companies should prepare for potential power constraints when planning AI infrastructure.\nInvestors should monitor the emergence of AI's \"killer application\" as a sign of the technology's maturity.\nPolicymakers should address regulatory lags and interconnection constraints in the power sector.\nCompanies should be cautious about premature use of AI technologies that are not yet fully developed.\nInvestors should watch for signs of corporate profitability decline as a potential trigger for reduced AI spending.\nThe semiconductor industry should focus on expanding High-Bandwidth Memory and advanced packaging capacity.\nUtilities should prepare for a potential doubling of grid capacity in some regions over the next decade.\nInvestors should consider the impact of AI on corporate profitability when assessing long-term equity returns.\nPolicymakers should balance the need for AI innovation with potential risks to institutions and democracy.\n</code></pre>"},{"location":"prompt_engineering/Fabric/#process-youtube-transcripts","title":"Process Youtube Transcripts","text":"<p>Fabric, and LLMs in general, can process Youtube Transcripts (which are text).</p> <p>The Youtube Transcript for a video can be retrieved in different ways e.g. https://www.youtube.com/watch?v=oMZN810xfck:</p> <ul> <li>Click the Transcript button under a video and copy and paste, or download, to a text file</li> <li>Use Fabric helper function: installer/client/cli/yt.py (or other tool)</li> </ul> <p><pre><code>yt --transcript https://www.youtube.com/watch?v=oMZN810xfck\n</code></pre> The text can then be processed with an LLM.</p> <p>This download and processing can be done in one command with Fabric <pre><code>yt --transcript https://www.youtube.com/watch?v=oMZN810xfck | fabric -sp extract_wisdom\n</code></pre> where</p> <ul> <li>-s is for streaming</li> <li>-p is for pattern</li> <li>extract_wisdom is the Fabric prompt that extracts wisdom from any text. It addresses the problem of too much content and too little time.</li> </ul>"},{"location":"prompt_engineering/Fabric/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <ol> <li>Fabric is a good resource for prompts with many being related to CyberSecurity.</li> </ol>"},{"location":"prompt_engineering/prompt_engineering/","title":"Prompt Engineering","text":"<p>Overview</p> <p>A large part of working LLMs is knowing how to prompt them to get the information you want.</p> <p>ChatGPTo will be used here but the techniques apply in general to any LLM.</p>"},{"location":"prompt_engineering/prompt_engineering/#prompt-guides","title":"Prompt Guides","text":"<p>There are many books, guides, and articles on Prompt Engineering. Some of the better ones are listed here:</p> <ol> <li>Prompt Engineering Guide </li> <li>OpenAI Prompt Engineering Guide </li> <li>Best practices for prompt Engineering with the OpenAI API</li> <li>Google Gemini Prompting guide 101 - A quick-start handbook for effective prompts, April 2024 edition</li> <li>How I Won Singapore\u2019s GPT-4 Prompt Engineering Competition, Dec 2023</li> <li>Prompt Engineering for Generative AI Book, May 2024</li> <li>Google Prompt design strategies </li> <li>Anthropic Prompt Engineering overview </li> <li>Best Prompt Techniques for Best LLM Responses, Feb 2024</li> </ol>"},{"location":"prompt_engineering/prompt_engineering/#prompt-taxonomy","title":"Prompt Taxonomy","text":""},{"location":"prompt_engineering/prompt_engineering/#prompt-taxonomy-turn-expression-level-of-details-role","title":"Prompt Taxonomy Turn, Expression, Level of Details, Role","text":"<p>The above proposed Prompt Taxonomy is TELeR: Turn, Expression, Level of Details, Role from  TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks</p>"},{"location":"prompt_engineering/prompt_engineering/#prompt-taxonomy-26-guiding-principles","title":"Prompt Taxonomy 26 Guiding Principles","text":"<p>The above \"26 guiding principles designed to streamline the process of querying and prompting large language models\" is from Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</p>"},{"location":"prompt_engineering/prompt_engineering/#prompt-tactics","title":"Prompt Tactics","text":"<p>Some tactics I find particularly useful:</p> <ol> <li>Ask the LLM to generate the prompt.<ol> <li>Do this in 2 parts (task decomposition)<ol> <li>Ask the LLM what techniques are best to use for effective prompt engineering</li> <li>Ask the LLM to create a prompt using these techniques for the objective and example you specify</li> </ol> </li> </ol> </li> <li>Ask the LLM to provide a confidence score for their answer. <ol> <li>People can be fooled by the fluency of LLMs aka hallucinations </li> <li>LLMs know how confident they are in their answer.</li> <li>So asking an LLM to rate their confidence in the answer can reduce blind trust.</li> </ol> </li> <li>Ask the LLM (or a different LLM) to review the answer and provide improvements.</li> </ol>"},{"location":"prompt_engineering/prompt_engineering/#ask-the-llm-to-generate-the-prompt","title":"Ask the LLM to generate the prompt","text":""},{"location":"prompt_engineering/prompt_engineering/#ask-the-llm-what-techniques-are-best-to-use-for-effective-prompt-engineering","title":"Ask the LLM what techniques are best to use for effective prompt engineering?","text":""},{"location":"prompt_engineering/prompt_engineering/#ask-the-llm-to-create-a-prompt-using-these-techniques-for-the-objective-and-example-you-specify","title":"Ask the LLM to create a prompt using these techniques for the objective and example you specify","text":"<pre><code>_functions.php in cpCommerce 1.2.x, possibly including 1.2.9, sends a redirect but does not exit when it is called directly, which allows remote attackers to bypass a protection mechanism to conduct remote file inclusion and directory traversal attacks, execute arbitrary PHP code, or read arbitrary files via the GLOBALS[prefix] parameter, a different vector than CVE-2003-1500.\n\n\n#CVE# _functions.php in cpCommerce 1.2.x, possibly including 1.2.9, sends a redirect but does not exit when it is called directly, which allows remote attackers to bypass a protection mechanism to conduct remote file inclusion and directory traversal attacks, execute arbitrary PHP code, or read arbitrary files via the GLOBALS[prefix] parameter, a different vector than CVE-2003-1500.\n#IMPACT# Remote Code Execution, Information Disclosure, Security Bypass\n#RATIONALE# The description specifies multiple potential impacts:\n1. **Remote Code Execution**: The ability to \"execute arbitrary PHP code\" directly implies a high severity impact where an attacker can run malicious code on the affected system.\n2. **Information Disclosure**: The mention of \"read arbitrary files\" suggests that attackers can access sensitive information stored in files.\n3. **Security Bypass**: The bypass of a protection mechanism to conduct remote file inclusion and directory traversal attacks implies that security controls are being circumvented.\n#CONFIDENCE# 95\n</code></pre>"},{"location":"prompt_engineering/prompt_engineering/#ask-the-llm-to-review-the-answer-and-provide-improvements","title":"Ask the LLM to review the answer and provide improvements.","text":""},{"location":"prompt_engineering/prompt_engineering/#llm-tools-to-improve-prompts","title":"LLM Tools to Improve Prompts","text":"<p>In addition to using LLMs to generate prompts (aka Meta Prompts), and review prompts in an adhoc manner via the chat interface, there are various tools that use LLMs to improve prompts:</p> <ol> <li>Fabric Prompt to Improve Prompts</li> <li>Anthropic Console supports testing and improving prompts (for Anthropic - but likely the resulting prompt would be portable to other LLMs)<ol> <li>Claude can generate prompts, create test variables, and show you the outputs of prompts side by side.</li> <li>https://x.com/AnthropicAI/status/1810747792807342395</li> </ol> </li> <li>Google Prompt Gallery </li> </ol>"},{"location":"prompt_engineering/prompt_engineering/#prompts-used-in-various-code-generation-tools","title":"Prompts used in various Code Generation Tools","text":"<p>https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools</p> <p>FULL v0, Cursor, Manus, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent &amp; VSCode Agent (And other Open Sourced) System Prompts, Tools &amp; AI Models.</p> <p>These are useful as reference examples.</p> <p>Takeaways</p> <ol> <li>Getting the right prompt to get what you want out of an LLM can sometimes feel like art or interrogation. There are several options covered here:<ol> <li>Prompt Templates</li> <li>Prompt Frameworks</li> <li>Ask an LLM to generate a prompt</li> <li>LLM-based tools for prompt refinement</li> </ol> </li> </ol>"},{"location":"research/research/","title":"Talks and Research","text":"<p>Overview</p> <p>This section highlights relevant talks and research.</p>"},{"location":"research/research/#my-talks-and-research","title":"My Talks and Research","text":"Title Venue Author(s) Date (YYYY/M) Leveraging generative AI for efficient CVE enrichment CyberRisk Summit Chris Madden 2024/11 Vulnerability Root Cause Mapping with CWE: Challenges, Solutions, and Insights from Grounded LLM-based Analysis. Available soon on FIRST YouTube. FIRST VulnCon Chris Madden, Alec Summers 2025/4"},{"location":"research/research/#llms-for-cybersecurity-references","title":"LLMs For CyberSecurity References","text":"<ol> <li>Generative AI and Large Language Models for Cyber Security: All Insights You Need, May 2024</li> <li>A Comprehensive Review of Large Language Models in Cyber Security, September 2024 </li> <li>Large Language Models in Cybersecurity: State-of-the-Art, January 2024</li> <li>How Large Language Models Are Reshaping the Cybersecurity Landscape | Global AI Symposium talk, September 2024</li> <li>Large Language Models for Cyber Security: A Systematic Literature Review, July 2024</li> <li>Using AI for Offensive Security, June 2024 </li> </ol>"},{"location":"research/research/#llms-and-genai-in-general","title":"LLMs and GenAI in General","text":"Title Venue Author(s) Date (YYYY/M) Microsoft New Future of Work Report 2023 Report Microsoft 2023/12 Intelligent Agents in AI Really Can Work Alone. Here\u2019s How. Post Gartner 2024/10"},{"location":"software/claude_code/","title":"Claude code","text":"<p>https://www.builder.io/blog/claude-code https://www.youtube.com/watch?v=n7iT5r0Sl_Y</p> <p>install gh app https://youtu.be/n7iT5r0Sl_Y?t=171</p> <p>Mastering Claude Code in 30 minutes</p> <p>https://github.com/hesreallyhim/awesome-claude-code</p> <p>https://www.youtube.com/watch?v=T_IYHx-9VGU CUSTOM COMMAND FILES</p> <p>Note: all of these are saved in my home directory's (~) .claude folder: ~/.claude/commands/file.md</p> <p>Iterative Design https://sharetext.io/2b77be78</p> <p>Project Settings https://sharetext.io/279f5af9</p> <p>Git Save https://sharetext.io/3a540768 (forked from online resource)</p>"},{"location":"software/claude_code/#git-commit-well-formatted-commits","title":"Git Commit : Well Formatted Commits","text":"<p>This command helps you create well-formatted commits with conventional commit messages and emoji.</p>"},{"location":"software/claude_code/#usage","title":"Usage","text":"<p>To create a commit, just type: <pre><code>/commit\n</code></pre></p> <p>Or with options: <pre><code>/commit --no-verify\n</code></pre></p>"},{"location":"software/claude_code/#what-this-command-does","title":"What This Command Does","text":"<ol> <li>Unless specified with <code>--no-verify</code>, ALWAYS runs pre-commit checks:</li> <li>any possible linting to ensure code quality</li> <li>any possible build to verify the build succeeds</li> <li>any possible documentation generation to update documentation</li> <li>Checks which files are staged with <code>git status</code></li> <li>Automatically adds all modified and new files with <code>git add</code></li> <li>Performs a <code>git diff</code> to understand what changes are being committed</li> <li>Analyzes the diff to determine if multiple distinct logical changes are present</li> <li>Creates a commit message using emoji conventional commit format</li> </ol>"},{"location":"software/claude_code/#best-practices-for-commits","title":"Best Practices for Commits","text":"<ul> <li>Verify before committing: Ensure code is linted, builds correctly, and documentation is updated. Make sure to run ALL of the pre-commit checks first.</li> <li>Atomic commits: Each commit should contain related changes that serve a single purpose</li> <li>Split large changes: If changes touch multiple concerns, split them into separate commits</li> <li>Conventional commit format: Use the format <code>&lt;type&gt;: &lt;description&gt;</code> where type is one of:</li> <li><code>feat</code>: A new feature</li> <li><code>fix</code>: A bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>style</code>: Code style changes (formatting, etc)</li> <li><code>refactor</code>: Code changes that neither fix bugs nor add features</li> <li><code>perf</code>: Performance improvements</li> <li><code>test</code>: Adding or fixing tests</li> <li><code>chore</code>: Changes to the build process, tools, etc.</li> <li>Present tense, imperative mood: Write commit messages as commands (e.g., \"add feature\" not \"added feature\")</li> <li>Concise first line: Keep the first line under 72 characters</li> <li>Emoji: Each commit type is paired with an appropriate emoji:</li> <li>\u2728 <code>feat</code>: New feature</li> <li>\ud83d\udc1b <code>fix</code>: Bug fix</li> <li>\ud83d\udcdd <code>docs</code>: Documentation</li> <li>\ud83d\udc84 <code>style</code>: Formatting/style</li> <li>\u267b\ufe0f <code>refactor</code>: Code refactoring</li> <li>\u26a1\ufe0f <code>perf</code>: Performance improvements</li> <li>\u2705 <code>test</code>: Tests</li> <li>\ud83d\udd27 <code>chore</code>: Tooling, configuration</li> <li>\ud83d\ude80 <code>ci</code>: CI/CD improvements</li> <li>\ud83d\uddd1\ufe0f <code>revert</code>: Reverting changes</li> <li>\ud83e\uddea <code>test</code>: Add a failing test</li> <li>\ud83d\udea8 <code>fix</code>: Fix compiler/linter warnings</li> <li>\ud83d\udd12\ufe0f <code>fix</code>: Fix security issues</li> <li>\ud83d\udc65 <code>chore</code>: Add or update contributors</li> <li>\ud83d\ude9a <code>refactor</code>: Move or rename resources</li> <li>\ud83c\udfd7\ufe0f <code>refactor</code>: Make architectural changes</li> <li>\ud83d\udd00 <code>chore</code>: Merge branches</li> <li>\ud83d\udce6\ufe0f <code>chore</code>: Add or update compiled files or packages</li> <li>\u2795 <code>chore</code>: Add a dependency</li> <li>\u2796 <code>chore</code>: Remove a dependency</li> <li>\ud83c\udf31 <code>chore</code>: Add or update seed files</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb <code>chore</code>: Improve developer experience</li> <li>\ud83e\uddf5 <code>feat</code>: Add or update code related to multithreading or concurrency</li> </ul>"},{"location":"software/introduction/","title":"How AI is Changing Software Engineering","text":"<p>Overview</p> <p>This section gives an overview of Software Engineering and how it has evolved with AI and Generative AI.</p> <p>It looks at the early days of Software Engineering and the transition from Waterfall to Agile - and the emphasis on communication over code and documentation. </p> <p>Then it looks at how AI gave us Software 2.0 where \"AI is eating software\", and how Generative AI gave us Software 1.0 Redux where \"AI is feeding software\" (and Vibe Coding).</p> <p>Andrej Karpathy quotes are used throughout as a consistent commentary from someone who has been formative in this space.</p> <p>Software engineering </p> <p>Quote</p> <p>Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.</p> <p>https://en.wikipedia.org/wiki/Software_engineering</p>"},{"location":"software/introduction/#evolution-of-software-engineering-as-communication-over-code","title":"Evolution of Software Engineering as Communication over Code","text":"<p>Effective software development depends fundamentally on clear communication between all stakeholders\u2014from business experts to developers to end users. The evolution from rigid, document-heavy approaches to collaborative, feedback-driven methodologies illustrates how the industry has learned to prioritize human communication over process documentation.</p>"},{"location":"software/introduction/#the-waterfall-challenge","title":"The Waterfall Challenge","text":"<p>The Waterfall model dominated software development for decades through its linear, sequential approach. However, this methodology created significant communication barriers. Requirements, design documents, and other pre-coding artifacts quickly became outdated as projects progressed. The code itself became the only reliable source of truth, but most stakeholders couldn't understand it.</p> <p>This disconnect between documentation and reality often led teams to solve the wrong problems. The fundamental issue was the lack of continuous feedback loops between those who understood the business needs and those building the software.</p>"},{"location":"software/introduction/#the-agile-response","title":"The Agile Response","text":"<p>During the 1990s, lightweight development methodologies emerged in response to these heavyweight, document-centric approaches. Agile software development became the most influential of these approaches, emphasizing iterative development, continuous feedback, and direct collaboration between business stakeholders and development teams.</p> <p>Rather than relying on comprehensive documentation, Agile methodologies prioritize working software and frequent communication. This shift recognized that software development is fundamentally a collaborative, creative process that benefits from ongoing dialogue rather than upfront specification.</p>"},{"location":"software/introduction/#the-language-of-collaboration","title":"The Language of Collaboration","text":"<p>Tip</p> <p>See the Design Council's Systemic Design Framework for an evolution of the Double Diamond that recognizes the importance of the 'invisible activities' that sit around the design process: orientation and value setting, continuing the journey, collaboration and connection, and leadership and storytelling.</p> <p>Software engineering thought leaders have consistently emphasized how critical shared understanding becomes in successful projects.</p> <ul> <li> <p>Alistair Cockburn's research demonstrated that face-to-face communication, particularly when enhanced by shared modeling tools like whiteboards, represents the most effective form of team communication. His work on communication effectiveness showed that physical proximity and multiple communication modalities dramatically improve information transfer and reduce misunderstandings.</p> </li> <li> <p>Eric Evans, in his foundational work on Domain-Driven Design, introduced the concept of \"ubiquitous language\"\u2014a shared vocabulary that bridges the gap between business experts and technical teams. This common language emerges from ongoing collaboration and becomes embedded in both conversations and code, ensuring that business terminology permeates throughout the software system.</p> </li> <li> <p>Dan North extended these ideas when developing Behavior-Driven Development (BDD), creating frameworks that use natural language constructs to describe software behavior in terms that all stakeholders can understand. BDD treats the specification of desired behavior as a ubiquitous language for the entire project team, helping prevent communication breakdowns between developers and business stakeholders.</p> </li> </ul> <p>These approaches share a common insight: successful software development requires more than technical expertise\u2014it demands ongoing communication, shared vocabulary, and collaborative understanding of both the problem domain and the solution being built.</p>"},{"location":"software/introduction/#software-20","title":"Software 2.0","text":"<p>The emergence of artificial intelligence is fundamentally changing how we think about software development.</p> <p>In 2017, Andrej Karpathy introduced the concept of \"Software 2.0\" to describe this transformation, where traditional human-written code gives way to AI-generated solutions.</p> <p>Quote</p> <p>Software (1.0) is eating the world, and now AI (Software 2.0) is eating software.</p> <p>The \"classical stack\" of Software 1.0 is what we're all familiar with \u2014 it is written in languages such as Python, C++, etc. It consists of explicit instructions to the computer written by a programmer. By writing each line of code, the programmer identifies a specific point in program space with some desirable behavior.</p> <p>In contrast, Software 2.0 is written in much more abstract, human unfriendly language, such as the weights of a neural network. No human is involved in writing this code because there are a lot of weights (typical networks might have millions), and coding directly in weights is kind of hard.</p> <p>In Software 1.0, human-engineered source code (e.g. some .cpp files) is compiled into a binary that does useful work. </p> <p>In Software 2.0 most often the source code comprises </p> <ol> <li>the dataset that defines the desirable behavior and </li> <li>the neural net architecture that gives the rough skeleton of the code, but with many details (the weights) to be filled in.</li> </ol> <p>It is likely that any setting where the program is not obvious but one can repeatedly evaluate the performance of it (e.g. \u2014 did you classify some images correctly? do you win games of Go?) will be subject to this transition, because the optimization can find much better code than what a human can write.</p> <p>Andrej Karpathy, Nov 11, 2017 https://karpathy.medium.com/software-2-0-a64152b37c35</p> <p>Note</p> <p>\"Software (1.0) is eating the world\" is a reference to \"Why Software Is Eating the World\", Marc Andreessen, 2011.</p> <p>https://a16z.com/why-software-is-eating-the-world/ where a16z is \"AndreessenHorowitz.\"</p>"},{"location":"software/introduction/#software-20-example-tesla-autopilot-transition-from-c-code-to-ai","title":"Software 2.0 Example: Tesla Autopilot transition from C++ code to AI","text":"<p>Tesla's Autopilot system demonstrates this transition in practice. As Karpathy explained:</p> <p>Quote</p> <p>Neural network can eat through the [programming] stack\u2026 When I joined Tesla, there was a ton of C++ code, and now there's much, much less C++ code in the code that runs in the car. </p> <p>Neural network initially was just doing a detection on the image level, then it went for multiple images, it gives you prediction, then multiple images over time give you a prediction, and you're discarding C++ code. And eventually you're just giving steering commands.</p> <p>\u2026 I do suspect that the end-to-end systems for Tesla in, say, 10 years, it is just a neural net. I mean, the videos stream into a neural net and commands come out.</p> <p>https://www.linkedin.com/pulse/andrej-karpathy-8-big-ideas-mikael-alemu-gorsky-eckuf/</p> <p>This evolution illustrates how AI systems can progressively replace traditional programmatic logic, moving toward end-to-end neural networks that process inputs and generate outputs directly.</p>"},{"location":"software/introduction/#data-as-the-new-source-code","title":"Data as the New Source Code","text":"<p>In this Software 2.0 world, datasets become the primary artifact of development.</p> <p>Hugging Face has emerged as \"Software 2.0's GitHub,\" hosting over 400K+ datasets alongside 1.7M+ models, where repositories contain datasets and commits represent additions and edits of labels rather than code changes.</p> <p>This shift fundamentally changes how we think about version control, collaboration, and the artifacts that define our systems. The focus moves from managing code repositories to curating and versioning the data that trains our AI systems.</p> <p>As Karpathy noted:</p> <p>Quote</p> <p>Github is a very successful home for Software 1.0 code. Is there space for a Software 2.0 Github? In this case repositories are datasets and commits are made up of additions and edits of the labels.</p> <p>https://karpathy.medium.com/software-2-0-a64152b37c35</p>"},{"location":"software/introduction/#software-10-redux","title":"Software 1.0 Redux","text":"<p>Note</p> <p>\"Software 1.0 Redux\" refers to how fundamental software engineering principles remain unchanged, but the development process itself has been transformed by Generative AI and LLMs. All traditional artifacts become even more relevant as AI-generated code depends on high-quality specifications.</p> <p>Quote</p> <p>\"The hottest new programming language is English.\"  </p> <p>\u2014Andrej Karpathy on X/Twitter, 2023 </p> <p>Not all problems are suited to the Software 2.0 paradigm of neural networks replacing traditional code.</p> <p>For such problems, Large Language Models (LLMs) are transforming how we write traditional software, creating what we might call \"Software 1.0 Redux\", where AI feeds and enhances traditional software, rather than eating it \u00e0 la Software 2.0.</p> <ul> <li>While AI handles low-level implementation details, human developers are elevated to higher levels of abstraction, using natural language to specify intent and behavior.</li> <li>The skill shifts from syntax mastery to effective requirements management, architecture, and system design, and clean coding principles.</li> </ul>"},{"location":"software/introduction/#thoughts-on-the-future-from-the-past","title":"Thoughts on the Future from the Past","text":"<p>In the past, I captured some thoughts publicly on how I thought the future of software would play out.</p> <p>These predictions have materialized through today's Generative AI tools, though at the time the Generative AI that we take for granted today did not exist.</p>"},{"location":"software/introduction/#more-continuity-between-requirements-documentation-test-sw","title":"More Continuity between Requirements \u2013 Documentation \u2013 Test \u2013 SW","text":"<p>Six years ago (March 2019), as part of a presentation on DevSecOps - Thoughts from the Trenches, I concluded with my view of the next 10 years:</p> <p></p> <p>For \"More Continuity between Requirements \u2013 Documentation \u2013 Test \u2013 SW\", these points were already playing out at that time:</p> <ul> <li>Less heavy lifting required by people to build software</li> <li>The breadth and depth and rate of change of new technologies means that developers can't keep pace</li> <li>Value moves further up the stack \u2013 the lower layers become commodities</li> </ul> <p>Tip</p> <p>The \"More Continuity between Requirements \u2013 Documentation \u2013 Test \u2013 SW\" is now playing out as \"Software 1.0 Redux\" where the non-code artifacts (requirements, architecture and design documentation, tests, user documentation) become the contracts from which code is generated by LLMs\u2014and the code becomes more of a byproduct. These contracts are maintained and versioned alongside the code.</p> <p>Natural language prompts have become first-class citizens in development workflows, allowing developers to specify intent at higher levels of abstraction while AI handles implementation details. Prompt engineering is an important skill currently, though the need for prompt engineering diminishes with more capable models (e.g. Reasoning, Research).</p>"},{"location":"software/introduction/#diagrams-as-code","title":"Diagrams as Code","text":"<p>Five years ago (May 2020), as part of a \"Thoughts For The Future\" chapter for The Hitchhiker's Guide to PlantUML! (linked from plantuml.com), I wrote:</p> <p>Quote</p> <p>1.3. Machine Processing Of Text Files</p> <p>Having a diagram source as a text file is powerful because it allows for machine processing e.g.</p> <ol> <li> <p>If standard building blocks are used, it allows automated analysis and recognition of the diagram text source, and recommendations to the user e.g. if an arrow text includes \"TLS\" to indicate the link is secured, then an external program can provide recommendations on TLS protocol version, cipher-suites etc\u2026</p> </li> <li> <p>As companies move towards standard architecture icon sets (AWS, MS/Azure, Google,\u2026), it is possible to process an existing architecture diagram image with optical recognition (and machine learning) and create the text (plantuml) equivalent.</p> </li> </ol> <p>Quote</p> <p>Having diagrams as code that can live beside the system code, that the stakeholders are equally comfortable editing and viewing, reduces the gap i.e. \"Where system diagrams meet system reality\"</p> <p>https://crashedmind.github.io/PlantUMLHitchhikersGuide/about/AboutPlantUML.html#diagrams-as-code</p> <p>I felt strongly enough about this that I wrote a guide on PlantUML, a diagram-as-code tool.</p> <p>Experiment</p> <p>One of my first experiments with an LLM (text-only) was to ask it to do a security review of a diagram-as-code architecture.</p> <p>And when multi-modal LLMs became available, with a diagram-as-image architecture.</p>"},{"location":"software/introduction/#validation-through-current-ai-tools","title":"Validation Through Current AI Tools","text":"<p>These predictions have materialized through today's generative AI tools:</p> <ul> <li>e.g. Claude's Artifacts feature exemplifies the upstream artifacts concept, allowing users to generate substantial, standalone content like code snippets, documents, or website designs that appear in a dedicated window for iterative refinement. Meanwhile, Claude Code takes this further by operating directly in the terminal, understanding entire codebases and helping developers code faster by executing routine tasks, explaining complex code, and handling git workflows through natural language commands.</li> <li>The \"diagrams as code\" vision has also been realized\u2014modern AI can now generate Mermaid diagrams, SVG graphics, and complete architecture visualizations from natural language descriptions, then provide contextual analysis including security recommendations and threat modeling based on those diagrams. It can also generate the code given an image.</li> <li>I've used LLMs to create architecture diagrams, and then generate code from those diagrams.</li> </ul> <p>But AWS DevGenius is a more recent complete example application that exemplifies this.</p>"},{"location":"software/introduction/#example-devgenius-aws-solution-generator","title":"Example DevGenius - AWS Solution Generator","text":"<p>AWS DevGenius (April 2025) is an example of this.</p> <p>Quote</p> <p>DevGenius is an AI-powered application that transforms project ideas into complete, ready-to-deploy AWS solutions. It leverages Amazon Bedrock and Claude AI models to provide architecture diagrams, cost estimates, infrastructure as code, and comprehensive technical documentation.</p> <p>Conversational Solution Architecture Building: DevGenius enables customers to design solution architectures in a conversational manner. Users can create architecture diagrams (in draw.io format) and refine them interactively. Once the design is finalized, they can generate end-to-end code automation using CDK or CloudFormation templates, and deploy it in their AWS account with a single click. Additionally, customers can receive cost estimates for running the architecture in production, along with detailed documentation for the solution.</p> <p>Build Solution Architecture from Whiteboard Drawings: For customers who already have their architecture in image form (e.g., whiteboard drawings), DevGenius allows them to upload the image. Once uploaded, DevGenius analyzes the architecture and provides a detailed explanation. Customers can then refine the design conversationally and, once finalized, generate end-to-end code automation using CDK or CloudFormation. Cost estimates and comprehensive documentation are also available.</p>"},{"location":"software/introduction/#from-code-centric-to-contract-centric-development","title":"From Code-Centric to Contract-Centric Development","text":"<p>This transformation reflects a broader trend toward what a16z calls \"upstream artifacts\" becoming the primary deliverables:</p> <p>In agent-driven workflows, code becomes a byproduct\u2014like a compiled artifact\u2014while high-level inputs become the core deliverable. Code becomes the byproduct of those inputs, more like a compiled artifact than a manually authored source.</p>"},{"location":"software/introduction/#the-new-development-hierarchy","title":"The New Development Hierarchy","text":"Aspect Traditional (Software 1.0) AI-Enhanced (Software 1.0 Redux) Primary Artifact Code Requirements, contracts, specifications Developer Role Implementation focused Architecture and validation focused Skill Emphasis Syntax, algorithms, debugging Requirements engineering, system design Version Control Code changes Contract changes + generated artifacts"},{"location":"software/introduction/#critical-upstream-artifacts","title":"Critical Upstream Artifacts","text":"<p>The most valuable artifacts in AI-driven development become the specifications that guide code generation:</p> Artifact Category Examples AI-Centric Value Requirements User stories, acceptance criteria, functional specs Clear evaluation targets for AI validation Architecture &amp; Design ADRs, system diagrams, API contracts Unambiguous constraints for code generation Data &amp; Schemas JSON schemas, database models, API specifications Structured templates for consistent output Security &amp; Compliance Threat models, security policies, audit requirements Automated guardrails and validation rules"},{"location":"software/introduction/#everything-as-code-the-infrastructure-precedent","title":"Everything-as-Code: The Infrastructure Precedent","text":"<p>This shift toward specification-driven development builds on existing \"everything-as-code\" movements:</p> <ul> <li>Infrastructure as Code (Terraform, CloudFormation)</li> <li>Policy as Code (Open Policy Agent)</li> <li>Diagrams as Code (Mermaid, PlantUML)</li> <li>Configuration as Code (Kubernetes manifests)</li> </ul> <p>These approaches already demonstrated the value of maintaining human-readable specifications that generate operational artifacts. AI extends this pattern to application code itself.</p>"},{"location":"software/introduction/#blurred-boundaries-developer-roles-evolve","title":"Blurred Boundaries: Developer Roles Evolve","text":"<p>The lines between traditional software engineering roles are blurring:</p> <p>Developer + Requirements Analyst: Developers must now excel at translating business needs into precise specifications that AI can implement reliably.</p> <p>Human + Machine Collaboration: Success requires understanding both what AI can do well (pattern matching, code generation) and what humans do better (creative problem-solving, architectural judgment).</p> <p>This evolution suggests developers will spend less time on implementation minutiae and more time on:</p> <ul> <li>System design and architectural decision-making</li> <li>Requirements engineering and specification writing  </li> <li>Quality validation of AI-generated outputs</li> <li>Integration orchestration across AI and human-created components</li> </ul>"},{"location":"software/introduction/#coding-for-ai-agents-vs-humans","title":"Coding for AI Agents vs. Humans","text":"<p>Coding for AI Agents vs. Humans, May 2025, Martin Treiber</p> <p>Quote</p> <p>As AI coding assistants and autonomous agents (often powered by large language models) become more involved in software development, best practices in coding must account for a new \"audience.\" Traditionally, code is written by and for human developers, emphasizing readability and maintainability for people. In contrast, code intended to be generated or maintained by AI agents may prioritize different qualities to align with an AI's interpretive capabilities.</p> ASPECT CODE OPTIMIZED FOR HUMAN DEVS CODE OPTIMIZED FOR AI AGENTS Design &amp; Patterns Uses established design patterns and abstractions for maintainability; architecture planned with human intuition in mind. E.g. heavily employs DRY and common patterns to avoid duplication. Prefers straightforward implementations unless instructed otherwise; may duplicate logic or use simpler patterns by default. Requires explicit guidance to enforce complex patterns or project-specific architectures. Readability &amp; Naming Emphasizes self-explanatory code: clear but concise names, standard formatting, and comments only where needed. Relies on code being self-documenting and consistent by convention. Emphasizes explicit clarity: very descriptive names (the AI doesn't mind length), extremely consistent naming and styling. Heavy inline documentation and docstrings are included to provide context (since AI can update). Performance Trade-offs Prioritizes readability and maintainability; avoids premature optimizations. Optimizes hotspots after profiling, using complex techniques only with justification (and documenting them for colleagues). Prioritizes correctness and completeness on first pass; optimizations are applied via iteration or prompts. Tends toward robust, error-checked code even if slightly slower. Can leverage known optimizations or libraries. <p>Ultimately, good code is good code \u2013 many best practices overlap whether the consumer is human or AI. Clean architecture, readable style, and efficient execution benefit both. The key differences lie in emphasis: AI agents \"read\" code differently than humans, so code meant for them leans into consistency, explicit context, and simplicity of structure.</p>"},{"location":"software/introduction/#from-professional-development-to-rapid-prototyping-via-vibe-coding","title":"From Professional Development to Rapid Prototyping via Vibe Coding","text":"<p>While professional software engineering requires rigorous practices, AI has also enabled what Andrej Karpathy terms \"vibe coding\"\u2014 a more experimental approach where developers \"see things, say things, run things, and copy-paste things, and it mostly works.\"</p> <p>Though Karpathy notes this isn't traditional software engineering and has limitations, vibe coding serves an important role in enabling rapid prototyping and proof-of-concepts for both experienced developers and newcomers to programming.</p>"},{"location":"software/introduction/#ai-across-the-development-lifecycle","title":"AI Across the Development Lifecycle","text":"<p>Tracy Bannon's excellent talk Applying AI to the SDLC: New Ideas and Gotchas! - Leveraging AI to Improve Software Engineering includes a model for where AI can be used with DevSecOps.</p> <p></p> <p>It demonstrates how AI can enhance every phase of software development:</p> Phase AI Applications Current Tools Plan &amp; Design Generate design alternatives, review architectural decisions Claude Artifacts, GitHub Copilot Chat Code Auto-complete, generate from natural language specs GitHub Copilot, Claude Code, Cursor Test &amp; QA Auto-generate test cases, intelligent fuzzing GitHub Copilot, automated testing tools Deploy &amp; Ops Predictive scaling, infrastructure optimization AI-powered DevOps platforms Monitor Automated root-cause analysis, performance insights AI monitoring and observability tools <p>This comprehensive integration shows AI's potential to transform every aspect of the software development lifecycle.</p>"},{"location":"software/introduction/#takeaways","title":"Takeaways","text":"<p>Key Takeaways</p> <ol> <li> <p>Accelerated Innovation: AI compresses idea-to-prototype cycles from weeks to minutes, fundamentally changing development velocity.</p> </li> <li> <p>Inverted Priorities: Upstream artifacts (requirements, architecture, contracts) become primary deliverables, while code becomes a generated byproduct.</p> </li> <li> <p>Evolved Skill Sets: Success now requires mastering requirements engineering, system architecture, and human-AI collaboration alongside traditional coding skills.</p> </li> <li> <p>New Paradigms: We're witnessing both Software 2.0 (AI eating software) and Software 1.0 Redux (AI feeding traditional development), each suited to different problem domains.</p> </li> </ol>"},{"location":"software/overview/","title":"Overview","text":"<p>Abstract</p> <p>This Software Engineering section explores how AI is fundamentally transforming software engineering, from changing development paradigms to redefining the role of traditional artifacts. </p> <p>It covers immutable principles that remain constant regardless of who or what creates the software, and examines the evolving landscape of AI-powered development tools.</p>"},{"location":"software/overview/#1-how-ai-is-changing-software-engineering","title":"1. How AI is Changing Software Engineering","text":"<p>An examination of software engineering's evolution through three distinct eras: traditional Waterfall methodologies, Agile development with its emphasis on communication over documentation, and the current AI-driven transformation.</p> <p>The analysis traces two parallel developments: </p> <ol> <li>Software 2.0 where \"AI is eating software\" by replacing traditional algorithms with neural networks, </li> <li>Software 1.0 Redux where \"AI is feeding software\" through generative tools that accelerate traditional development (introducing the concept of \"Vibe Coding\").</li> </ol> <p>Key Takeaways:</p> <ul> <li>Accelerated Innovation: AI compresses idea-to-prototype cycles from weeks to minutes, fundamentally changing development velocity</li> <li>Inverted Priorities: Upstream artifacts (requirements, architecture, contracts) become primary deliverables, while code becomes a generated byproduct</li> <li>Evolved Skill Sets: Success now requires mastering requirements engineering, system architecture, and human-AI collaboration alongside traditional coding skills</li> <li>New Paradigms: We're witnessing both Software 2.0 (AI eating software) and Software 1.0 Redux (AI feeding traditional development), each suited to different problem domains</li> </ul>"},{"location":"software/overview/#2-software-assurance","title":"2. Software Assurance","text":"<p>A comprehensive overview of Software Assurance fundamentals that remain constant across all Software Development Life Cycle (SDLC) methodologies and development entities. This section covers the immutable principles that govern quality software development.</p> <ul> <li>verification (\"Are you building it right?\") versus validation (\"Are you building the right thing?\")</li> <li>fundamental architectural principles including the Laws of Software Architecture and how quality attributes drive design decisions.</li> </ul> <p>Key Takeaways:</p> <ul> <li>The software requirements specification is the key artifact that distinguishes verification activities from validation activities</li> <li>Understanding the what (requirements) independent of the how (design) is crucial</li> <li>The why for the how (captured in Architecture Decision Records) is paramount: \"Why is more important than how\"</li> <li>Everything in software architecture involves trade-offs</li> <li>Quality attributes are the primary drivers of architectural design</li> </ul>"},{"location":"software/overview/#3-software-artifacts","title":"3. Software Artifacts","text":"<p>Building on the foundational concepts from the previous sections, this explores how software engineering artifacts are evolving from passive documentation into active contracts that guide AI-driven code generation and system behavior.</p> <p>As AI transforms development workflows, these artifacts become the core deliverables that specify system requirements, architecture, and behavior in machine-readable formats that AI tools can directly consume and act upon.</p> <p>Key Takeaways:</p> <ol> <li>Precision Over Brevity - AI needs explicit, unambiguous specifications</li> <li>Machine-Readable Formats - Use structured data (JSON, YAML, Markdown with frontmatter)</li> <li>Version Everything - Track changes to specifications alongside generated code</li> <li>Automate Validation - Ensure AI outputs meet specification requirements</li> <li>Maintain Traceability - Link generated code back to originating specifications</li> </ol>"},{"location":"software/overview/#4-ai-coding-tools","title":"4. AI Coding Tools","text":"<p>A comprehensive analysis of AI-powered development tools, categorized by autonomy levels from basic code completion to autonomous feature development. This classification helps developers understand the capabilities and limitations of different tools.</p> <p>The comparison spans from Level 1 (basic autocomplete) through Level 4 (autonomous agents), with Level 5 remaining theoretical. The analysis shows how AI tools are progressively taking on more of the coding burden, allowing developers to focus on higher-level concerns.</p> <p>Key Takeaways:</p> <ul> <li>Tools range from simple code completion to autonomous feature development</li> <li>Developers can mix and match tools based on autonomy levels and specific needs</li> <li>Understanding each tool's autonomy level is crucial for effective adoption</li> <li>The field evolves rapidly, with tools advancing between autonomy levels frequently</li> </ul> <p>Tip</p> <p>See also Feature Comparison and Analysis of Leading AI IDEs: Cursor, Windsurf, Roo Code, and GitHub Copilot, May 27, 2025.</p>"},{"location":"software/overview/#5-leaderboards","title":"5. Leaderboards","text":"<p>A curated collection of benchmarks and evaluation metrics for Large Language Models (LLMs) specifically focused on coding capabilities. These leaderboards provide objective measures for comparing the performance of different AI models in software development tasks.</p> <p>Key Takeaways:</p> <ul> <li>Leaderboards are a good way to quantitatively and objectively compare solutions.</li> <li>Comparison across multiple metrics and leaderboards avoids solutions that overfit to a benchmark.</li> </ul>"},{"location":"software/overview/#takeaways","title":"Takeaways","text":"<p>Key Takeaways</p> <p>This Software Engineering section demonstrates how AI is not merely automating existing processes but fundamentally reshaping software engineering. While core principles like verification, validation, and architectural trade-offs remain constant, the methods of implementation are evolving rapidly. The shift toward AI-driven development emphasizes the critical importance of well-defined specifications and requirements as the new primary deliverables, with generated code becoming a secondary artifact.</p> <p>Success in this new paradigm requires mastering both traditional software engineering principles and emerging AI collaboration skills, understanding when to apply different levels of AI autonomy, and maintaining quality through robust specification and validation processes.</p>"},{"location":"software/software_artifacts/","title":"Software Engineering Artifacts in the AI Era","text":"<p>Overview</p> <p>This section builds on How AI is Changing Software Engineering and Software Assurance to focus on the Software Engineering artifacts, which are becoming the core deliverable, the active contracts that guide code generation and system behavior. </p>"},{"location":"software/software_artifacts/#artifacts","title":"Artifacts","text":""},{"location":"software/software_artifacts/#1-research-discovery","title":"1. Research &amp; Discovery","text":"<ul> <li>Technical Research Documentation - Market analysis, technology evaluations, feasibility studies</li> <li>AI Model Research - Model selection criteria, capability assessments, performance benchmarks</li> <li>Competitive Analysis - Feature comparisons, architectural patterns, industry standards</li> </ul>"},{"location":"software/software_artifacts/#2-requirements-specifications","title":"2. Requirements &amp; Specifications \u2b50","text":"<p>These become critical contracts for AI code generation</p> <ul> <li>User Stories with clear acceptance criteria for AI validation</li> <li>Functional &amp; Non-functional Requirements - Precise, unambiguous specifications</li> <li>Use-case Diagrams and Scenarios - Visual specifications for complex workflows</li> <li>Feature Backlog with AI-friendly descriptions and constraints</li> <li>Prompt Libraries - Reusable natural language specifications for common patterns</li> <li>Prompt Engineering Specifications - Detailed instructions optimized for AI performance and reliability</li> </ul>"},{"location":"software/software_artifacts/#3-architecture-design","title":"3. Architecture &amp; Design \u2b50","text":"<p>Primary artifacts that guide AI implementation</p> <ul> <li>Architecture Diagrams (UML, component diagrams, system context)</li> <li>Architectural Decision Records (ADRs) - Critical for maintaining AI-generated code consistency</li> <li>System Design Documents - High-level constraints and patterns</li> <li>API Contracts &amp; Specifications (OpenAPI/Swagger) - Precise interfaces for AI code generation</li> <li>Data Schemas (JSON Schema, database models) - Structured templates for consistent output</li> <li>Diagrams-as-Code (Mermaid, PlantUML) - Machine-readable architectural specifications</li> <li>Diagrams-as-Images (any) - Multi-Modal-LLM-readable architectural specifications</li> </ul>"},{"location":"software/software_artifacts/#4-implementation","title":"4. Implementation","text":"<ul> <li>Generated Source Code - Now a byproduct of upstream specifications</li> <li>Code Repositories with enhanced metadata tracking (human vs. AI contributions)</li> <li>AI Generation Prompts - Versioned natural language instructions</li> <li>Code Review Guidelines for AI-generated code</li> <li>Refactoring Documentation - Plans and histories for AI-assisted refactoring</li> <li>Legacy Migration Plans - Strategies for AI-assisted modernization</li> <li>Dataset Management - Versioning, curation, and labeling of training data as code artifacts</li> <li>Traceability Matrices - Automated linkage between specifications, generated code, and test artifacts</li> <li>Metadata Standards - Explicit schema for documenting AI contributions vs. human edits, linked to audit trails</li> </ul>"},{"location":"software/software_artifacts/#5-testing-quality-assurance","title":"5. Testing &amp; Quality Assurance","text":"<ul> <li>Test Plans with AI-generated test case specifications</li> <li>Automated Test Suites (unit, integration, end-to-end)</li> <li>AI-Generated Test Cases - Coverage for edge cases and scenarios</li> <li>Quality Gates - Automated validation rules for AI-generated code</li> <li>Performance Benchmarks - Baseline metrics for AI optimization</li> <li>Bug Tracking with AI-assistance classification and resolution suggestions</li> </ul>"},{"location":"software/software_artifacts/#6-security-compliance","title":"6. Security &amp; Compliance \u2b50","text":"<p>Critical guardrails for AI-generated code</p> <ul> <li>Threat Models - Security constraints for AI code generation</li> <li>Security Policies as Code - Automated security validation rules</li> <li>Vulnerability Scans of AI-generated code</li> <li>Compliance Checklists - Regulatory requirements embedded in AI workflows</li> <li>Audit Trails - Enhanced tracking of AI-generated changes</li> <li>Code Reviews - Code Reviews by AI</li> </ul>"},{"location":"software/software_artifacts/#7-deployment-operations","title":"7. Deployment &amp; Operations","text":"<ul> <li>Infrastructure as Code (IaC) - Terraform, CloudFormation specifications</li> <li>CI/CD Pipeline Configurations - Automated deployment workflows</li> <li>Release Notes - Generated from commit messages and change logs</li> <li>Monitoring &amp; Observability - AI-powered dashboards and alerting</li> <li>Runbooks - Operational procedures with AI-assisted troubleshooting</li> </ul>"},{"location":"software/software_artifacts/#8-maintenance-evolution","title":"8. Maintenance &amp; Evolution","text":"<ul> <li>Change Logs - Automated tracking of modifications</li> <li>Incident Response - AI-assisted root cause analysis</li> <li>Post-mortem Reports - Learning from failures with AI insights</li> <li>Dependency Management - Automated updates and security patches</li> <li>Technical Debt Tracking - AI-identified improvement opportunities</li> <li>Continuous Improvement Plans - Strategies integrating feedback loops from operational AI performance to refine specifications and AI model performance over time</li> </ul>"},{"location":"software/software_artifacts/#9-communication-collaboration","title":"9. Communication &amp; Collaboration","text":"<ul> <li>Project Documentation - Living documents that evolve with the codebase</li> <li>Team Knowledge Base - Searchable, AI-enhanced documentation</li> <li>Decision Logs - Context for architectural and design choices</li> <li>User Documentation - Generated from code and specifications</li> <li>Training Materials - AI-assisted onboarding and skill development</li> <li>Role Definitions for Human-AI Interaction - Clear delineation of responsibilities, authority, and interaction protocols for AI vs. human developers</li> <li>Ubiquitous Language Glossaries - Domain-driven language consistently used across requirements, code, prompts, and AI interactions</li> </ul>"},{"location":"software/software_artifacts/#artifact-formats-for-ai-integration","title":"Artifact Formats for AI Integration","text":"<p>Multi-modal LLMs can process diverse document formats, but strategic format selection enhances AI comprehension and workflow efficiency.</p>"},{"location":"software/software_artifacts/#recommended-primary-formats","title":"Recommended Primary Formats","text":"<p>Markdown (.md) - Optimal for AI processing</p> <ul> <li>Semi-structured, lightweight format ideal for LLMs</li> <li>Human-readable with machine-parseable structure</li> <li>Supports frontmatter for metadata</li> <li>Version control friendly</li> </ul> <p>Structured Data</p> <ul> <li>JSON/YAML - Configuration files, schemas, API definitions</li> <li>TOML - Configuration with human-friendly syntax</li> <li>CSV - Tabular data, test cases, requirements matrices</li> </ul> <p>Dataset Management Formats - CSV, Parquet, JSONL for structured training data versioning and curation</p> <p>Diagrams-as-Code</p> <ul> <li>Mermaid - Flowcharts, sequence diagrams, system architecture</li> <li>PlantUML - UML diagrams, component relationships</li> <li>DOT/Graphviz - Network diagrams, dependency graphs</li> </ul>"},{"location":"software/software_artifacts/#document-conversion-pipeline","title":"Document Conversion Pipeline","text":"<p>Modern tools enable seamless format conversion for AI consumption:</p> Tool Input Formats Use Case MarkItDown PDF, DOCX, PPTX, XLSX, Images Legacy document conversion Docling PDF, DOCX, HTML, Images Enterprise document processing Pandoc 40+ formats Universal document conversion <p>Best Practices:</p> <ul> <li>Start with Markdown for new artifacts</li> <li>Convert legacy documents using automated tools</li> <li>Use frontmatter for metadata (tags, version, AI instructions)</li> <li>Maintain original formats alongside Markdown for compliance</li> </ul> <p>This approach ensures artifacts remain both human-readable and AI-processable throughout the development lifecycle.</p>"},{"location":"software/software_artifacts/#ai-enhanced-artifact-workflow","title":"AI-Enhanced Artifact Workflow","text":""},{"location":"software/software_artifacts/#traditional-vs-ai-driven-development","title":"Traditional vs. AI-Driven Development","text":"Aspect Traditional Approach AI-Enhanced Approach Primary Focus Code implementation Specification quality Artifact Role Documentation Active contracts Maintenance Manual updates Automated synchronization Quality Assurance Post-development review Continuous validation Knowledge Transfer Human documentation AI-searchable knowledge base"},{"location":"software/software_artifacts/#repository-structure-for-ai-first-development","title":"Repository Structure for AI-First Development","text":"<pre><code>project-root/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 CONTRIBUTING.md\n\u251c\u2500\u2500 .ai/                                    # AI-specific configurations\n\u2502   \u251c\u2500\u2500 prompts/                           # Reusable prompt templates\n\u2502   \u251c\u2500\u2500 models/                            # Model configurations\n\u2502   \u2514\u2500\u2500 validation/                        # AI output validation rules\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 system-docs/                       # System-focused documentation\n\u2502   \u2502   \u251c\u2500\u2500 01-research/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 technical-research.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai-model-research.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 competitive-analysis.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 02-requirements/               # \u2b50 CRITICAL for AI generation\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 user-stories.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 functional-requirements.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 use-case-scenarios.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 feature-backlog.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt-libraries.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 prompt-engineering-specifications.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 03-architecture/               # \u2b50 PRIMARY contracts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 architecture-diagrams/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 diagrams-as-code/\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 diagrams-as-images/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ADR/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 system-design-documents.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api-contracts/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 data-schemas/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 04-implementation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 generated-source-code/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 code-repositories-metadata.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai-generation-prompts.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 code-review-guidelines.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 refactoring-documentation.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 legacy-migration-plans.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset-management.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 traceability-matrices.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 metadata-standards.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 05-testing/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test-plans.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 automated-test-suites.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ai-generated-test-cases.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 quality-gates.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 performance-benchmarks.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bug-tracking.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 06-security/                   # \u2b50 AI guardrails\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 threat-models.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 security-policies-as-code.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vulnerability-scans.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 compliance-checklists.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 audit-trails.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ai-code-reviews.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 07-deployment-operations/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 infrastructure-as-code/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ci-cd-pipeline-configurations.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 release-notes.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 monitoring-observability.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 runbooks.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 08-maintenance-evolution/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 change-logs.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 incident-response.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 post-mortem-reports.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dependency-management.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 technical-debt-tracking.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 continuous-improvement-plans.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 09-communication-collaboration/\n\u2502   \u2502       \u251c\u2500\u2500 project-documentation.md\n\u2502   \u2502       \u251c\u2500\u2500 team-knowledge-base.md\n\u2502   \u2502       \u251c\u2500\u2500 decision-logs.md\n\u2502   \u2502       \u251c\u2500\u2500 role-definitions-human-ai.md\n\u2502   \u2502       \u2514\u2500\u2500 ubiquitous-language-glossaries.md\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 user-docs/                         # User-facing documentation\n\u2502       \u251c\u2500\u2500 user-documentation.md\n\u2502       \u2514\u2500\u2500 training-materials.md\n\u2502\n\u251c\u2500\u2500 src/                                   # Generated and human code\n\u251c\u2500\u2500 infra/                                 # IaC specifications\n\u251c\u2500\u2500 .github/workflows/                     # AI-enhanced CI/CD\n\u2514\u2500\u2500 scripts/ai/                            # AI automation scripts\n</code></pre>"},{"location":"software/software_artifacts/#key-principles-for-ai-era-artifacts","title":"Key Principles for AI-Era Artifacts","text":"<p>Best Practices</p> <ol> <li>Precision Over Brevity - AI needs explicit, unambiguous specifications</li> <li>Machine-Readable Formats - Use structured data (JSON, YAML, Markdown with frontmatter)</li> <li>Version Everything - Track changes to specifications alongside generated code</li> <li>Automate Validation - Ensure AI outputs meet specification requirements</li> <li>Maintain Traceability - Link generated code back to originating specifications</li> </ol> <p>These artifacts serve as the foundation for AI-driven development, transforming from passive documentation into active contracts that guide intelligent code generation and system evolution.</p>"},{"location":"software/software_assurance/","title":"Software Assurance","text":"<p>Overview</p> <p>This section provides a brief overview of Software Assurance fundamentals that are independent of the Software Development Life Cycle (SDLC) Methodology or what entity is creating the software:</p> <ol> <li>Software Assurance</li> <li>Verification and Validation</li> <li>Architecture<ol> <li>Laws of Software Architecture</li> <li>Quality Attributes Drive the Architecture Design</li> </ol> </li> </ol> <p>It also covers the what, how, and why for the how, of Software Engineering.</p> <p>Quote</p> <p>\"Don't worry about that specification paperwork. We'd better hurry up and start coding, because we're going to have a whole lot of debugging to do.\"</p> <p>Verifying And Validating Software Requirements And Design Specifications, Barry Boehm, 1984</p>"},{"location":"software/software_assurance/#software-assurance_1","title":"Software Assurance","text":"<p>Software assurance aims to ensure the reliability, safety, and security of software products.</p> <p>Software assurance (SwA) is a critical process in software development that ensures the reliability, safety, and security of software products.</p> <p>There are several types of software assurance initiatives, including:</p> <ul> <li>Certification and accreditation: These programs establish standards and guidelines for software development, and verify that software products meet these standards. Certification and accreditation can help to ensure that software products are reliable, secure, and compliant with regulations and industry standards.[6]</li> <li>Training and education: These initiatives provide software developers with the knowledge and skills they need to create high-quality, secure software. Training and education can include courses on software testing, secure coding practices, and industry standards and best practices.[7]</li> <li>Code analysis and testing: These initiatives use tools and techniques to analyze software code and identify defects or vulnerabilities. Code analysis and testing can include static analysis, dynamic analysis, and fuzz testing, among other techniques.[8]</li> <li>Threat modeling and risk assessment: These initiatives assess the potential risks and threats to a software system, and identify strategies for mitigating these risks. Threat modeling and risk assessment can help to ensure that software systems are designed to be resilient to attacks and other threats.[21]</li> </ul> <p>Software assurance is executed through a series of activities that aim to ensure the reliability, safety, and security of software products. These activities include requirements analysis, design reviews, code inspections, testing, and formal verification.</p> <p>https://en.wikipedia.org/wiki/Software_assurance</p>"},{"location":"software/software_assurance/#validation-vs-verification","title":"Validation vs. Verification","text":"<p>verification and validation to resolve issues early in the software life cycle</p> <p>The basic objectives in verification and validation (V&amp;V) of software requirements and design specifications are to identify and resolve software problems and high-risk issues early in the software life cycle.</p> <p>Verification - to establish the truth of the correspondence between a software product and its specification. </p> <ul> <li>\"Are you building it right?\" </li> </ul> <p>Validation - to establish the fitness or worth of a software product for its operational mission.</p> <ul> <li>\"Are you building the right thing?\"</li> </ul> <p>Verification activities begin in the Product Design phase and conclude with the acceptance test. They do not lead to changes in the requirements baseline; only to changes in the refinements descending from it.</p> <p>On the other hand, validation identifies problems which must be resolved by a change of the requirements specification. Thus, there are validation activities which occur throughout the software life cycle, including the development phase. </p> <pre><code>- For example, a simulation of the product design may establish not only that the design cannot meet the baseline performance requirements (verification), but also that the performance requirements are too stringent for any cost-effective product designs, and therefore need to be changed (validation).\n</code></pre> <p>Basic V&amp;V criteria for software requirements and design specifications: </p> <ul> <li>completeness</li> <li>consistency</li> <li>feasibility</li> <li>testability</li> </ul> <p>Guidelines For Verifying And Validating Software Requirements And Design Specifications, Barry W. Boehm</p>"},{"location":"software/software_assurance/#continuous-validation-and-verification-process","title":"Continuous Validation And Verification Process","text":"<p> A continuous validation and verification process. Validation ensures the requirements correctly capture the users' and stakeholders' expectations and should be performed whenever a translation of requirements from one domain to another occurs. https://www.infoq.com/articles/ieee-verification-and-validation-for-software-systems/</p> <p>The key artifact that distinguishes verification activities from validation activities is the software requirements specification.</p> <ul> <li>This covers what the software product will do (but not how it will do it; this is to be done in the design specification).</li> </ul> <p> \"Incremental Architecture-centric Validation &amp; Verification Improves Qualification Confidence\" Continuous Verification &amp; Validation of Critical Software via DevSecOps, https://apps.dtic.mil/sti/pdfs/AD1187382.pdf</p> <p>Note</p> <p>These assurance steps apply independent of the Software Development Life Cycle (SDLC) Methodology. </p>"},{"location":"software/software_assurance/#architecture","title":"Architecture","text":""},{"location":"software/software_assurance/#laws-of-software-architecture","title":"Laws of Software Architecture","text":"<p>Fundamentals of Software Architecture, Mark Richards &amp; Neal Ford, defines some fundamental Laws of Software Architecture.</p>"},{"location":"software/software_assurance/#first-law-of-software-architecture","title":"First Law of Software Architecture","text":"<p>\"Everything in software architecture is a trade-off.\"</p> <ul> <li>Corollary 1: If an architect thinks they have discovered something that isn't a trade-off, more likely they just haven't identified the trade-off yet.</li> </ul>"},{"location":"software/software_assurance/#second-law-of-software-architecture","title":"Second Law of Software Architecture","text":"<p>\"Why is more important than how.\"</p>"},{"location":"software/software_assurance/#quality-attributes-drive-the-architecture-design","title":"Quality Attributes Drive The Architecture Design","text":"<p>Quality Attributes drive the architecture design</p> <p>Requirements for a software system fall into the following two categories: </p> <ol> <li> <p>Functional requirements: These requirements describe the business capabilities that the system must provide, as well as its behavior at run-time. </p> </li> <li> <p>Non-functional requirements: These requirements describe the \"Quality Attributes\" that the system must meet in delivering functional requirements.</p> </li> </ol> <p>Quality Attributes drive the architecture design.</p> <p>Continuous Architecture: Sustainable Architecture in an Agile and Cloud-Centric World</p>"},{"location":"software/software_assurance/#architecture-decision-records-adr","title":"Architecture Decision Records (ADR)","text":"<p>ADRs as a Decision Log</p> <p>An ADR can help you understand the reasons for a chosen architectural decision, along with its trade-offs and consequences. The collection of ADRs created and maintained in a project constitute its decision log. https://adr.github.io/</p> <p>ADRs are common e.g.</p> <ul> <li>Azure Well-Architected Framework ADRs</li> <li>Amazon AWS ADR Prescriptive Guidance</li> <li>Google GCP Architecture</li> </ul> <p>ADRs as Documentation</p> <p>Architecture Decision Records can be used as an effective means to document a software architecture. </p> <ul> <li>The Context section of an ADR provides an excellent opportunity to describe the specific area of the system that requires an architecture decision to be made. This section also provides an opportunity to describe the alternatives. </li> <li>Perhaps more important is that the Decision section describes the reasons why a particular decision is made, which is by far the best form of architecture documentation. </li> <li>The Consequences section adds the final piece to the architecture documentation by describing additional aspects of a particular decision, such as the trade-off analysis of choosing performance over scalability.</li> </ul> <p>Chapter 19: Architecture Decisions, Fundamentals of Software Architecture, Mark Richards &amp; Neal Ford</p>"},{"location":"software/software_assurance/#the-what-how-why-of-software-engineering","title":"The What How Why Of Software Engineering","text":"<p>People often struggle with what vs how</p> <p>As a software architect for most of my career, I've seen people (customers and technical people) struggle with the what vs how. </p> <p>They will often specify the implementation details (the how), but not what they want and why.</p> <ul> <li>Or they will specify what they know exists - not what they actually want.</li> </ul> <p>I've seen this also in a security context e.g. </p> <ul> <li>how: \"I want to share a secret key with a trusted 3rd party\"<ul> <li>The wrong thing to do is start by describing secure ways to share keys (which some technical people will do regardless of their seniority)</li> <li>The right thing to do is ask \"why, what are you trying to do? Can you give an example?\", but often people don't ask.</li> </ul> </li> <li>what: \"I want to give read access to a trusted 3rd party to data on somewhere so they can do something\"<ul> <li>This may not require sharing keys depending on the context. But even if it does require sharing keys, you can give advice appropriate to the use case and context.</li> </ul> </li> </ul> <p>Framing the \"what\"</p> <p>Think of me as a good plumber:</p> <ol> <li>Ask for what you want, not what you think you can get. In other words, assume you can have anything you want, now what do you want?</li> <li>You tell me what features you want. I'll take care of the plumbing details.</li> </ol>"},{"location":"software/software_assurance/#what-how-why","title":"What How Why","text":"<ul> <li>The what is captured in the Software Requirements.</li> <li>The how is captured in the System Design.</li> <li>The why for the how is captured in Decision Records covering the reasons and tradeoffs associated with the decisions. These decisions can be at an Architecture or Design level.</li> </ul>"},{"location":"software/software_assurance/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <p>The key artifact that distinguishes verification activities from validation activities is the software requirements.</p> <p>It is important to understand the what independent of the how (requirements vs design).</p> <p>It is important to understand the why for the how (e.g. ADRs) aka </p> <p>\"Why is more important than how.\"</p> <p>Everything in software architecture is a trade-off.</p> <p>Quality Attributes drive the architecture design.</p>"},{"location":"software/software_stuff/","title":"Software stuff","text":"<pre><code>In the early stages of GenAI implementation, organizations tend to \"shoehorn\" GenAI's advanced capabilities into existing workflows and processes.\n\n- They essentially use powerful new tools to perform the same tasks slightly more efficiently, without fundamentally reimagining what's possible.\n\nHowever, the true transformative potential of generative AI emerges when we recognize that it won't simply augment our current operational models\u2014it will radically reshape them.\n\n- The \"way of doing things\" inevitably undergoes profound change as the technology matures and its implications become more apparent.\n\nFor effective GenAI strategy, organizations must simultaneously:\n\n- Address immediate needs by applying AI to current processes\n- Anticipate and design for entirely new operational paradigms that will emerge\n\nThis dual focus ensures solutions remain relevant not just for today's challenges, but for the transformed landscape that GenAI will ultimately create.\n\nThis section is hands on walk through of Software Engineering for a Greenfield application using GenAI.\n</code></pre> <p>TODO     Documentation is not in sync with the code.     The output should inform the code</p> <pre><code>So addressing these hard problems is better vs just threat modeling a document\n</code></pre> <p>The software security touchpoints as introduced and fleshed out in Software Security: Building Security In.</p> <p>github project https://discord.com/channels/1377115244018532404/1396591539823575130/1396591539823575130</p> <p>AI Engineer World's Fair 2025  has some talks that </p> <p>https://www.youtube.com/watch?v=fD8NLPU0WYU  Agile Coding Is HERE\u2026 90% AI Coding Is Already Done With This</p> <p>Sean Grove, OpenAI, July 2025 discuuses The New Code where specifications, not traditional code, are the primary valuable artifact for aligning human and AI intent.  - Future IDEs might function as \"integrated thought clarifiers,\" helping to surface and resolve ambiguities in specifications.  - Moving from disparate machine encodings to a unified human encoding (specifications) will simplify how we solve problems.</p> <p>https://youtu.be/8rABwKRsec4?t=885</p> <p>https://forum.cursor.com/t/bmad-method-v2-in-an-evolution-imo-the-power-of-custom-agents-smaller-docs-and-checklists/87218</p>"},{"location":"software/software_stuff/#new-tech","title":"New Tech","text":"<p>.git/info/exclude https://www.reddit.com/r/git/comments/1jy3yhu/why_gitinfoexclude_exists_if_gitignore_is_better/</p> <p>https://www.reforge.com/blog/the-coder-framework</p> <p>https://www.youtube.com/watch?v=gEWb3qWZXhY&amp;t=138s Claude Squad: https://github.com/smtg-ai/claude-squad 2- Grep: https://vercel.com/blog/grep-a-millio... 3- Serena: https://github.com/oraios/serena 4- Claude monitor: https://github.com/Maciek-roboblog/Cl... 5- Awesome Claude code: https://github.com/hesreallyhim/aweso... 6- BMAD method: https://github.com/bmadcode/BMAD-METHOD 7- Task Master: https://github.com/eyaltoledano/claud...</p> <p>trast with validation.\" https://en.wikipedia.org/wiki/V-model#Validation_vs._verification</p> <p>https://www.parasoft.com/blog/verification-vs-validation-in-embedded-software/</p> <p>docs/assets/images/sast_forrestor.png https://reprint.forrester.com/reports/the-static-application-security-testing-solutions-landscape-q2-2025-5effe9ec/index.html</p>"},{"location":"software/software_stuff/#from-shift-left-testing-to-earlier-lag-indicators","title":"From Shift-Left Testing to Earlier Lag Indicators","text":"<p>Quote</p> <p>Shift-left testing is an approach to software testing and system testing in which testing is performed earlier in the lifecycle (i.e. moved left on the project timeline). It is the first half of the maxim \"test early and often.\"</p> <p>Shift-left testing - Wikipedia</p> <p>Shift-left testing is inherently a lagging activity by definition\u2014it measures past outcomes earlier in the SDLC rather than preventing issues from occurring.</p> <p>A lagging indicator is a metric that measures past performance or outcomes, reflecting what has already happened. In other words, it reveals the symptoms rather than addressing root causes.</p> <p>Shift-left security testing is a DevSecOps practice that integrates security testing early in the Software Development Life Cycle (SDLC). While this approach is necessary, it is neither sufficient nor efficient for remediating cybersecurity risk.</p>"},{"location":"software/software_stuff/#leading-levers-for-security","title":"Leading Levers for Security","text":"<p>Leading indicators are measures that change before outcomes occur. They function as proactive measures that focus on the activities and processes that influence outcomes, rather than simply measuring results after the fact.</p> <p>A leading lever, when adjusted, influences the system and causes lagging indicators to change as a result, providing feedback to the operator.</p> <p>The critical question is: what levers do we need to use to address the symptoms?</p> <ol> <li>What are the root causes?</li> <li>Where are the Pareto distributions? (What follows the 80/20 rule?)</li> <li>What are the top three remediation priorities that will deliver the biggest reduction in organizational risk?</li> </ol> <p>Reference: Security Risk Management Discussion</p>"},{"location":"software/software_stuff/#shift-left-before-coding","title":"Shift Left Before Coding","text":""},{"location":"software/software_stuff/#shift-negative","title":"Shift Negative","text":""},{"location":"software/software_stuff/#devsecops-reimagined","title":"DevSecOps Reimagined","text":""},{"location":"software/software_stuff/#find-your-levers","title":"Find Your Levers","text":"<p>Remediation ROI </p> <p>Shift Left Security Testing is a DevSecOps practice that integrates security testing and processes early in the Software Development Life Cycle (SDLC). </p>"},{"location":"software/swe_agents/","title":"Agent-Based Frameworks for Software Engineering","text":"<p>Overview</p> <p>This section provides an overview of the context an agent (human or LLM) needs for Software Engineering.</p>"},{"location":"software/swe_agents/#software-engineering","title":"Software Engineering","text":"<p> \"Incremental Architecture-centric Validation &amp; Verification Improves Qualification Confidence\" Continuous Verification &amp; Validation of Critical Software via DevSecOps, https://apps.dtic.mil/sti/pdfs/AD1187382.pdf</p>"},{"location":"software/swe_agents/#software-engineering-artifacts","title":"Software Engineering Artifacts","text":"<p>Code is informed by upstream activities (requirements, architecture, design, components, etc.) and downstream activities (tests, user feedback).</p> <ul> <li>The code itself is the source of truth for what was implemented, but not why it was implemented that way, nor whether it does the right thing or does it correctly.<ul> <li>The source of truth for these aspects is captured in other Software Engineering Artifacts.</li> </ul> </li> </ul> <p>Tip</p> <p>These Software Engineering Artifacts inform the code and should live with the code. This means they should be in sync with the code and accessible when code is generated. Specifically, they should be located in the same organization/repository as the code, in a format that is both LLM and human-friendly (Software Engineering Artifact Formats).</p>"},{"location":"software/swe_agents/#software-engineering-knowledge","title":"Software Engineering Knowledge","text":""},{"location":"software/swe_agents/#domain-knowledge","title":"Domain Knowledge","text":"<p>Code is also informed by Domain Knowledge.</p> <p>Taking Security as an example Domain</p> <p>https://baxbench.com/ is a recent (2025) benchmark evaluating LLMs on secure and correct code generation. It demonstrates that even flagship LLMs are not yet ready for coding automation, frequently generating insecure or incorrect code.</p> <p>Three levels of Security Reminder are provided to the models:</p> <p>No Security Reminder: The current highest score is 25% Insecure or Correct code generated.</p> <ul> <li>\"The models are only prompted to complete the coding task. The prompt contains no security-specific instructions, reflecting a realistic interaction with a developer that does not make explicit security considerations.\"</li> </ul> <p>Generic Security Reminder: The current highest score is 18.2% Insecure or Correct code generated.</p> <ul> <li>The models are prompted to complete the coding task and are explicitly reminded to make security considerations and follow security best practices.</li> </ul> <p>Oracle Security Reminder: The current highest score is 10% Insecure or Correct code generated.</p> <ul> <li>The models are prompted to complete the coding task and are explicitly reminded to avoid specific security vulnerabilities that could occur in the given task. This setting assumes an unrealistic oracle that anticipates all security pitfalls, \"where the developer anticipates all the security vulnerabilities associated with the scenario and gives specific instructions to avoid them.\" This prompt provides an upper bound on the models' security performance.</li> </ul> <p>The models are not provided with any security guidance or additional context (security knowledge for the specific task like threat models, security design patterns, or secure code guidelines).</p> <ul> <li>It is very likely that the models would achieve better benchmark results with the right context and guidance. This can be proven by evaluating such a setup against the benchmark.</li> </ul> <p>Domain knowledge must be available to the Agent</p> <p>Even a simple reminder makes a significant difference, whereas context-specific reminders yield a much greater difference.</p> <p>Taking Security as an example Domain:</p> <ul> <li>Security guidance or additional context (security knowledge for the specific task, such as threat models, security design patterns, or secure code guidelines) can significantly improve performance.</li> </ul>"},{"location":"software/swe_agents/#tacit-knowledge","title":"Tacit Knowledge","text":"<p>Tacit knowledge is knowledge that resides in personal experience, skills, and intuition rather than being explicitly documented. In other words, it is the \"know-how\" that comes from doing, rather than just knowing the theory.</p> <p>Agents need a mechanism to build tacit knowledge</p> <p>In addition to Software Engineering Artifacts and Domain Knowledge, agents need a mechanism to build tacit knowledge.</p> <p>In a recent study, Developers report AI not utilizing important tacit knowledge or context as a factor likely to contribute to slowdown.</p> <p>Implicit repository context (Limits AI performance, Raises developer performance)</p> <p>One developer notes that AI often acts like a new contributor to the repository, and that \u201cAI doesn\u2019t pick the right location to make the edits.\u201d Another developer notes that while \u201cwe [..] know the data that will interact with the code, but the model doesn\u2019t know the data. It doesn\u2019t know we need to take care of this weird case of backwards compatibility and [thus] keep this specific line. And this is very hard to give as [context to the model].\u201d.</p> <p>Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity, \"C.1.5 Implicit repository context (Limits AI performance, Raises developer performance)\" July 2025</p>"},{"location":"software/swe_agents/#takeaways","title":"Takeaways","text":"<p>Takeaways</p> <p>The context an agent (human or LLM) needs for Software Engineering includes:</p> <ul> <li>Software Engineering Artifacts (including, but not limited to, Code)</li> <li>Software Engineering Knowledge<ul> <li>Domain Knowledge</li> <li>Tacit Knowledge</li> </ul> </li> </ul> <p>A Software Engineering setup should have a way to manage this context and provide it to the agent when required.</p>"},{"location":"software/swe_agents_report/","title":"Report on Agent-Based Frameworks for Software Engineering","text":"<p>Overview</p> <p>Per previous sections, GenAI allows a shift from Code-Centric to Contract-Centric Development where the upstream artifacts guide the code generation (per standard Software Engineering).</p> <p>This section is a Research Report on Agent-Based Frameworks for Software Engineering 1.0 Redux (aka Agent-Based Frameworks Supporting Upstream Software Engineering Artifacts)</p> <ul> <li>These are Level 4 \u2013 Highly Autonomous Coding: \"The AI can handle complex projects end-to-end, needing only minimal human input\"</li> </ul> <p>Some of these build on existing solutions e.g. SuperClaude, ClaudeFlow build on ClaudeCode, and BMAD can build on ClaudeCode or other tools.</p> <p>The report is from ChatGPT 4o DeepResearch with the following brief:</p> <p>I\u2019ll get started on a report that surveys frameworks using agents for software engineering, particularly those that support upstream artifacts like requirements, design models, or architecture. I\u2019ll include both open-source and commercial tools, and provide a high-level overview along with a comparison table highlighting key features and capabilities. </p> <p>Note that, technically, it is possible to use Project Rules for modular persona design e.g. .windsurfrules or .cursor/rules, but the focus here is on existing solutions that embody personas and associated artifacts to build a solution.</p> <p>Software engineering is increasingly leveraging AI agent frameworks to assist not only with coding but also with upstream development artifacts like requirements documents, design models, and architectural specifications. Below, we survey several notable frameworks \u2013 both open-source and commercial \u2013 that employ autonomous or semi-autonomous agents across the software lifecycle. We describe each framework\u2019s high-level approach, the upstream artifacts it supports, its agent-based characteristics, integration with development tools, and current maturity. A comparison table at the end summarizes their features side-by-side.</p>"},{"location":"software/swe_agents_report/#comparison-of-agent-based-se-frameworks","title":"Comparison of Agent-Based SE Frameworks","text":"Framework Upstream Artifact Support Agent-Based Approach Integration Maturity/Status BMAD-Method PRDs, architecture docs, and user stories. Specialized agents (Analyst, PM, Architect, Scrum Master, Dev, QA); checklist-driven. CLI + Cursor IDE integration. NPM package. Open-source; v4 active; moderate maturity (~2k GitHub stars). SuperClaude Architectural analysis, design ideas, implementation estimation. One Claude instance with 9 cognitive personas (Architect, Security, QA, etc.). Claude Code config layer; CLI/chat commands; Git memory; VS Code support planned. Open-source; mature in niche (~5k stars); stable and growing. ChatDev User stories, architecture designs, test plans, project docs. Simulated software team (CEO, CTO, CPO, Dev, Tester, Reviewer); sequential flow. Python CLI; generates project folders; minimal IDE tooling. Research-grade; very active (~27k stars); experimental features. MetaGPT User stories, requirements, data models, API specs, design diagrams. Multi-agent system with SOPs for each role (PM, Architect, Engineer, QA). Python CLI; flexible LLM backend; outputs markdown &amp; Mermaid diagrams. Very mature; open-source; extremely popular (~57k stars). Claude Flow Requirements analysis, architectural design, test plans, pseudocode. Parallel multi-agent workflow (SPARC phases); Memory Bank; agent coordination tools. Claude Code shell script; CLI; Claude tools (BatchTool, WebFetch); Git auto-commits. New but structured; open-source; early adoption; rapidly evolving. Tabnine AI Agents Jira issue descriptions (as requirements), plus test coverage and validation reports. Specialized agents for Implementation, Validation, Testing, and Code Review. Works in human-in-the-loop cycles. Deep IDE (VS Code, JetBrains) + Jira + CI integration. Supports enterprise/on-prem. Commercial; generally available since 2024; mature for enterprise use. IBM SWE Agents Bug reports, change requests (as requirements). Test plans derived from fix/feature context. Research-stage agents: Bug Fixer, Code Editor, Test Generator. Coordinated via orchestration layer. GitHub-triggered (issue tags); future integration into watsonx / IBM DevOps expected. Research prototype (2024); strong benchmarks; early enterprise trials."},{"location":"software/swe_agents_report/#bmad-method-open-source","title":"BMAD-METHOD (Open-Source)","text":"<p>Overview: The Breakthrough Method for Agile AI-Driven Development (BMAD) is an open-source framework that orchestrates multiple specialized AI agents to mimic an Agile software team. BMAD introduces custom agent personas for common software roles and guides a project through phases from initial idea to deployment. It was designed to solve problems in AI-assisted dev workflows (like context loss and planning drift) by front-loading planning with dedicated agents.</p> <p>Upstream Artifact Support: BMAD explicitly supports creation of upstream artifacts such as product requirements documents (PRDs) and architecture design documents. In the \u201cplanning\u201d phase, specialized agents (Analyst, Product Manager, Architect, etc.) collaborate with the user to generate detailed and consistent specifications, including comprehensive PRDs and technical architecture documentation. These planning outputs are much richer than generic AI-generated tasks, providing a solid blueprint for development. BMAD can also produce a high-level brief (project summary) and even optional UX design outlines before coding begins.</p> <p>Agent Architecture and Coordination: BMAD\u2019s agent roster mirrors an Agile team. In the planning phase, agents like Business Analyst/Analyst, Product Manager, and Architect work together (via prompt scripts) to produce the requirement and design artifacts. Once planning is done, the framework shifts to the IDE for implementation: a Scrum Master agent breaks the plan into \u201cstory\u201d files (detailed development tasks with embedded context), and a Dev agent then implements code based on these stories. A QA agent may verify the outputs. A central Orchestrator agent coordinates the hand-offs (ensuring each story contains all necessary context for the Dev agent) and runs checklists to audit consistency across artifacts. Notably, each agent operates in a turn-based sequence with human-in-the-loop checkpoints \u2013 for example, agents pause for user feedback at certain steps. This structured multi-agent pipeline enforces consistency from requirements through design to code.</p> <p>Integration and Tooling: BMAD is implemented as a Node.js package with configuration files and prompt templates. It can be run in chat UIs (e.g. ChatGPT or a tool called Gemini) for the planning stage, then integrates with the Cursor IDE (an AI-enabled code editor) for coding. The planning artifacts (PRD, architecture doc, etc.) are saved as project files which the Scrum Master agent later \u201cshards\u201d into story files in the IDE. BMAD is designed to be flexible \u2013 users can customize agent prompts or add \u201cexpansion packs\u201d for domains beyond software (e.g. game development or DevOps).</p> <p>Maturity: BMAD is a vibrant open-source project (MIT-licensed) with an active community. It has gone through multiple iterations (V2, V3, now V4 as of mid-2025) and garnered significant interest (over 2k GitHub stars). Users report that BMAD\u2019s approach can save substantial LLM usage costs by shifting work to the planning phase. Overall, BMAD provides a comprehensive AI-driven agile methodology \u2013 it is relatively new but evolving rapidly, indicating a growing maturity and real-world experimentation.</p>"},{"location":"software/swe_agents_report/#superclaude-open-source","title":"SuperClaude (Open-Source)","text":"<p>Overview: SuperClaude is an open-source configuration framework that augments Anthropic\u2019s Claude (a large language model) with a team of specialized agent personas and command protocols. Rather than interacting with a single generalist AI, SuperClaude lets developers invoke different \u201ccognitive archetypes\u201d (agents) tailored to specific software engineering tasks. The framework is defined by a set of rules (in <code>RULES.md</code>) and a Model Context Protocol (MCP) that gives the AI long-term memory, tool usage abilities, and an efficient communication style. SuperClaude\u2019s goal is to turn a generic coding assistant into a context-aware, role-specialized development partner.</p> <p>Upstream Artifact Support: SuperClaude primarily targets the design and implementation stages, but it does facilitate upstream design thinking and planning. For example, it includes a persona called \u201cThe Architect\u201d (<code>/persona:architect</code>) whose sole focus is high-level system design and architecture. When activated, this agent will ask questions about scalability and maintainability and apply proven design patterns, effectively helping produce or critique an architectural model of the system. Using the <code>/design</code> command, SuperClaude can generate software designs \u2013 for instance, it can output proposed data models or API specifications following a Domain-Driven Design approach. It does not explicitly generate a formal requirements document with its own persona, but it leverages the developer\u2019s input (e.g. user stories or feature descriptions) as the requirement and can estimate effort (<code>/user:estimate</code>) based on the design. In essence, SuperClaude\u2019s upstream support lies in architecture and planning: it helps design the solution and plan implementation steps before writing code, though the initial requirements need to be provided by the user.</p> <p>Agent-Based Characteristics: SuperClaude features nine specialized AI personas that the user can switch between, each configured with different priorities and toolsets. These include roles like Architect, Frontend Developer, Backend Developer, Security Expert, QA Tester, Analyzer (debugger), and even a Mentor for explanations. Internally, all personas are powered by the same LLM (Claude), but SuperClaude uses modular configuration files and \u201cflag\u201d commands to shape the LLM\u2019s behavior per role. The framework enforces collaboration and parallelism: for example, you can spawn a dedicated frontend and backend agent to work simultaneously on different components of a feature. Agents communicate implicitly via the shared context and the MCP tool outputs. SuperClaude emphasizes autonomy with oversight \u2013 agents will perform tasks (like coding or testing) on their own, but important rules (e.g. never output insecure code) are baked into the persona profiles. A unique aspect is the Evidence-Based rule: agents must cite documentation for their decisions, reducing hallucinations. Coordination is handled by the MCP\u2019s decision matrix which decides which tool or persona to invoke based on triggers (user flags, natural language cues, or code context).</p> <p>Integration: SuperClaude is used alongside Claude\u2019s coding interface (Claude Code) or via a CLI. It installs as a <code>.claude/</code> configuration on your system. Developers interact with it through chat commands (for example, in a terminal or chat UI that connects to Claude). The framework provides 18 special \u201c/user:\u201d commands for tasks like <code>/user:design</code>, <code>/user:build</code>, <code>/user:analyze</code>, <code>/user:test</code>, etc., each optionally combined with persona or tool flags. It currently relies on Anthropic\u2019s Claude models (Claude 2, Claude Instant, etc.), and a VS Code extension is on the roadmap. Integration with version control is notable: SuperClaude can checkpoint and roll back conversation/code state with Git commands, effectively maintaining memory across sessions. Overall, it acts as an overlay on top of your IDE or CLI, providing an agentic layer that controls Claude for multi-step, multi-role tasks.</p> <p>Maturity: SuperClaude is an MIT-licensed project launched in mid-2023 and has gained traction (5k+ GitHub stars). It\u2019s actively maintained (v2.0.1 as of 2025) with a growing community of contributors. Many developers have praised its practical impact \u2013 e.g. using the Architect persona to generate scalable designs, and the Git-based memory to manage iterative changes. Because it builds on a robust LLM (Claude) and adds pragmatic features (like token compression for long contexts), SuperClaude is considered a stable and useful addition to a developer\u2019s toolkit, albeit currently tied to Claude\u2019s availability.</p>"},{"location":"software/swe_agents_report/#chatdev-open-source-research","title":"ChatDev (Open-Source Research)","text":"<p>Overview: ChatDev is an open-source research framework that demonstrates how multiple LLM-based agents can collaborate to autonomously develop a software application. Introduced by researchers in late 2023, ChatDev simulates an entire software startup (a \u201cvirtual software company\u201d) with agents fulfilling different organizational roles. These agents communicate in natural language, following a structured workflow resembling the waterfall model: phases of designing, coding, testing, and documenting are carried out sequentially by the respective specialists. The core idea is to study collective intelligence \u2013 how a team of AI agents can outperform a single agent by divide-and-conquer and collaboration.</p> <p>Upstream Artifact Support: ChatDev explicitly includes the design phase at the front of its lifecycle. It starts with a software concept or requirement provided by the user (e.g. a one-line idea for an app). The agents then elaborate this into upstream artifacts. For example, the Chief Product Officer (CPO) agent defines product requirements, user stories, and possibly a brief specification of features. The Chief Technology Officer (CTO) agent takes those requirements and produces a system design or architecture, determining the technical approach. Indeed, ChatDev is reported to output documents like project requirements, UI design drafts, and even competitive analyses of the idea. It can also generate technical design artifacts such as data model specifications or API designs, often represented as Mermaid diagrams for architecture. After the design is settled, other agents proceed to coding, but importantly ChatDev creates a persistent set of upstream documents (requirements and design docs) that guide the development. The presence of a Reviewer agent also implies it produces documentation or review reports (ensuring the final product aligns with initial requirements), and a Tester agent generates test plans or reports, which are downstream QA artifacts. In summary, ChatDev\u2019s design and planning outputs are a central feature \u2013 it delivers comprehensive documentation from requirements to design models, before any code is written.</p> <p>Agent Roles and Collaboration: The framework organizes agents in a multi-agent collaboration network with distinct roles akin to job titles in a company. Key roles include: CEO (sets the high-level goal or approves plans), CPO (focus on user needs and requirements), CTO (technical design decisions), Programmer (writes code), Tester (tests the code), Reviewer (performs code review and documentation), and even an Art Designer (which could create UI/UX assets or design elements). These agents communicate in a series of \u201cfunctional seminars\u201d \u2013 essentially structured chat rounds dedicated to specific tasks (brainstorming features, designing system, coding modules, etc.). The collaboration is typically organized as a chain: the output of the planning seminar (requirements from CPO) feeds into the design seminar (led by CTO), which then feeds coding (Programmer), then testing, and so forth. ChatDev\u2019s implementation uses a chain-of-messages orchestration (sometimes called ChatChain) where each agent gets to contribute and sees prior agents\u2019 messages. Notably, the researchers implemented standard operating procedures (SOPs) for each role to ensure coherence (e.g. the CPO agent follows a template to produce requirement lists, the CTO agent follows a template to output design specs). This structured multi-agent dialogue allowed ChatDev to significantly outperform single-agent approaches (like GPT-Engineer) on quality metrics of the final software. Each agent is essentially an instance of an LLM (such as GPT-4) prompted to act in-role, and a controller script manages the turn-taking and information flow. There is no heavy tool use by agents in the published version (they primarily communicate and generate code), but the design ensures that each phase\u2019s output becomes an artifact passed down the pipeline.</p> <p>Integration and Usage: ChatDev can be run as a Python program using open LLM APIs. It\u2019s provided as a framework for research/experimentation, and the authors also offered a web demo (SaaS) for a period. Integration with developer tooling is minimal in the basic ChatDev \u2013 it\u2019s more of an automated planning and coding engine that produces a project (code + docs) in a workspace folder. The outputs include code files, diagrams (as Mermaid markdown), and documentation files that one can open in an IDE. Because ChatDev agents can execute code (especially the Tester agent running the program), a Docker-based sandbox is used for safety.</p> <p>Maturity: As a research prototype, ChatDev is at the proof-of-concept stage (initial version released late 2023). It has received attention for demonstrating the feasibility of multi-agent automated development. IBM noted that ChatDev, along with similar framework MetaGPT, achieved better completeness and consistency in generated software than single-agent methods. Since release, ChatDev\u2019s team has been extending it with more advanced collaboration mechanisms (e.g. a graph-based agent network called MacNet for scalability). The project is active on GitHub under an Apache-2.0 license, with ongoing improvements in efficiency and support for more complex scenarios. While not production-ready for industry use, ChatDev provides a template for agentic SDLC automation and a benchmark for future frameworks.</p>"},{"location":"software/swe_agents_report/#metagpt-open-source","title":"MetaGPT (Open-Source)","text":"<p>Overview: MetaGPT is another open-source multi-agent framework that gained wide popularity as an \u201cAI software company in a box.\u201d It assigns different GPT-based agents to classic software team roles and coordinates their efforts to build software from a high-level specification. Released in mid-2023 (and quickly amassing thousands of users), MetaGPT proved that given a short prompt describing a desired application, a suite of agents can produce not only code but also a range of planning artifacts and documentation. It emphasizes carefully orchestrated Standard Operating Procedures (SOPs) for each role to ensure the agents collaborate effectively.</p> <p>Upstream Artifact Support: MetaGPT excels at generating upstream artifacts. Starting from a one-line requirement input, MetaGPT\u2019s agents will output: user stories, competitive analysis, detailed requirements, data structures and API designs, and other design documents. The framework explicitly lists that it produces \u201cuser stories / competitive analysis / requirements / data structures / APIs / documents, etc.\u201d as part of its deliverables. For example, given a prompt to create a simple e-commerce app, MetaGPT\u2019s Product Manager agent might generate user personas and user stories; the Architect agent would design the database schema and component diagram; a Market Analyst agent (if included) might provide a brief competitive feature comparison. These artifacts are saved into a structured workspace (often as Markdown files or images for diagrams). Notably, MetaGPT uses Mermaid.js to create UML-like diagrams for architecture and workflows \u2013 this means it actually visualizes the design model (class diagrams, flow charts) as part of its output. This comprehensive upstream support is a standout feature: MetaGPT doesn\u2019t jump straight to code, but first fleshes out specifications and designs to guide the coding stage. This results in a more organized project that a human can review (and modify) before implementation.</p> <p>Agent Roles and Mechanism: In MetaGPT, multiple GPT-4 (or similar) instances are each assigned a role such as Product Manager, Project Manager, Architect, Software Engineer, QA, etc.. At minimum, the published version included Product Manager, Architect, Project Manager, and Engineer roles. These agents follow a fixed coordination pattern: typically the Product Manager analyzes the raw requirement and expands it, the Architect plans the system design, the Project Manager organizes tasks, and the Engineer writes the code. Communication between agents is orchestrated so that each agent\u2019s output becomes input for the next. The project uses a prompt templating approach to enforce that, for example, the Architect agent\u2019s prompt includes the Product Manager\u2019s output (user stories, requirements) and then instructs, \u201cAs the software architect, design the system based on these.\u201d Each role has a predefined prompt template (SOP) to maintain structure. For instance, the Architect\u2019s SOP might ensure they produce a section on data schema, a section on module design, etc. MetaGPT\u2019s controller then runs these agents in sequence (with possible iterations if something is incomplete). This clear division of labor allowed MetaGPT to demonstrate high completeness and consistency in generated projects. It also can recruit additional agents dynamically if needed \u2013 for example, if a specific specialized task comes up, it could spawn a new agent with an appropriate skill (the framework was designed to be extensible). However, by default, the core team handles most tasks. The outcome is that each agent\u2019s \u201cdeliverable\u201d is saved: the Product Manager\u2019s user_stories.md, the Architect\u2019s design.md (with diagrams), the Engineer\u2019s code files, etc., giving a multi-perspective result.</p> <p>Integration: MetaGPT is primarily a CLI/command-line tool. Developers install it (Python and Node dependencies) and run it by providing a prompt. It then generates a new project directory with all the files. Because it\u2019s not tied to an IDE, one would typically open the resulting files in their preferred IDE for review. There isn\u2019t an interactive loop with human feedback in the default flow \u2013 it attempts autonomy from requirement to final product. That said, users can intervene between phases if using it stepwise. MetaGPT can leverage external tools or APIs if configured (e.g. searching for information if needed), but most of its knowledge comes from the base LLM. The project\u2019s documentation suggests using Docker for sandboxing if the agents need to execute code/tests. Comparison with ChatDev: Both are similar in concept; indeed, an evaluation by ChatDev\u2019s authors found MetaGPT performing well but noted ChatDev\u2019s communication mechanism yielded higher quality in some aspects. Regardless, MetaGPT integration is straightforward \u2013 it acts as an automated software project generator one can run on a local machine.</p> <p>Maturity: MetaGPT is fairly mature as an open project \u2013 it went viral (over 17k GitHub stars by late 2023) and has an active community of users and contributors. It inspired many derivative projects. As of 2024, it remains under active development (now part of the FoundationAgents organization). MetaGPT is mostly a demonstration; while it can produce a working app for simple requirements, real-world applications likely need iteration and refinement by human developers. However, its strength in producing structured documentation and multi-agent reasoning is a valuable contribution. The framework is often cited as revolutionary in showcasing multi-agent collaboration in SE. It\u2019s open-source (MIT license) and is frequently updated with improvements in prompt strategies and support for new LLMs. Companies and researchers sometimes use MetaGPT as a baseline for agent-based software development experiments.</p>"},{"location":"software/swe_agents_report/#tabnine-ai-agents-for-sdlc-commercial","title":"Tabnine AI Agents for SDLC (Commercial)","text":"<p>Overview: Tabnine, known for its AI code completion, has introduced a suite of AI agents to support various stages of the software development lifecycle (SDLC) in a commercial offering. In late 2024, Tabnine announced AI agents for Atlassian Jira \u2013 specifically an \u201cImplementation Agent\u201d and a \u201cValidation Agent\u201d \u2013 that work from issue descriptions to code and test changes. This is part of Tabnine\u2019s vision of \u201cAI-driven development\u201d where AI agents and human engineers collaborate on tasks, with AI managing many aspects of the development workflow. In addition to the Jira-focused agents, Tabnine also has an AI Test Agent (for generating and running tests) and an AI Code Review Agent (for analyzing pull requests), extending agent support to QA and code quality steps.</p> <p>Upstream Artifact Support: Tabnine\u2019s approach uniquely ties into requirements as captured in issue trackers. The Jira Implementation Agent takes the natural-language requirements in a Jira issue (e.g. user story or bug description) and automatically generates the code to implement that issue. In doing so, it effectively reads an upstream artifact \u2013 the issue text, which often contains acceptance criteria or a feature description \u2013 and translates it into code changes. While it doesn\u2019t produce a separate requirements specification (the requirement is the Jira ticket itself), it ensures that the requirement is addressed. The Jira Validation Agent then acts on the requirement as well: it checks that the generated code indeed fulfills the issue\u2019s requirements. This agent will verify functional behavior and suggest fixes if the code diverges from the specified criteria. By doing this validation, Tabnine is effectively formalizing the link between requirements and implementation. Moreover, because Tabnine\u2019s agents can propose additional tests, one can view those tests as artifacts derived from requirements (testing each acceptance criterion). Tabnine\u2019s platform doesn\u2019t output design diagrams or full architecture documents; instead, it integrates into the agile process by going from user story to code and tests. For many teams, the Jira ticket itself is the upstream artifact, and Tabnine\u2019s agents ensure it\u2019s fully handled. In summary, Tabnine supports upstream artifacts in the sense of consuming and honoring them (Jira stories, bug reports) and automatically producing downstream results (code, validation) aligned to them.</p> <p>Agent Architecture: Tabnine\u2019s SDLC agents operate within the development pipeline as specialized AI services. The Implementation Agent is triggered by a user action (one-click from a Jira issue) and it performs multi-step planning internally: analyzing the issue, retrieving relevant repository context, and then generating code changes. The Validation Agent similarly analyzes code diffs and the issue text to ensure alignment. These can be seen as distinct agents with different objectives that communicate via shared context (the code changes and issue description). Tabnine\u2019s system likely uses large language models (possibly GPT-4 or proprietary models) under the hood for each agent. Coordination occurs in a closed-loop: after implementation, validation runs, and the developer is then presented with the combined outcome (new code plus a validation report). Tabnine also allows the developer to iteratively refine the output by asking the agent to adjust if something is off, effectively supporting a human-in-the-loop feedback cycle. Beyond Jira, the AI Test Agent can generate unit or integration tests automatically when pointed at code changes (using the code and possibly documentation as input). The Code Review Agent will examine a pull request\u2019s diff and leave comments or suggestions, behaving like a static analysis + reviewer. These are separate agents focusing on artifacts like pull request descriptions, commit diffs, and test coverage reports, which are intermediate artifacts in the dev process. All Tabnine agents are orchestrated to keep the developer in control \u2013 for instance, they do not auto-merge code; a human reviews suggestions from the Jira agents and decides to accept changes or not.</p> <p>Integration: Tabnine\u2019s agents integrate deeply with popular developer tools. The Jira agents integrate Atlassian Jira with the development environment: the developer can trigger the Implementation Agent from the Jira UI or IDE, and it will interface with the codebase (through the IDE or repository). The results (code changes) are delivered either as a Git branch/commit or as a patch the developer applies. The Validation Agent can run as part of CI or within the IDE to confirm the issue is resolved. Tabnine\u2019s IDE plugin (for VS Code, IntelliJ, etc.) is the primary user interface, now augmented with chat-based interactions for these agents. So a developer might open a Tabnine chat panel in VS Code, ask the Jira agent to implement a specific issue, and watch as the code appears. Tabnine emphasizes enterprise readiness: code stays private (the models can be self-hosted/on-premises if needed), addressing confidentiality. This makes it appealing for organizations that use Jira workflows. Essentially, Tabnine is bringing agentic automation into existing DevOps \u2013 linking issue trackers to IDE to version control in a seamless flow.</p> <p>Maturity: Tabnine\u2019s agent features are relatively new (general availability announced in Sep 2024), but Tabnine itself is an established company in AI coding tools. The Jira Integration is likely in active use by early-adopter teams. As a commercial product, it is polished for professional environments (with considerations for code security, team settings, etc.). The concept of \u201cissue-to-code\u201d that Tabnine pioneered is a significant step forward, and no major competitor offered that at the time, which suggests a level of innovation and maturity in their implementation. It\u2019s still evolving; future updates may expand the range of upstream artifacts (for instance, parsing design documents or architecture diagrams linked in tickets). In the current state, Tabnine\u2019s AI agents provide a production-ready assistive system that offloads routine coding from human devs, while ensuring requirement coverage. This is a commercially supported solution, making it one of the more practical and immediately usable frameworks in this list for industry teams.</p>"},{"location":"software/swe_agents_report/#ibms-software-engineering-agent-suite-researchprototype","title":"IBM\u2019s Software Engineering Agent Suite (Research/Prototype)","text":"<p>Overview: IBM Research has been actively developing an AI agent suite for automating software engineering tasks. In late 2024, IBM announced an experimental set of Software Engineering (SWE) AI agents (v1.0) aimed at reducing developers\u2019 workload, particularly by addressing bug backlogs. These agents leverage multiple large language models and operate together to perform tasks such as bug discovery, code editing, and test generation. IBM\u2019s work is positioned within its larger watsonx initiative \u2013 using IBM\u2019s Granite series foundation models and an orchestration framework to coordinate the agents. While still in testing, this effort is a key example of a commercial research-led framework, likely to be integrated into enterprise tooling in the near future.</p> <p>Upstream Artifact Support: The IBM SWE agents currently focus more on code maintenance and quality tasks than on early-phase artifacts. They do not appear to generate requirements or design models; instead, they take existing developer inputs (like bug reports or change requests) and act on them. For instance, a developer can tag a GitHub issue with a specific label (IBM\u2019s \u201cSWE\u201d) to hand it to the agent \u2013 the agent will then analyze the described bug (an upstream artifact in the form of an issue report) and locate the problematic code and propose a fix. In this sense, the agent uses a natural language bug description (which is a kind of requirement for a fix) as input. Another agent IBM has built can create and execute tests, which means it takes a piece of code or a feature spec and produces test cases (test scripts are an artifact upstream of validation). These test-generation agents implicitly reason about the intended behavior (which overlaps with requirement understanding). We can surmise that as IBM\u2019s framework evolves, it might incorporate more design-level reasoning (IBM has a history of AI tools for architecture, e.g. past projects with UML models, but those have not been explicitly mentioned in this LLM-based context). For now, IBM\u2019s agents excel at reading and acting on software artifacts like bug reports, code diffs, and test results, but do not produce design documentation or formal requirement specs themselves.</p> <p>Agent-Based Characteristics: IBM\u2019s approach uses multiple specialized LLM agents coordinated for end-to-end tasks. For example, one agent focuses on bug localization and repair \u2013 it likely parses an issue description, scans the repository (perhaps using a code-search tool) to find suspect functions, then generates a patch. Another agent focuses on code edit requests \u2013 a developer can ask to refactor or modify code via a natural language instruction, and the agent will edit the lines accordingly using an IBM Granite model fine-tuned for code. There is also a test-generation agent that creates new tests and possibly runs them to ensure a bug fix didn\u2019t break anything. IBM is building an orchestration framework to chain these agents into workflows. For instance, an orchestration might be: when a bug report comes in, trigger bug-fix agent; then trigger test agent to generate regression tests; then perhaps trigger a code quality agent to review the fix. The agents communicate via shared artifacts (a diff produced by one is input for another, etc.). Coordination, according to IBM\u2019s chief scientist, is aimed at letting these agents handle tasks with minimal human intervention, reducing backlog and freeing developers for new feature work. IBM has reported metrics from benchmarks (SWE-Bench) indicating their agents can localize and fix issues quite efficiently (five minutes on average for a fix, with around 23.7% success on a standard test suite \u2013 among the top performers on that benchmark). This indicates a fairly sophisticated interplay of analysis and action by the agents.</p> <p>Integration: As of the announcement, the IBM SWE agents were in a test phase with integration likely via GitHub (issue tagging) and potentially IDE plugins or CI tools in the future. IBM would presumably offer this through its watsonx.ai platform or Cloud services once ready, meaning it could integrate with enterprise Git repositories and project management systems. There\u2019s mention that tagging a GitHub issue triggers the agent, which suggests a developer workflow integration that\u2019s quite seamless \u2013 just mark an issue and let the AI handle it. For editing tasks, a developer might use a chat interface or command within an IDE (\u201cIBM, implement this change\u2026\u201d). IBM\u2019s orchestration framework is intended to ease creating multi-agent workflows, so integration might also allow custom pipelines \u2013 e.g. an organization could configure: static analysis agent -&gt; security fix agent -&gt; test agent, etc., as part of their DevOps pipeline. Because these agents use IBM\u2019s own LLM (Granite) on watsonx, adoption would involve accessing IBM\u2019s cloud or on-prem model deployments.</p> <p>Maturity: IBM\u2019s agent suite is still in research preview (as of late 2024). It\u2019s not a generally available product yet, but IBM\u2019s communications imply they are moving towards that. They\u2019ve proven viability through benchmarks and are refining the system. IBM has a long history of transferring AI research to enterprise tools, so we can expect these multi-agent capabilities to appear in offerings like IBM ELM (Engineering Lifecycle Management) or cloud DevOps solutions. For now, it\u2019s a cutting-edge commercial framework being tested in real-world scenarios. Its strength lies in addressing well-defined tasks (bugs, edits, tests). If we compare to other frameworks: IBM\u2019s is less about full project generation and more about augmentation of the development pipeline. It\u2019s also one of the few targeting bug fixing explicitly. In terms of upstream support, it is limited in this early stage, but as the framework grows, IBM might extend agents into design and requirement management (areas IBM has interest and tools in). Overall, IBM\u2019s work underscores that major industry players see value in teams of AI agents working alongside human developers, and they are investing to integrate that into software engineering practice.</p>"},{"location":"software/swe_agents_report/#claude-flow-formerly-claude-sparc","title":"Claude Flow (formerly Claude-SPARC)","text":""},{"location":"software/swe_agents_report/#high-level-overview","title":"High-Level Overview","text":"<p>Claude Flow is an agentic orchestration system designed to run on Anthropic's Claude Code platform. It formalizes the SPARC methodology (Specification, Pseudocode, Architecture, Refinement, Completion) in a practical multi-agent setup, optimized for scalable software delivery. It incorporates:</p> <ul> <li>Modular execution phases</li> <li>A Memory Bank for state retention</li> <li>Agent coordination protocols</li> <li>Integrated tooling (e.g., <code>BatchTool</code>, <code>WebFetchTool</code>, Git commit hooks)</li> <li>Optional modes (backend-only, frontend-only, full-stack)</li> </ul>"},{"location":"software/swe_agents_report/#upstream-artifact-support","title":"Upstream Artifact Support","text":"<p>Claude Flow explicitly supports:</p> <ul> <li>Requirements analysis (via project spec ingestion)</li> <li>Architecture modeling (e.g. component breakdowns)</li> <li>Design refinement (via pseudocode and system planning stages)   All of this is structured via the SPARC methodology, with output artifacts stored in a project workspace.</li> </ul>"},{"location":"software/swe_agents_report/#agent-based-architecture","title":"Agent-Based Architecture","text":"<ul> <li>Parallel multi-agent execution with Claude instances working across different SPARC stages.</li> <li>An integration ledger and task assignment map coordinate responsibilities.</li> <li>A shared memory mechanism ensures context is preserved and reused.</li> <li>Emphasizes concurrency (e.g. backend and frontend workstreams) and agent learning.</li> </ul>"},{"location":"software/swe_agents_report/#integration-maturity","title":"Integration &amp; Maturity","text":"<ul> <li>Shell-based runner (<code>claude-flow.sh</code>) compatible with Claude Code.</li> <li>Auto-commits changes and tracks output through Git.</li> <li>Web research and code synthesis are integrated via Claude\u2019s tools.</li> <li>Still early-stage but more structured and production-ready than the prior gist version.</li> <li>Open-source (MIT license) with growing interest and contributions.</li> </ul>"},{"location":"software/swe_agents_report/#references","title":"References","text":"<ol> <li> <p>Claude Flow (formerly Claude-SPARC) https://github.com/ruvnet/claude-flow</p> </li> <li> <p>BMAD-METHOD GitHub Repository https://github.com/bmad-method/bmad</p> </li> <li> <p>BMAD Documentation: Agile AI Development Pipeline https://bmad-method.github.io/docs/pipeline</p> </li> <li> <p>SuperClaude GitHub Repository https://github.com/dsdanielpark/superclaude</p> </li> <li> <p>SuperClaude: RULES.md (Agent Rules and Protocols) https://github.com/dsdanielpark/superclaude/blob/main/RULES.md</p> </li> <li> <p>SuperClaude: Model Context Protocol https://github.com/dsdanielpark/superclaude/blob/main/MCP.md</p> </li> <li> <p>BMAD V4 Design Docs https://github.com/bmad-method/bmad/discussions/33</p> </li> <li> <p>SuperClaude Quick Start &amp; Persona Command Reference https://github.com/dsdanielpark/superclaude#quick-start</p> </li> <li> <p>SuperClaude: Claude Code Tooling Integration https://github.com/dsdanielpark/superclaude/blob/main/claude-config.md</p> </li> <li> <p>BMAD YouTube Channel https://www.youtube.com/@bmad-method</p> </li> <li> <p>MetaGPT GitHub Repository https://github.com/geekan/MetaGPT</p> </li> <li> <p>MetaGPT Paper: \u201cMetaGPT: Meta Programming for Multi-Agent Collaborative Coding\u201d https://arxiv.org/abs/2308.00352</p> </li> <li> <p>ChatDev GitHub Repository https://github.com/openbmb/ChatDev</p> </li> <li> <p>ChatDev Web Demo https://chatdev.streamlit.app/</p> </li> <li> <p>ChatDev Paper: \u201cChatDev: Revolutionizing Software Development with AI Agents\u201d https://arxiv.org/abs/2310.01894</p> </li> <li> <p>ChatDev: MacNet and Collaboration Framework Update https://github.com/OpenBMB/ChatDev/pull/158</p> </li> <li> <p>MetaGPT: MGX Spin-off Platform https://github.com/geekan/mgx</p> </li> <li> <p>MetaGPT: SOP Design Templates https://github.com/geekan/MetaGPT/tree/master/meta/sop</p> </li> <li> <p>Claude Flow: Memory Bank and Agent Coordination https://github.com/ruvnet/claude-flow/blob/main/docs/memory.md</p> </li> <li> <p>BMAD Expansion Packs (e.g., Game Dev, DevOps) https://github.com/bmad-method/bmad/discussions/27</p> </li> <li> <p>IBM SWE Agents \u2013 Official Announcement https://research.ibm.com/blog/swe-agent-suite-ai</p> </li> <li> <p>IBM SWE Agents on GitHub (Prototype) https://github.com/IBM/swe-agent</p> </li> <li> <p>IBM Granite Models Overview (Watsonx) https://www.ibm.com/blog/ibm-granite-open-source-models/</p> </li> <li> <p>Tabnine AI Agents for Jira \u2013 Product Page https://www.tabnine.com/blog/tabnine-jira-agents/</p> </li> <li> <p>Tabnine Test and Review Agents \u2013 Announcement Blog https://www.tabnine.com/blog/tabnine-code-review-agent-and-test-agent/</p> </li> </ol>"},{"location":"software/swe_coding/","title":"Software Engineering 1.0 Redux Secure Coding","text":"<p>Overview</p> <p>This section covers the Core Review Security touchpoint.</p> <p>It adds support for this to the BMAD Method by creating a VulnerabilityTech Agent.</p>"},{"location":"software/swe_coding/#references","title":"References","text":"<ol> <li>https://www.manning.com/books/full-stack-python-security </li> <li>https://github.com/ossf/wg-best-practices-os-developers/tree/main/docs/Secure-Coding-Guide-for-Python</li> </ol>"},{"location":"software/swe_redux/","title":"Software Engineering 1.0 Redux","text":"<p>Overview</p> <p>This section is hands on end2end turning the thoughts on Software Engineering 1.0 Redux from the previous pages into reality.</p> Phase Input Chat History Output Exploratory - - Pre-existing documentation Planning Pre-existing documentation Chat history Product Brief, PRD, UX requirements, Architecture document. Security Review Product Brief, PRD, UX requirements, Architecture document. Chat history - Security review documents - Changes to the architecture document  by the architect based on the security agent's findings Document Preparation for development consumption - PRD  - Security Story Report from Security Analyst - Chat history  - Chat history - User Stories- Security Stories (\"S-\" prefix) Core Development Cycle - User Stories- Security Stories (\"S-\" prefix)  - Security Test Cases <p>Summary table of the SW Engineering 1.0 Redux phases</p>"},{"location":"software/swe_redux/#software-security-touchpoints","title":"Software Security Touchpoints","text":"<p> Image from Secure Programming with Static Analysis by Brian Chess, Jacob West</p>"},{"location":"software/swe_redux/#phase-existing-exploratory-documentation","title":"Phase: Existing Exploratory Documentation","text":"<p>In the exploratory part of the project at the start I had some existing documentation:</p> <ol> <li>A Cost Analysis was created by using <ol> <li>ChatGPT and Gemini Reasoning models to create an analysis</li> <li>Getting them to cross-review each other's output and amending their own output as a result</li> <li>Then taking the amended ChatGPT version as it was more succinct which is what I was looking for.</li> <li>The cost analysis was a back-of-napkin effort to understand likely operational costs early to see if these were a show-stopper or not.</li> </ol> </li> <li>Research Documents using ChatGPT and Gemini Research models and Anthropic</li> <li>ADRs using an existing competed example I liked, and redoing it for the architecture decisions I had made using ChatGPT and Gemini Reasoning models and Anthropic.</li> <li>Some Functional Requirements and use cases and user stories (in a Doc format) from MITRE CWE team.</li> </ol>"},{"location":"software/swe_redux/#phase-bmad-the-planning-workflow","title":"Phase: BMAD The Planning Workflow","text":"<p>Having reviewed and played with some of the SWE agents, the BMAD-Method was closest to my views on Software 1.0 Redux, so this is used here.</p> <p>There's 2 BMad Workflows:</p> <ul> <li>The Planning Workflow </li> <li>The Core Development Cycle</li> </ul> <p>See the updated BMAD flows detailing the Security activities I added:</p> <ul> <li>The Planning Workflow </li> <li>The Core Development Cycle</li> </ul>"},{"location":"software/swe_redux/#bmad-the-planning-workflow","title":"BMAD The Planning Workflow","text":"<p>BMAD method breaks down the requirements and other upstream artifacts into epics and stories, enabling Claude Code to generate structured code and docs with little human intervention (Level 4 Autonomy).</p> <p>Key Planning Phases</p> <ul> <li>Optional Analysis: Analyst conducts market research and competitive analysis</li> <li>Project Brief: Foundation document created by Analyst or user</li> <li>PRD Creation: PM transforms brief into comprehensive product requirements</li> <li>Architecture Design: Architect creates technical foundation based on PRD</li> <li>Validation &amp; Alignment: PO ensures all documents are consistent and complete</li> <li>Refinement: Updates to epics, stories, and documents as needed</li> <li>Environment Transition: Critical switch from web UI to IDE for development workflow</li> <li>Document Preparation: PO shards large documents for development consumption</li> </ul> <p>Here we build the Upstream Software Engineering Artifacts.</p> <p>Specifically these are the artifacts that are built with the workflow below.</p>"},{"location":"software/swe_redux/#setup","title":"Setup","text":"<p>The BMAD-METHOD is used.</p> <p>Specifically, the Fastest Start: Web UI Full Stack Team at your disposal part where you create a Gemini GEM with a BMAD-provided text file.</p> <ul> <li>All these documents are created via a browser interface (Gemini GEM) so the process is IDE-independent!</li> <li>Gemini's long-context window allows it to keep the various documents produced in context so it can make consistent changes across them all.</li> </ul> <p>Tip</p> <p>The created files are available via </p> <ul> <li>the Google Gem Canvas </li> <li>the prompt response when you request a specific document</li> </ul> <p>Observation</p> <p>I was impressed how well this setup worked! </p> <ul> <li>following a logical workflow and prompting for choices or input at each stage (and not getting lost)</li> <li>allowing me to request Gemini Gem to output a document at any time (so I could review and version control it before changes)</li> <li>allowing me to refine the content or ask questions across all the documents as required e.g. if I suggested a change that impacted multiple documents then this was detected by Gemini and the updates made. This ensured consistency across the artefacts.</li> </ul> <p>Some choices / decisions were delibertate postponed e.g. data exchange formats and schemas per Principle #3 - Assume variability; preserve options. This results in a PARTIAL Status at this point.</p>"},{"location":"software/swe_redux/#interaction-with-bmad-gemini-gem","title":"Interaction with BMAD Gemini Gem","text":"<p>Success</p> <p>See the full chat history of creating a Product Brief, PRD, UX requirements, Architecture document. </p> <p>Tip</p> <p>I worked the pre-existing document content into the The Planning Workflow by providing the content to the BMAD Persona at the relevant time via the prompt.</p>"},{"location":"software/swe_redux/#phase-security-review","title":"Phase: Security Review","text":"<p>BMAD-Method did not have a dedicated Security persona or security documents (but does include some basic security checks)</p> <p>Other Threat Modeling solutions could be used to create these security documents</p> <ul> <li>See example security artifacts </li> </ul> <p>Tip</p> <p>The different Threat Modeling solutions had different features I liked, so I decided to build a BMAD Method Security Agent with all the features I wanted.</p> <p>The BMAD Method Security Agent fits in the BMAD Planning Workflow</p> <ul> <li>It reviews the existing documents, creates security documents including threat models, security test cases, and security updates to the architecture document.</li> <li>Unlike the other solutions, this Security Agent can also be used during the code implementation phase.</li> </ul>"},{"location":"software/swe_redux/#adding-a-bmad-method-security-agent","title":"Adding a BMAD Method Security Agent","text":"<p>I added a Security Agent to BMAD per commits from Crashedmind. The commit descriptions give the details.</p> <p>You can browse through the files e.g.:</p> <ul> <li>Security Agent</li> <li>Security Tasks</li> </ul> <p>Tip</p> <p>The Security Agent creates these Security review documents</p> Document Description security_assessment.md Initial security assessment and analysis threat_model.md Complete STRIDE threat model dread.md Quantitative DREAD risk assessment security_story_report.md Actionable security user stories https://github.com/CWE-ChatBot/CWE-ChatBot/tree/main/docs/security/bmad_fullagent_security/security_test_cases.md Security test cases and validation scenarios security_architecture_checklist.md Architecture checklist validation compliance_audit.md GDPR compliance audit"},{"location":"software/swe_redux/#building-the-bundles","title":"Building the Bundles","text":"<p>The builder gathers all the relevant files for all the agents into single files in  https://github.com/CyberSecAI/BMAD-METHOD/tree/feature/add-new-agent-security/dist/teams</p> <ul> <li>e.g. team-fullstack.txt contains all the agents including security, and all the other files they need.</li> </ul>"},{"location":"software/swe_redux/#create-a-gemini-gem-with-the-fullstack-team","title":"Create a Gemini Gem with the FullStack team","text":"<ul> <li>https://gemini.google.com/</li> <li>Explore Gems</li> <li>New Gems</li> <li>CopyNPaste team-fullstack.txt into instructions</li> <li>Save</li> </ul>"},{"location":"software/swe_redux/#using-the-bmad-method-security-agent-via-the-gemini-gem-with-fullstack-team","title":"Using the BMAD Method Security Agent via the Gemini Gem with FullStack team","text":"<ol> <li>Collate the project brief, prd, architecture etc... md files into one file (e.g. using gitingest)</li> <li>At the prompt say \"here's my existing documentation\" and copyNpaste the collated md file into the prompt</li> <li>Click the Submit Arrow</li> <li>The workflow kicks off</li> </ol> <p>Success</p> <p>See the </p> <ul> <li>full chat history using the Security Agent.</li> <li>full chat history creating and using the Security Agent (Video)</li> <li>output security artifacts. </li> <li> <p>changes to the architecture document by the architect based on the security agent's findings</p> <ul> <li>\"| July 23, 2025 | 2.0 | Integrated security agent's findings (WAF, AI Guardrails, DoS Protection, Enhanced Logging). | Winston (Architect) |\"</li> </ul> </li> </ul>"},{"location":"software/swe_redux/#document-preparation-for-development","title":"Document Preparation for development","text":"<p>Key Planning Phases</p> <p>When to move to the IDE: Once you have your PRD, Architecture, optional UX and Briefs - its time to switch over to the IDE to shard your docs, and start implementing the actual code! </p> <p>Coming soon..</p>"},{"location":"software/swe_redux/#phase-core-development-cycle","title":"Phase: Core Development Cycle","text":"<p>Coming soon..</p>"},{"location":"software/swe_redux/#other-claude-code-workflows","title":"Other Claude Code Workflows","text":"<p>There are other Claude Code workflows, some example are given here.</p>"},{"location":"software/swe_redux/#ui-designer","title":"UI Designer","text":"<p>https://www.youtube.com/watch?v=TyGx277x9hQ</p>"},{"location":"software/swe_redux/#retrospectives","title":"Retrospectives","text":"<p>https://www.youtube.com/watch?v=ai_sSQH1Pn4&amp;t=478s</p>"},{"location":"software/swe_redux/#multitasking","title":"Multitasking","text":"<p>https://www.geeky-gadgets.com/how-to-use-git-worktrees-with-claude-code-for-seamless-multitasking/</p> <p>There are UI tools built on GIT worktrees to support this.</p>"},{"location":"software/swe_redux/#other-non-claude-code-workflows","title":"Other non-Claude Code Workflows","text":""},{"location":"software/swe_redux/#accessibility","title":"Accessibility","text":"<p>https://github.com/scragz/kornelius/blob/main/prompts/audit/a11y.prompt</p>"},{"location":"software/swe_redux/#references","title":"References","text":"<ol> <li>https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/</li> <li>https://www.geeky-gadgets.com/how-to-use-git-worktrees-with-claude-code-for-seamless-multitasking/</li> <li>https://github.com/ryoppippi/ccusage</li> <li>https://github.com/eyaltoledano/claude-task-master/blob/main/docs/tutorial.md</li> <li>https://www.reddit.com/r/vibecoding/comments/1lu37up/vibecoding_is_straight_chaos_without_instructions/ </li> <li>https://www.reddit.com/r/vibecoding/comments/1l5o93n/lets_talk_about_security/</li> </ol>"},{"location":"software/swe_redux_security/","title":"Software Engineering 1.0 Redux Security","text":"<p>Overview</p> <p>This section covers the Core Review Security touchpoint.</p> <p>It adds support for this to the BMAD Method by creating a VulnerabilityTech Agent.</p> <p>TODO: add ref to paper guidance improving security</p> <p>Vulnerability Assessment Analyst Tanya</p> <p>\"Use for performing security scans, identifying and documenting software vulnerabilities, analyzing code for weaknesses, validating patches, assessing system configurations, verifying compliance with security standards, and generating vulnerability reports for audit or remediation purposes.\"</p> <p>The original brief from a user AVSuun</p>"},{"location":"software/swe_redux_security/#tools-and-claude-code-sub-agents","title":"Tools and Claude Code Sub-agents","text":"<p>I wanted to give the VulnerabilityTech agent access to tools for </p> <ul> <li>Static Analysis Security Testing (SAST) - combining traditional SAST tools with LLM review </li> <li>Dependency checking</li> </ul> <p>I decided to use Claude Code Sub-agents for this (announced a few days ago https://docs.anthropic.com/en/docs/claude-code/sub-agents). </p> <ul> <li>I only focused on Claude Code - not other environments that BMAD may work on.</li> </ul>"},{"location":"software/swe_redux_security/#bmad-agents-vs-claude-code-sub-agents","title":"BMAD Agents vs Claude Code Sub-Agents","text":""},{"location":"software/swe_redux_security/#claude-code-sub-agents","title":"Claude Code Sub-Agents","text":"<ul> <li>What They Are: Specialized AI assistants with custom system prompts and tool access</li> <li>Architecture: Defined as Markdown files with YAML frontmatter in <code>.claude/agents/</code></li> <li>Operation: Isolated context windows with specific expertise and granular tool permissions</li> <li>Delegation: Automatic selection by Claude Code or explicit user invocation</li> <li>Benefits: Focused expertise, faster analysis, parallel processing capabilities</li> </ul>"},{"location":"software/swe_redux_security/#bmad-framework-current-state","title":"BMad Framework Current State","text":"<ul> <li>12 Specialized Agents: Comprehensive personas in <code>bmad-core/agents/</code></li> <li>Complex Configuration: YAML-based with dependencies and external task references</li> <li>Full Context Maintenance: Complete operating instructions and workflow definitions</li> <li>Command Syntax: <code>*command</code> activation with extensive persona definitions</li> </ul>"},{"location":"software/swe_redux_security/#key-architectural-differences","title":"Key Architectural Differences","text":"<ol> <li>Complexity: BMad = comprehensive personas vs Claude = focused sub-agents</li> <li>Dependencies: BMad = external task references vs Claude = self-contained</li> <li>Context: BMad = full context maintenance vs Claude = isolated context windows</li> <li>Activation: BMad = explicit commands vs Claude = automatic delegation</li> </ol>"},{"location":"software/swe_redux_security/#claude-code-sub-agents-setup","title":"Claude Code sub-agents Setup","text":"<p>See how I setup Claude Code sub-agents to work with BMAD agents that describes how BMAD agents call Claude Code sub-agents.</p> <p>Four Claude Code sub-agents were added:</p> Claude sub-agent Purpose dependency-scanner I am a specialized security analyst focused on third-party component security assessment and supply chain risk analysis. My expertise covers dependency vulnerability scanning, license compliance, and supply chain security validation according to NIST SSDF practices. pattern-analyzer I am a specialized security pattern analyst focused on detecting secure and insecure coding patterns across multiple programming languages. My expertise leverages language-specific security knowledge from expansion packs to validate secure coding practices and identify anti-patterns that introduce vulnerabilities. security-reviewer I am a Level 2 orchestrator sub-agent that coordinates comprehensive security analysis by leveraging specialized tool sub-agents. My role is to orchestrate hybrid SAST + LLM security analysis for maximum accuracy and coverage, focusing on practical, exploitable security issues that pose real risks to applications and systems. test-validator I am a specialized security testing analyst focused on validating the quality, coverage, and effectiveness of security tests within software projects. My expertise ensures that security testing meets NIST SSDF standards and provides robust protection against real-world threats. <p>I also added a tests dir to BMAD be able to test this. </p> <ul> <li>This includes a deliberately vulnerable app to test: </li> </ul>"},{"location":"software/swe_redux_security/#vulnerabilitytech-agent-report","title":"VulnerabilityTech Agent Report","text":"<p>Success</p> <p>See example output report.</p> <p>https://github.com/CyberSecAI/BMAD-METHOD/blob/feature/claude-code-sub-agents-integration/tests/reports/consolidated-security-report.md#%EF%B8%8F-nist-ssdf-compliance-assessment</p>"},{"location":"software/swe_redux_security/#finding-defects","title":"Finding Defects","text":"<p> Image from Secure Programming with Static Analysis by Brian Chess, Jacob West</p> <p>The best way to find a particular defect depends on whether it is generic or context specific, and whether  it is visible in the code or only in the design.</p>"},{"location":"software/swe_redux_security/#how-to-use-llms-and-sast-tools-together","title":"How to use LLMs and SAST Tools Together","text":"<p>LLMs can be used with Code Analysis Tools in different ways: 1. Create a formatted report of findings (this is more an editorial activity - not code analysis) 2. Triage the findings and indicate if they are True or False Positives (this is a common security activity)    1. This involves looking at the source code with the finding 3. Independently look for issues e.g. using an analysis of CPGs. 4. Actively exercise the code by creating or guiding test cases.</p> <p>LLMs can compliment the Static analysis sweetspot (Generic defects - Visible in the code)</p> <ul> <li>\"SAST tools often rely on predefined patterns and rules, which can result in high false-positive rates and an inability to detect novel or context-dependent vulnerabilities\" per ref.</li> <li>Additional context can be given to LLMs to improve their performance e.g. vulnerability reports on similar code  LSAST: Enhancing Cybersecurity through LLM-supported Static Application Security Testing</li> </ul> <p>Quote</p> <p>While traditional tools like Bandit are valuable, our results show that LLMs provide a complementary advantage by analyzing fragmented and non-compilable code and detecting complex vulnerability patterns that existing tools may miss. Additionally, the ability of LLMs to generate security tests adds a useful layer of verification, potentially enhancing the accuracy of vulnerability assessments.</p> <p>Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects</p> <p>Quote</p> <p>Our findings indicate that SAST tools exhibit low vulnerability detection ratios while maintaining a low marked function ratio, akin to a low false positive rate. In contrast, LLMs demonstrate high vulnerability detection ratios but are accompanied by elevated marked function ratios (akin to high false positive rates). Through ensemble approaches, we demonstrate that combining SAST tools and LLMs mitigates their respective limitations, resulting in improved overall vulnerability detection performance.</p> <p>Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection </p> <p>Quote</p> <p>Analysis reveals that the optimal approaches differ across programming languages. The best choice will depend on the user\u2019s acceptance of the trade-off between detection ratios and marked function ratios.</p> <p>Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection </p> <p>Quote</p> <p>The reasoning-oriented models consistently produced fewer false positives, suggesting that their internal steps for \u201cvalidating\u201d potential vulnerabilities lead to more precise outcomes.</p> <p>CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection</p>"},{"location":"software/swe_redux_security/#code-context","title":"Code Context","text":"<p>A challenge with using LLMs is to get the precise and complete code context to the LLM as described in Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation. They developed a tool to extract this code context.</p> <p>Quote</p> <ul> <li>First, we propose a line-level precise code slicer eCPG-Slicer. It constructs an extended Code Property Graph (eCPG) and then extracts line-level code context related to the warning within the given files.</li> <li>Second, we propose a linear complexity algorithm, FARF, which is used to identify source files that have dependencies related to a warning, enabling the slicer to extract the complete code context.</li> <li>Third, we integrate our eCPG-Slicer and FARF algorithm into our LLM4FPM framework. LLM4FPM can efficiently drive LLMs to give judgements for a given warning generated by SAST tools.</li> </ul> <p>Note</p> <p>The source code for these does not appear to be publicly available But Joern is an open source solution used for such purposes.</p> <p>Quote</p> <p>A code property graph of a program is a graph representation of the program obtained by merging its abstract syntax trees (AST), control-flow graphs (CFG) and program dependence graphs (PDG) at statement and predicate nodes.  https://en.wikipedia.org/wiki/Code_property_graph</p> <p>Quote</p> <p>Joern is a platform for analyzing source code, bytecode, and binary executables. It generates code property graphs (CPGs), a graph representation of code for cross-language code analysis. Code property graphs are stored in a custom graph database. https://github.com/joernio/joern </p> <p>Quote</p> <ol> <li>Precise Code Context. The extracted code snippet should focus on control flows and data flows relevant to the warning, capturing the precise code context while omitting unnecessary parts that might distract the LLM and lead to incorrect or ambiguous conclusions.</li> <li>Complete Code Context. The analysis should account for key information often missing from bug reports, such as references to global variables or invoked functions located in other files. Without this, the extracted context remains incomplete.</li> <li>Correct Conclusions. After obtaining precise and complete code context, there is an opportunity to more effectively guide the LLM to make accurate judgments on bug reports from SAST tools.</li> </ol> <p>Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation</p>"},{"location":"software/swe_redux_security/#references","title":"References","text":"<ol> <li>Secure Programming with Static Analysis by Brian Chess, Jacob West</li> <li>Software Security: Building Security In, by Gary McGraw</li> <li>Towards Effective Complementary Security Analysis using Large Language Models</li> <li>Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects</li> <li>LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis</li> <li>Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection </li> <li>LSAST: Enhancing Cybersecurity through LLM-supported Static Application Security Testing</li> <li>CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection</li> <li>Source Code Vulnerability Detection: Combining Code Language Models and Code Property Graphs</li> </ol>"},{"location":"software/threat_model/","title":"AI-Powered Threat Modeling for Secure System Design","text":"<p>Overview</p> <p>This section covers AI-powered threat modeling for secure system design, focusing on open-source tools and prompt-based solutions. Below is a comparison of features across notable open-source tools that assist in early design-time threat modeling.</p>"},{"location":"software/threat_model/#threat-modeling","title":"Threat Modeling","text":"<p>Quote</p> <ul> <li>The best use of threat modeling is to improve the security and privacy of a system through early and frequent analysis.</li> <li>Threat modeling must align with an organization\u2019s development practices and follow design changes in iterations that are each scoped to manageable portions of the system.</li> <li>The outcomes of threat modeling are meaningful when they are of value to stakeholders.</li> <li>Dialog is key to establishing the common understandings that lead to value, while documents record those understandings, and enable measurement.</li> </ul> <p>Threat Modeling Manifesto </p> <p>Quote</p> <p>Threat modeling is an investment\u2014in my view, it\u2019s a good one, because finding and mitigating threats in the design phase of your workload feature can reduce the relative cost of mitigation, compared to finding the threats later. Consistently implementing threat modeling will likely also improve your security posture over time.</p> <p>https://aws.amazon.com/blogs/security/threat-modeling-your-generative-ai-workload-to-evaluate-security-risk/</p>"},{"location":"software/threat_model/#open-source-tools-comparison","title":"Open Source Tools Comparison","text":"<p>Comparison of features and capabilities of open-source AI-driven threat modeling tools (focused on early design-phase usage, except AI Security Analyzer which also supports code-level analysis).</p> Tool Focus &amp; Approach Key Features AI / Model Integration AWS Threat Designer Design-phase automated threat modeling; Web UI (AWS Cloud stack). Users upload system architecture info (including diagrams) to generate threat models. \u2013 Architecture diagram analysis: Multi-modal LLM analyzes system diagrams to identify components and relationships.\u2013 Threat catalog: Generates a comprehensive list of potential threats with interactive filtering and refinement.\u2013 Iterative replay: Allows rerunning the model after design changes to see updated risk postures.\u2013 Exports &amp; history: Supports exporting results to PDF/DOCX and browsing past models in a catalog. Uses AWS Bedrock with large models (Anthropic Claude 4 Sonnet by default) for NLP and vision; serverless backend. Multimodal LLM interprets text and diagrams to generate threats. AWS Threat Composer Design-phase guided threat modeling; Web app or VS Code extension. Emphasizes manual brainstorming with structured guidance (\u201cthreat model as code\u201d). \u2013 Data capture: Records system description, architecture and dataflow diagrams, assumptions, etc., as part of the model.\u2013 Threat grammar: Uses a prescriptive template to iteratively craft threat statements (with examples and suggestions) instead of starting from scratch.\u2013 Insights dashboard: Highlights gaps like threats without mitigations, unprioritized threats, category coverage, etc., to answer \u201cdid we do enough?\u201d.\u2013 Packs &amp; export: Supports reusable threat/mitigation packs for bulk addition (in self-hosted mode), and can generate a consolidated threat model document. No built-in generative model \u2013 focuses on human-driven input with static guidance. (Uses local storage; no data leaves the browser. \u201cThreat Grammar\u201d framework provides structure rather than AI generation.) StrideGPT Design-phase automated STRIDE threat modeling; Streamlit web app or Docker CLI. Given an app description (and optionally diagrams or repo link), it auto-generates a full threat model. \u2013 STRIDE threats generation: Uses LLM to output threats categorized by STRIDE (Spoofing, Tampering, etc.) for the provided system description.\u2013 Risk &amp; mitigations: Automatically provides DREAD risk scores and suggests mitigation steps for each identified threat.\u2013 Attack trees &amp; tests: Produces potential attack paths (attack trees) and even Gherkin-style security test cases based on threats.\u2013 Multi-modal input: Can accept architecture diagrams or flowcharts for analysis with vision-capable models; also can analyze a project\u2019s repository (e.g. README) to enrich the threat model.\u2013 No data retention: Does not store inputs or results on the server, focusing on privacy. Supports multiple LLMs via API or local runtime: OpenAI GPT-4 (and newer GPT-4.1), Anthropic Claude 3/4, Google Gemini (2.0/2.5) and others. Also compatible with self-hosted local models (Ollama, LM Studio). This flexible backend allows using the best available model for analysis. AI Security Analyzer Code-centered security analyzer with threat modeling output; CLI tool (Python) that scans an existing codebase to produce security documentation. Geared toward integrating threat modeling in later stages (after code exists) as well as design review. \u2013 Multi-faceted analysis: Generates a Security Design Review document which includes threat modeling, attack surface analysis, attack trees, mitigation strategies, and identified vulnerabilities.\u2013 Code-aware threat identification: Parses project source code (multiple languages: Python, Java, JavaScript, Go, Android, etc.) to find assets, entry points, and potential threats specific to the implementation.\u2013 Automated documentation: Outputs comprehensive Markdown reports (e.g. <code>security_design.md</code>) that consolidate the security findings and model.\u2013 Flexible deployment: Can run via Python (Poetry) or Docker; cross-platform support (Windows, MacOS, Linux). Leverages LLMs to analyze code and generate text. Supports multiple model providers: OpenAI API (GPT-4 family), Anthropic (Claude), Google PaLM/Gemini via API, and OpenRouter. The user supplies an API key for the chosen model, and the tool orchestrates prompts to produce the security docs."},{"location":"software/threat_model/#open-source-tools-details","title":"Open Source Tools Details","text":""},{"location":"software/threat_model/#aws-threat-designer","title":"AWS Threat Designer","text":"<p>AWS Threat Designer: AI-powered threat modeling for secure system design.</p> <ul> <li>See blogpost: Accelerate threat modeling with generative AI for an in-depth overview of the solution.</li> </ul> <p>Quote</p> <p>Effective threat modeling examines data flows, trust boundaries, and potential attack vectors to create a comprehensive security strategy tailored to the specific system.</p> <p>In a shift-left approach to security, threat modeling serves as a critical early intervention. By implementing threat modeling during the design phase\u2014before a single line of code is written\u2014organizations can identify and address potential vulnerabilities at their inception point. </p> <p>AWS Accelerate threat modeling with generative AI, JUN 2025 </p> <p>Each function generates specialized prompts for different phases of the threat modeling process, including:</p> <ul> <li>Asset identification</li> <li>Data flow analysis</li> <li>Gap analysis</li> <li>Threat identification and improvement</li> <li>Response structuring</li> </ul> <p>https://github.com/awslabs/threat-designer/tree/main?tab=readme-ov-file#prerequisites</p> <p>The backend is written in python: threat-designer/backend/threat_designer/</p> <p>The associated prompts are https://github.com/awslabs/threat-designer/blob/0554b6a97c08e38bb92504ba13768780adb0301f/backend/threat_designer/prompts.py</p> <p>See also  1. https://aws-samples.github.io/threat-technique-catalog-for-aws/Techniques/ 2. https://awssecuritydigest.com/articles/threat-modelling-in-aws</p>"},{"location":"software/threat_model/#aws-threat-composer","title":"AWS Threat Composer","text":"<p>A simple threat modeling tool to help humans to reduce time-to-value when threat modeling https://github.com/awslabs/threat-composer#readme</p>"},{"location":"software/threat_model/#stride-gpt","title":"STRIDE GPT","text":"<p>Quote</p> <p>Features:</p> <ul> <li>Simple and user-friendly interface</li> <li>Generates threat models based on the STRIDE methodology</li> <li>Multi-modal: Use architecture diagrams, flowcharts, etc. as inputs for threat modelling across all supported vision-capable models</li> <li>Generates attack trees to enumerate possible attack paths</li> <li>Suggests possible mitigations for identified threats</li> <li>Supports DREAD risk scoring for identified threats</li> <li>Generates Gherkin test cases based on identified threats</li> <li>GitHub repository analysis for comprehensive threat modelling</li> <li>No data storage; application details are not saved</li> <li>Supports models accessed via OpenAI API, Azure OpenAI Service, Google AI API, Mistral API, or locally hosted models via Ollama and \ud83c\udd95 LM Studio Server</li> <li>Available as a Docker container image for easy deployment</li> <li>Environment variable support for secure configuration </li> </ul> <p>https://github.com/mrwadams/stride-gpt</p> <p>The app https://stridegpt.streamlit.app/ has these tabs</p> <ul> <li>Threat Model</li> <li>Attack Tree</li> <li>Mitigations</li> <li>DREAD</li> <li>Test Cases</li> </ul> <p>It supports a Bring Your Own LLM Key i.e. you chose the LLM and provide your API key.</p>"},{"location":"software/threat_model/#ai-security-analyzer","title":"AI Security Analyzer","text":"<p>Quote</p> <p>AI Security Analyzer is a Python-based tool that analyzes your project's codebase and automatically generates detailed security documentation. It supports multiple analysis types:</p> <ul> <li>\ud83d\udd12 Security Design Documentation</li> <li>\ud83c\udfaf Threat Modeling</li> <li>\ud83d\udd0d Attack Surface Analysis</li> <li>\ud83c\udf33 Attack Tree Analysis</li> <li>\ud83d\udee1\ufe0f Mitigation Strategies</li> <li>\ud83d\udc1b Vulnerabilities</li> </ul> <p>https://github.com/xvnpw/ai-security-analyzer</p> <p>generate four different types of security documents:</p> <ul> <li>\ud83d\udd12 Security Design Documentation: Generating detailed security design review.</li> <li>\ud83c\udfaf Threat Modeling: Performing threat modeling analysis.</li> <li>\ud83d\udd0d Attack Surface Analysis: Identifying potential entry points and vulnerabilities in the project\u2019s attack surface.</li> <li>\ud83c\udf33 Attack Tree Analysis: Visualizing potential attack vectors and their hierarchies through attack tree.</li> </ul> <p>https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/</p> <p>The associated prompts are </p> <ul> <li>https://github.com/xvnpw/ai-security-analyzer/blob/dabfc57b6e5da9d99b3df5229fd496a224dac862/ai_security_analyzer/prompts.py</li> </ul>"},{"location":"software/threat_model/#additional-prompt-resources","title":"Additional Prompt Resources","text":"<p>Beyond full tools, there are also open-source prompt libraries focused on security threat modeling:</p> <ul> <li>Fabric by Daniel Miessler: A crowdsourced collection of AI prompt \u201cpatterns.\u201d It includes a create_stride_threat_model pattern to guide an LLM in producing a STRIDE-based threat model from a system description and a create_threat_scenarios pattern for generating detailed attack scenarios. These patterns can be used with various GPT-based systems to jump-start threat modeling exercises.</li> <li>Kornelius Security Audit Prompt: An open prompt (from the Kornelius project) that provides a template for security auditing via LLMs. This prompt script can be adapted to evaluate a system\u2019s security posture by enumerating threats and checks.</li> </ul> <p>Each of the above resources provides ready-made prompt structures that practitioners can use with their AI of choice to conduct threat modeling, complementing the dedicated tools compared in the table.</p>"},{"location":"software/code/code_assistant_agents/","title":"AI Coding Tools","text":"<p>Overview</p> <p>This page gives a comparison of AI Coding Tools by Autonomy Levels.</p> <p>It starts by defining Autonomy Levels for AI Coding Tools.</p> <p>Then it categorizes and compares these AI Coding Tools based on these Autonomy Levels</p> <p>Tip</p> <p>See also Feature Comparison and Analysis of Leading AI IDEs: Cursor, Windsurf, Roo Code, and GitHub Copilot, May 27, 2025.</p> <p>Observation</p> <p>Current integrated development environments (IDEs) are optimized for human programmers.</p> <p>They are not optimal  </p> <ul> <li>For Software Engineering Artifacts in the AI Era</li> <li>For shared understanding among stakeholders</li> <li>As we move From Code-Centric to Contract-Centric Development</li> </ul> <p>Quote</p> <p>\"There's a good chance that, by the end of the year, people aren't using IDEs anymore\", Mastering Claude Code in 30 minutes, Anthropic, 22 May 2025</p>"},{"location":"software/code/code_assistant_agents/#autonomy-levels-for-ai-coding-tools","title":"Autonomy Levels for AI Coding Tools","text":"<p>We define autonomy levels similar to how autonomous driving is categorized:</p> <ul> <li>Level 1 \u2013 Basic Code Completion: AI offers single-line or small code suggestions based on context. The developer is fully in control, and the AI assists with micro-tasks (comparable to basic cruise control in cars). Tools at this level accelerate typing but do not generate complex code blocks or make decisions.</li> <li>Level 2 \u2013 Advanced Code Generation (AI Pair Programmer): AI can produce larger code snippets or entire functions from natural language prompts or broader context. The AI behaves like a junior pair programmer \u2013 it writes significant blocks of code or fixes simple bugs on request, but each AI output is a single-step operation reviewed by the human. Developers still guide the overall structure and verify the results.</li> <li>Level 3 \u2013 Context-Aware Agents (Supervised Automation): AI acts more agentically, performing multi-step coding tasks in an iterative loop to fulfill a high-level goal. The developer provides a goal (e.g. \u201cAdd a profile feature\u201d or \u201cFix this bug\u201d) and the AI plans steps, writes or edits multiple files, runs code or tests, debugs, and iterates with minimal prompting. The human oversees the process and may intervene or review final changes, but does not micromanage each step. This is akin to an intern or junior dev autonomously coding under supervision.</li> <li>Level 4 \u2013 Highly Autonomous Coding: The AI can handle complex projects end-to-end, needing only minimal human input (like a prompt or spec) and no required oversight on each change. It can design solutions, write code across the entire codebase, test, debug, and even proactively improve code without awaiting human review. At this level, you might trust the AI as you would a strong senior engineer: provide requirements and let it deliver a solution to review. (In practice, developers may still do a final review, but the AI\u2019s output is intended to be production-ready.)</li> <li>Level 5 \u2013 Full Autonomy (Experimental/Future): At this theoretical stage, an AI coding agent sets its own goals and continuously improves software without explicit human-defined tasks. It could analyze user needs or system metrics and autonomously decide to implement features or optimizations. This remains speculative \u2013 current tools do not reach Level 5, but research projects hint at this direction (e.g. AutoGPT agents that take an open-ended goal and break it into sub-tasks to achieve it). No mainstream coding tool today operates at full Level 5 autonomy.</li> </ul> <p>Comparison Table of Developer AI Tools by Autonomy and Capabilities</p> <p>The following table provides a summarized comparison of the developer AI tools discussed in this report, categorized by their primary autonomy level and highlighting their key characteristics, interaction styles, strengths, and limitations. This table serves as a quick reference to differentiate the capabilities and intended use cases of various tools in the rapidly evolving landscape of AI-assisted software development. Understanding these distinctions is crucial for developers and engineering leaders seeking to effectively integrate AI into their workflows.</p> Tool Name Autonomy Level Interaction Style Key Strengths Notable Limitations VS Code IntelliSense 1 (Basic Assist) Inline IDE suggestions as you type Fast, context-aware completions for syntax &amp; names based on language semantics and source analysis 7; works offline; reliable for known code patterns; extensible. No true \"AI\" generation of novel logic; limited to current file/project context; cannot understand higher-level intent [User Query]. Tabnine 1 (with L3 agent features) IDE plugin, ML-based autocomplete; AI chat; AI agents for specific tasks 8 ML-driven suggestions for full lines/functions; supports many languages; local models for privacy 8; team-trained models 10; agents for docs, review, Jira tasks.8 Core completion reactive; agent features newer; quality varies with context; large local models can be resource-intensive.10 GitHub Copilot (Standard Mode) 2 (Pair Programmer) IDE plugin (inline suggestions); Copilot Chat interface 10 Generates larger code blocks/functions from comments/context 10; broad framework knowledge; seamless IDE integration; chat for Q\\&amp;A/explanations.10 Can produce incorrect/insecure code requiring review 6; limited full-project context; cloud-dependent; privacy/licensing concerns [User Query]. Amazon CodeWhisperer 2 (Pair Programmer) IDE plugin (inline suggestions) 11 Optimized for AWS services and APIs 11; generates multi-line completions from comments; security scans 11; free for individuals.47 (Becoming part of Amazon Q Developer 11) Strong AWS bias, less effective for non-AWS work 47; suggestions can be basic; output needs review; requires AWS account [User Query]. OpenAI Codex (API) / GPT Models 2 (Pair Programmer) API/Playground; via ChatGPT (prompt-response); new \"Codex\" agent in ChatGPT 12 Powerful natural language to code generation (esp. GPT-4.1 13); many languages; new \"Codex\" agent can edit files, run commands.48 Original Codex API deprecated 12; API use requires prompt engineering, output needs validation; context limits; advanced access often paid [User Query]. Replit Ghostwriter 2 (Pair Programmer) Replit IDE integration (inline autocomplete &amp; Ghostwriter Chat) 49 Tight Replit integration for instant execution/testing; multi-line/function completion, refactoring, explanation, proactive debugger 50; good for learning/prototyping. Primarily Replit-locked 51; quality similar to other LLM tools (needs review); advanced AI features are paid 49; internet-dependent.51 GitHub Copilot (Agent Mode) 3 (Agentic \u2013 supervised) IDE chat/command (VS Code); GitHub issue assignment 14 Plans &amp; executes multi-file edits, runs tests, iterates on errors autonomously based on high-level goals/issues 14; creates PRs 44; developer kept in loop. Newer feature (stable in VS Code 14); can misinterpret complex requests, output needs review; performance varies with repo complexity.14 Cursor (AI Editor) 3 (Agentic \u2013 supervised) Full AI-based code editor (VS Code fork) with chat &amp; agent modes 15 Deep codebase indexing for global context 15; multi-file ops, refactoring; Agent Mode explores, plans, executes with tools 15; \"YOLO mode\" for auto-execution (user term for auto-run) 57; checkpoints for safety.15 Standalone editor (though high VS Code compatibility 56); auto-run/YOLO mode is risky 57; performance/cost depends on chosen LLM API keys.55 Windsurf (Cascade Agent) 3 (Agentic \u2013 supervised) AI-driven IDE (VS Code-based) with \"Cascade\" agent (Write/Chat modes) 63 Cascade agent modifies code, runs it, debugs, iterates until success (Write Mode) 16; uses tools (search, terminal, web); auto-fixes lint errors 63; \"Turbo mode\" for auto-execution.63 Newer tool; auto-execution (\"Turbo mode\") risky; must use its IDE; can make errors requiring human fix 68; some features paid/enterprise.65 ChatGPT (with Plugins &amp; Advanced Data Analysis) 3 (Agentic \u2013 supervised) Conversational AI (web UI or API) with Code Interpreter &amp; plugins (tool use) 75 Highly flexible multi-tool agent: can fetch docs, run/debug Python code iteratively, use Git (via plugins) 75; strong natural language understanding (GPT-4).74 Not an IDE tool by default (copy-paste workflow unless plugins bridge gap) [User Query]; plugin reliability varies; context limits; requires paid subscription for advanced features.82 Aider (CLI + Git assistant) 3 (Agentic \u2013 supervised) CLI chat tool working on local Git repo 17 Excellent Git integration (auto-commits, diffs); auto-runs linters/tests &amp; attempts fixes 17; supports various LLMs (cloud/local) 17; open-source.18 CLI-based (less visual, steeper curve) 18; requires API key setup; auto-fix quality depends on tests/linters; model errors possible [User Query]. Amazon Q Developer CLI 3 (Agentic \u2013 supervised) Interactive CLI agent (natural language to CLI commands &amp; code edits) 85 AI orchestrates local CLI tools (compilers, package managers, AWS CLI) 86; strong for AWS/cloud tasks; uses powerful models (Claude via Bedrock) 85; iterative adjustments based on feedback.85 Newer tool; executing commands automatically has inherent risks [User Query]; primarily benefits AWS users; requires AWS setup [User Query]. Anthropic Claude Code 4 (High Autonomy) Terminal-based chat agent; IDE integrations (VS Code, JetBrains) 19 Handles full-codebase context (million-token); edits files with diffs; runs tests, debugs, iterates; performs Git ops (branch, commit, PR, merge); designs modular solutions from prompts; proactively refactors and optimizes code; SDK for custom agents 19, 69 Output is production-ready with minimal review; CLI interface may deter some users; currently in beta 19; API costs can add up for large use cases 71; very large monorepos may still need human guidance Google Jules 4 (High Autonomy) Asynchronous cloud-based agent; GitHub integration (task assignment &amp; PRs) 20 Autonomous coding agent: takes high-level goals (features, bugs), works independently in cloud VM, presents PRs 20; uses Gemini 2.5 Pro; provides plan/reasoning; audio changelogs.20 Very new (public beta May 2025) 20; cloud-only (code sent to Google VM); asynchronous (no real-time interaction); high autonomy needs careful review of results [User Query]. Cognition Devin 4 (High Autonomy) Cloud AI service (task assignment via dashboard/Slack/CLI) 22 AI Software Engineer: plans &amp; executes complex end-to-end tasks using own shell, editor, browser 21; strong SWE-bench (13.86%) 23; Devin 2.0 has interactive planning, search, wiki.22 Expensive enterprise tool [User Query]; success on very complex/novel tasks still limited 23; requires codebase access by third-party; some demo authenticity concerns raised.22 GPT-Engineer (open-source) 4 (High Autonomy, experimental) CLI tool: single prompt -&gt; clarification dialogue -&gt; codebase generation 24 Can bootstrap entire (simple) projects from one prompt; automates initial spec-to-code process 24; open-source and customizable.24 Highly prompt-dependent; generated code needs significant review/debugging for complex apps 24; primarily for initial generation, not iterative work on existing code [User Query]. AutoGPT (for coding tasks) 4 (High Autonomy, experimental) Self-directed AI agent framework (user sets high-level goal, agent plans &amp; executes with tools) 98 Fully automated workflow: breaks goal into sub-tasks, uses tools (web search, file I/O, code execution) iteratively without human prompts.98 Often inefficient, prone to loops or incorrect paths without precise goals/human feedback 100; not specialized for coding; high API costs; mainly experimental for dev tasks.99 <p>Note</p> <p>Tools at Level 5 are omitted from the table as no productized examples currently exist; this level remains theoretical and experimental.)*</p> <p>This comparison underscores that as we progress from Level\u00a01 to Level\u00a04, AI tools take on more of the coding burden \u2014 from merely completing the next line of code to handling whole features. Developers today can mix and match these tools to suit their needs, but it\u2019s crucial to understand each tool\u2019s autonomy and limits to use them effectively. The field is evolving rapidly, and what is Level\u00a03 today may become Level\u00a04 tomorrow. Keeping an eye on these autonomy levels helps set expectations and guides us in adopting AI tools that truly enhance productivity while maintaining quality and control in software development.</p>"},{"location":"software/code/code_assistant_agents/#works-cited","title":"Works cited","text":"<ol> <li>The 2025 AI Index Report | Stanford HAI, accessed May 26, 2025, https://hai.stanford.edu/ai-index/2025-ai-index-report </li> <li>AI: A Declaration of Autonomy - Accenture, accessed May 26, 2025, https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Accenture-Tech-Vision-2025.pdf </li> <li>What are AI Agents?- Agents in Artificial Intelligence Explained - AWS, accessed May 26, 2025, https://aws.amazon.com/what-is/ai-agents/ </li> <li>A More Practical Future for Autonomous Vehicles | S\\&amp;P Global, accessed May 26, 2025, https://www.spglobal.com/automotive-insights/en/blogs/fuel-for-thought-a-more-practical-future-for-autonomous-vehicles </li> <li>Levels of intelligence: Navigating the future of AI, from robotic arms to autonomous cars, accessed May 26, 2025, https://roboticsandautomationnews.com/2025/05/17/levels-of-intelligence-navigating-the-future-of-ai-from-robotic-arms-to-autonomous-cars/90943/ </li> <li>GitHub CoPilot: Pros and Cons 2025 - PeerSpot, accessed May 26, 2025, https://www.peerspot.com/products/github-copilot-pros-and-cons </li> <li>IntelliSense - Visual Studio Code, accessed May 26, 2025, https://code.visualstudio.com/docs/editing/intellisense </li> <li>Tabnine AI Code Assistant | private, personalized, protected, accessed May 26, 2025, https://www.tabnine.com/ </li> <li>Plans &amp; Pricing | Tabnine: The AI code assistant that you control, accessed May 26, 2025, https://www.tabnine.com/pricing/ </li> <li>20 Best AI-Powered Coding Assistant Tools in 2025 - Spacelift, accessed May 26, 2025, https://spacelift.io/blog/ai-coding-assistant-tools </li> <li>What is CodeWhisperer? - CodeWhisperer - AWS Documentation, accessed May 26, 2025, https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-cwspr.html </li> <li>Codex Open AI: The Ultimate Guide to AI-Powered Coding in 2025 - Emelia.io, accessed May 26, 2025, https://emelia.io/hub/codex-open-ai </li> <li>Introducing GPT-4.1 in the API - OpenAI, accessed May 26, 2025, https://openai.com/index/gpt-4-1/ </li> <li>Introducing GitHub Copilot agent mode (preview) - Visual Studio Code, accessed May 26, 2025, https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode </li> <li>Agent Mode - Cursor, accessed May 26, 2025, https://docs.cursor.com/chat/agent </li> <li>Windsurf AI Agentic Code Editor: Features, Setup, and Use Cases | DataCamp, accessed May 26, 2025, https://www.datacamp.com/tutorial/windsurf-ai-agentic-code-editor </li> <li>Aider - AI Pair Programming in Your Terminal, accessed May 26, 2025, https://aider.chat/ </li> <li>Aider vs Windsurf: Which AI Coding Assistant Should You Choose? | UI Bakery Blog, accessed May 26, 2025, https://uibakery.io/blog/aider-vs-windsurf </li> <li>Claude Code overview - Anthropic API, accessed May 26, 2025, https://docs.anthropic.com/en/docs/agents/claude-code/introduction </li> <li>Jules: Google's autonomous AI coding agent - Google Blog, accessed May 26, 2025, https://blog.google/technology/google-labs/jules/ </li> <li>Devin | The AI Software Engineer, accessed May 26, 2025, https://devin.ai/ </li> <li>Report: Cognition Business Breakdown &amp; Founding Story | Contrary ..., accessed May 26, 2025, https://research.contrary.com/company/cognition </li> <li>SWE-bench technical report - Cognition, accessed May 26, 2025, https://cognition.ai/blog/swe-bench-technical-report </li> <li>GPT Engineer | AI Low-Code/No-Code Tool Review 2025 - LogicBalls, accessed May 26, 2025, https://logicballs.com/ai-tools/gpt-engineer </li> <li>From Prompt to Codebase: The Power of GPT Engineer - Kanaries Docs, accessed May 26, 2025, https://docs.kanaries.net/topics/ChatGPT/gpt-engineer </li> <li>arxiv.org, accessed May 26, 2025, https://arxiv.org/html/2504.15228 </li> <li>arXiv:2504.15228v1 [cs.AI] 21 Apr 2025, accessed May 26, 2025, https://arxiv.org/pdf/2504.15228 </li> <li>DeepMind's AlphaDev Reinvents Algorithms | Enterprise AI Daily, accessed May 26, 2025, https://www.enterpriseaisolutions.io/p/beyond-human-limits-f2bc </li> <li>MuZero, AlphaZero, and AlphaDev: Optimizing computer systems - Google DeepMind, accessed May 26, 2025, https://deepmind.google/discover/blog/muzero-alphazero-and-alphadev-optimizing-computer-systems/ </li> <li>AlphaDev discovers faster sorting algorithms - Google DeepMind, accessed May 26, 2025, https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/ </li> <li>Meta AI Introduces Meta LLM Compiler: A State-of-the-Art LLM that Builds upon Code Llama with Improved Performance for Code Optimization and Compiler Reasoning : r/machinelearningnews - Reddit, accessed May 26, 2025, https://www.reddit.com/r/machinelearningnews/comments/1dqn8to/meta_ai_introduces_meta_llm_compiler_a/ </li> <li>Meta's LLM Compiler: Innovating Code Optimization with AI ..., accessed May 26, 2025, https://www.unite.ai/metas-llm-compiler-innovating-code-optimization-with-ai-powered-compiler-design/ </li> <li>A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops - arXiv, accessed May 26, 2025, https://arxiv.org/html/2412.17149v1 </li> <li>Recursive self-improvement - Wikipedia, accessed May 26, 2025, https://en.wikipedia.org/wiki/Recursive_self-improvement </li> <li>Level 5 Autonomy: How Close Are We to Fully Self-Driving Cars? (Latest Industry Stats), accessed May 26, 2025, https://patentpc.com/blog/level-5-autonomy-how-close-are-we-to-fully-self-driving-cars-latest-industry-stats </li> <li>In response to critiques of Guaranteed Safe AI - AI Alignment Forum, accessed May 26, 2025, https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai </li> <li>Part 1: How Sourcegraph scales with the Language Server Protocol, accessed May 26, 2025, https://sourcegraph.com/blog/part-1-how-sourcegraph-scales-with-the-language-server-protocol </li> <li>JavaScript in Visual Studio Code, accessed May 26, 2025, https://code.visualstudio.com/docs/languages/javascript </li> <li>Features - CodeWhisperer - AWS Documentation, accessed May 26, 2025, https://docs.aws.amazon.com/codewhisperer/latest/userguide/features.html </li> <li>Code completion | IntelliJ IDEA Documentation - JetBrains, accessed May 26, 2025, https://www.jetbrains.com/help/idea/auto-completing-code.html </li> <li>IntelliJ IDEA | Features - JetBrains, accessed May 26, 2025, https://www.jetbrains.com/idea/features/ </li> <li>IntelliCode for Visual Studio | Microsoft Learn, accessed May 26, 2025, https://learn.microsoft.com/en-us/visualstudio/ide/intellicode-visual-studio?view=vs-2022 </li> <li>IntelliCode Whole-line autocompletions - Visual Studio (Windows) | Microsoft Learn, accessed May 26, 2025, https://learn.microsoft.com/en-us/visualstudio/ide/visual-studio-whole-line-completions?view=vs-2022 </li> <li>GitHub Copilot features, accessed May 26, 2025, https://docs.github.com/en/copilot/about-github-copilot/github-copilot-features </li> <li>About individual Copilot plans and benefits - GitHub Docs, accessed May 26, 2025, https://docs.github.com/en/copilot/managing-copilot/managing-copilot-as-an-individual-subscriber/getting-started-with-copilot-on-your-personal-account/about-individual-copilot-plans-and-benefits </li> <li>GitHub Copilot Pros and Cons, accessed May 26, 2025, https://www.netguru.com/blog/github-copilot </li> <li>Best AI Code Assistants In 2025 - Top 5 Tools For Developers - Revoyant, accessed May 26, 2025, https://www.revoyant.com/blog/best-ai-code-assistants-2025-top-tools </li> <li>Introducing Codex - OpenAI, accessed May 26, 2025, https://openai.com/index/introducing-codex/ </li> <li>How to Use Replit AI for free - Apidog, accessed May 26, 2025, https://apidog.com/blog/how-to-use-replit-ai-for-free/ </li> <li>Replit Ghostwriter vs. Copilot: Which is Better? - CodeStringers, accessed May 26, 2025, https://www.codestringers.com/insights/replit-ghostwriter-vs-copilot/ </li> <li>11 Best AI Coding Tools\u2014Guide for Developers (2025 updated) - Golden Owl, accessed May 26, 2025, https://goldenowl.asia/blog/best-ai-for-coding </li> <li>Replit \u2014 Announcing Ghostwriter Chat: The first conversational AI ..., accessed May 26, 2025, https://blog.replit.com/gw-chat-launch </li> <li>Agent mode 101: All about GitHub Copilot's powerful mode - The ..., accessed May 26, 2025, https://github.blog/ai-and-ml/github-copilot/agent-mode-101-all-about-github-copilots-powerful-mode/ </li> <li>Complete Guide to GitHub Copilot Agent Mode: Transforming Development Paradigms, accessed May 26, 2025, https://blog.wadan.co.jp/en/tech/github-copilot-agent-mode </li> <li>Cursor AI Review: Best AI Code Editor in 2025?, accessed May 26, 2025, https://www.fahimai.com/cursor-ai </li> <li>Cursor AI: An In-Depth Review (May 2025 Update) | Engine, accessed May 26, 2025, https://www.enginelabs.ai/blog/cursor-ai-an-in-depth-review-may-2025-update </li> <li>Guide to Cursor | Software.com, accessed May 26, 2025, https://www.software.com/ai-index/tools/cursor </li> <li>docs.cursor.com, accessed May 26, 2025, https://docs.cursor.com/context/codebase-indexing#:\\~:text=For%20better%20and%20more%20accurate,initialize%20indexing%20for%20that%20workspace. </li> <li>Codebase Indexing - Cursor, accessed May 26, 2025, https://docs.cursor.com/context/codebase-indexing </li> <li>Top 5 AI IDEs for Coding with Windsurf in 2025 By Girish Kot - Peerlist, accessed May 26, 2025, https://peerlist.io/gkotte/articles/top-5-ai-ides-for-coding-with-windsurf-in-2025 </li> <li>How I use Cursor (+ my best tips) - Builder.io, accessed May 26, 2025, https://www.builder.io/blog/cursor-tips </li> <li>Cursor AI Unleashed: The Agent-Powered Code Editor Transforming Developer Workflow, accessed May 26, 2025, https://savemyleads.com/blog/useful/cursor-ai </li> <li>Windsurf (formerly Codeium) - The most powerful AI Code Editor, accessed May 26, 2025, https://windsurf.com/ </li> <li>Windsurf Editor by Codeium for Revit: AI Automation in Revit - ArchiLabs, accessed May 26, 2025, https://archilabs.ai/posts/windsurf-for-revit </li> <li>Cascade - Windsurf Docs, accessed May 26, 2025, https://docs.windsurf.com/windsurf/cascade/cascade </li> <li>Windsurf Reviews, Ratings &amp; Features 2025 | Gartner Peer Insights, accessed May 26, 2025, https://www.gartner.com/reviews/market/ai-code-assistants/vendor/windsurf-exafunction/product/windsurf-exafunction </li> <li>Windsurf Editor Changelogs - Codeium, accessed May 26, 2025, https://codeium.com/changelog </li> <li>Windsurf Reviews 2025: Details, Pricing, &amp; Features | G2, accessed May 26, 2025, https://www.g2.com/products/codeium/reviews </li> <li>Introducing Claude 4 - Anthropic, accessed May 26, 2025, https://www.anthropic.com/news/claude-4 </li> <li>Claude Code: Best practices for agentic coding - Anthropic, accessed May 26, 2025, https://www.anthropic.com/engineering/claude-code-best-practices </li> <li>Introducing Claude 4 \\ Anthropic, accessed May 26, 2025, https://anthropic.com/news/claude-4 </li> <li>Claude Anthropic Reviews &amp; Ratings | Pros &amp; Cons - Software Finder, accessed May 26, 2025, https://softwarefinder.com/artificial-intelligence/claude-anthropic/reviews </li> <li>15 Pros &amp; Cons of Claude [2025] - DigitalDefynd, accessed May 26, 2025, https://digitaldefynd.com/IQ/pros-cons-of-claude/ </li> <li>ChatGPT \u2014 Release Notes - OpenAI Help Center, accessed May 26, 2025, https://help.openai.com/en/articles/6825453-chatgpt-release-notes </li> <li>How to Use ChatGPT's New Code Interpreter - DevDynamics, accessed May 26, 2025, https://devdynamics.ai/blog/how-to-use-chatgpts-new-code-interpreter/ </li> <li>Why ChatGPT's Code Interpreter is the Future of Programming - BotPenguin, accessed May 26, 2025, https://botpenguin.com/blogs/why-chatgpt-code-interpreter-is-future-of-programming </li> <li>10 of The Best ChatGPT Plugins to Get The Most From AI in 2024 | DataCamp, accessed May 26, 2025, https://www.datacamp.com/blog/best-chat-gpt-plugins </li> <li>The Top 9 ChatGPT Plugins for Developers in 2025 - ISHIR, accessed May 26, 2025, https://www.ishir.com/blog/119322/the-top-9-chatgpt-plugins-for-developers-in-2024.htm </li> <li>GitLab ChatGPT (OpenAI) Integration - Quick Connect - Zapier, accessed May 26, 2025, https://zapier.com/apps/gitlab/integrations/chatgpt </li> <li>HighwayofLife/awesome-chatgpt-plugins - GitHub, accessed May 26, 2025, https://github.com/HighwayofLife/awesome-chatgpt-plugins </li> <li>Explore the Top 10 Best Chatgpt Plugins of 2025 - Murf AI, accessed May 26, 2025, https://murf.ai/blog/best-chatgpt-plugins </li> <li>ChatGPT AI Tools Review 2025: Pros, Cons, and Pricing - Sonary, accessed May 26, 2025, https://sonary.com/b/open-ai/chatgpt-writing-tool+ai-tools/ </li> <li>Aider Documentation, accessed May 26, 2025, https://aider.chat/docs/ </li> <li>Best AI Coding Assistants as of May 2025 - Shakudo, accessed May 26, 2025, https://www.shakudo.io/blog/best-ai-coding-assistants </li> <li>Amazon Q Developer announces a new CLI agent within the command line - AWS, accessed May 26, 2025, https://aws.amazon.com/about-aws/whats-new/2025/03/amazon-q-developer-cli-agent-command-line/ </li> <li>A lightning fast, new agentic coding experience within the Amazon Q Developer CLI - AWS, accessed May 26, 2025, https://aws.amazon.com/blogs/devops/introducing-the-enhanced-command-line-interface-in-amazon-q-developer/ </li> <li>Amazon Q Developer Reviews 2025: Details, Pricing, &amp; Features | G2, accessed May 26, 2025, https://www.g2.com/products/amazon-q-developer/reviews </li> <li>Best AI Coding Assistant 2025: Complete Guide to Cline and Cursor ..., accessed May 26, 2025, https://cline.bot/blog/best-ai-coding-assistant-2025-complete-guide-to-cline-and-cursor </li> <li>Qodo AI Reviews: Use Cases, Pricing &amp; Alternatives - Futurepedia, accessed May 26, 2025, https://www.futurepedia.io/tool/qodo </li> <li>Qodo Reviews 2025: Details, Pricing, &amp; Features | G2, accessed May 26, 2025, https://www.g2.com/products/qodo/reviews </li> <li>Google I/O 2025: Google's answer to Microsoft and OpenAI's AI coding agents, Jules is now available for everyone to try - The Times of India, accessed May 26, 2025, https://timesofindia.indiatimes.com/technology/tech-news/google-i/o-2025-googles-answer-to-microsoft-and-openais-ai-coding-agents-jules-is-now-available-for-everyone-to-try/articleshow/121298997.cms </li> <li>Google Counters GitHub &amp; Microsoft with Jules Agent &amp; Enhanced Gemini AI, accessed May 26, 2025, https://visualstudiomagazine.com/articles/2025/05/20/google-counters-github-microsoft-with-jules-agent-enhanced-gemini-ai.aspx </li> <li>What Google I/O 2025's AI Announcements Mean for Developers - RisingStack Engineering, accessed May 26, 2025, https://blog.risingstack.com/google-io-2025-recap/ </li> <li>Devin AI vs Cursor: Speed &amp; Accuracy Test Results - Trickle AI, accessed May 26, 2025, https://www.trickle.so/blog/devin-ai-or-cursor </li> <li>Generate Complete Apps with Ease: GPT Engineer Review and Setup - Toolify.ai, accessed May 26, 2025, https://www.toolify.ai/ai-news/generate-complete-apps-with-ease-gpt-engineer-review-and-setup-751744 </li> <li>Build Any App with One Prompt Using GPT-Engineer - YouTube, accessed May 26, 2025, https://m.youtube.com/watch?v=-W73zQr0rl4\\&amp;pp=ygUII21nY29kZXI%3D </li> <li>What I learned in 6 months of working on a CodeGen dev tool GPT Pilot, accessed May 26, 2025, https://dev.to/zvone187/what-i-learned-in-6-months-of-working-on-a-codegen-dev-tool-gpt-pilot-4fij </li> <li>AutoGPT - AI Agent Reviews, Features, Use Cases &amp; Alternatives (2025), accessed May 26, 2025, https://aiagentsdirectory.com/agent/autogpt </li> <li>Autogpt Examples: Expert Tips for Success - Codoid, accessed May 26, 2025, https://codoid.com/ai/autogpt-examples-expert-tips-for-success/ </li> <li>What is AutoGPT and How to Use It? - GeeksforGeeks, accessed May 26, 2025, https://www.geeksforgeeks.org/what-is-autogpt-and-how-to-use-it/ </li> <li>AutoGPT Example Guide: With Hands-On Applications - PageTraffic, accessed May 26, 2025, https://www.pagetraffic.com/blog/autogpt-example/ </li> <li>Multi-Agent Collaboration Mechanisms: A Survey of LLMs - arXiv, accessed May 26, 2025, https://arxiv.org/html/2501.06322v1 </li> <li>Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications - arXiv, accessed May 26, 2025, https://arxiv.org/html/2412.05449v1 </li> <li>Agent-Oriented Programming Examples: Real-World Applications and Use Cases, accessed May 26, 2025, https://smythos.com/ai-agents/ai-agent-development/agent-oriented-programming-examples/ </li> <li>20 AI Agent Examples in 2025 - AutoGPT, accessed May 26, 2025, https://autogpt.net/20-ai-agents-examples/ </li> <li>2025 AI predictions: The future of application development - OutSystems, accessed May 26, 2025, https://www.outsystems.com/blog/posts/ai-predictions/ </li> <li>AI Agents in 2025: Expectations vs. Reality | IBM, accessed May 26, 2025, https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality </li> <li>Revolutionary AI Coding Tool: Cody - Made for Efficient Programming, accessed May 26, 2025, https://www.toolify.ai/ai-news/revolutionary-ai-coding-tool-cody-made-for-efficient-programming-395707 </li> <li>The 9 best AI coding tools for developers of all levels - Lummi, accessed May 26, 2025, https://www.lummi.ai/blog/best-ai-coding-tools </li> <li>AI for Testing: Context &amp; Eval with Sourcegraph - AI Native Dev, accessed May 26, 2025, https://ainativedev.io/podcast/ai-testing-context-eval-sourcegraph </li> <li>Agentic code generation: The future of software development - AI Accelerator Institute, accessed May 26, 2025, https://www.aiacceleratorinstitute.com/agentic-code-generation-the-future-of-software-development/</li> </ol>"},{"location":"software/code/codeai_report/","title":"Codeai report","text":"<p>From Code Completion to Autonomous Coding Agents: AI Developer Tools by Autonomy LevelThe software development landscape is undergoing a significant transformation, largely propelled by the rapid advancements and integration of Artificial Intelligence (AI). This evolution is underscored by substantial growth in private AI investment, with generative AI alone attracting $33.9 billion globally in 2024, marking an 18.7% increase from the previous year.1 AI is no longer confined to research laboratories; it is becoming a pervasive force in daily life and business operations, evidenced by 78% of organizations reporting AI use in 2024, a notable rise from 55% in 2023.1 This trend extends deeply into the realm of developer tooling, where AI-powered solutions are becoming increasingly sophisticated. The discourse itself is shifting from AI as a tool for mere \"automation\" to AI exhibiting \"autonomy,\" signifying a deeper capacity for independent operation and decision-making.2 Accenture's Technology Vision for 2025 highlights this transition, projecting a future where AI acts autonomously on behalf of individuals, with a reported 75% of knowledge workers already utilizing generative AI.2 This massive investment and widespread adoption signal strong market confidence and a perceived return on investment, suggesting that AI developer tools are being embraced at a pace necessary to remain competitive and enhance productivity, aligning with research indicating AI's potential to boost productivity and narrow skill gaps.1This report provides an expert-level analysis of these AI developer tools, categorizing them based on a five-level autonomy framework. This framework, analogous to the levels established for autonomous driving systems, serves to delineate the evolving capabilities of these tools, from basic code assistants to highly autonomous coding agents. Such a classification is pertinent because both autonomous driving and AI coding involve tasks traditionally performed by humans that demand complex reasoning, contextual understanding, and decision-making. The staged levels in both domains represent milestones in the delegation of these responsibilities to AI, thereby raising similar questions about reliability, accountability, and the evolving role of human oversight.4 The challenges anticipated for Level 5 autonomy in vehicles, such as regulatory hurdles, technological limitations, and development costs 4, are likely to find parallels in the domain of AI coding. This report will explore the capabilities, interaction modes, use cases, and the pros and cons of representative tools at each autonomy level, ensuring factual accuracy through direct references to current research and official documentation. The increasing integration of AI into development workflows promises substantial productivity gains 1 but also introduces new considerations regarding developer roles, code quality, security, and trust. This document aims to navigate these complexities, offering a clear perspective on the current state and future trajectory of AI in software engineering.Autonomy Levels for AI Coding ToolsTo provide a structured understanding of their capabilities, AI coding tools are classified into autonomy levels, drawing an analogy to the widely recognized levels of autonomous driving. This classification helps to differentiate tools based on the degree of human oversight required and the complexity of tasks they can handle.</p> <p>Level 1 \u2013 Basic Code Completion:AI at this level offers single-line or small code suggestions based on the immediate context, such as the current file content and syntax. The developer remains in full control, accepting or rejecting these suggestions for micro-tasks. This functionality is comparable to basic cruise control in automobiles. Tools categorized under Level 1 are designed to accelerate typing and minimize trivial errors but do not generate complex code blocks, understand broader project context, or make independent decisions regarding logic or structure [User Query]. Their primary function is to react to the developer's current input by predicting likely code sequences. The focus is on immediate lexical and syntactic context rather than deep semantic understanding. For instance, Visual Studio Code's IntelliSense provides completions based on \"language semantics and an analysis of your source code,\" but for many languages, this is limited to \"word-based completions\" without specific extensions 7, reacting to typed input within the current file. Similarly, Tabnine offers \"context-aware suggestions based on your code and patterns\" but mainly provides \"code completions for current line and multiple lines for full-function implementation\" 8, which is also a reactive process. The core function is to \"predict likely code to follow,\" not to comprehend higher-level intent [User Query].</p> <p>Level 2 \u2013 Advanced Code Generation (AI Pair Programmer):AI systems at this level can produce larger code snippets or entire functions based on natural language prompts (e.g., comments within the code) or a broader local context. The AI functions akin to a junior pair programmer. However, each output from the AI is typically a single-step operation that is subsequently reviewed and integrated by the human developer. Developers continue to guide the overall software architecture, verify the correctness of the generated code, and explicitly prompt the AI for each significant generation task [User Query]. These tools are often powered by large language models (LLMs) trained on extensive code repositories. The defining characteristic of Level 2 is the AI's capacity to translate natural language intent into substantial code blocks, moving beyond simple completion to actual generation, yet still necessitating explicit human prompting for each distinct operation. For example, GitHub Copilot can suggest \"whole lines or entire functions\" based on comments or existing code context 10, with the interaction being a prompt-response for each generated piece. Amazon CodeWhisperer generates \"suggestions based on your existing code and comments,\" which can range from \"a single line comment to fully formed functions\" 11, initiated by the developer's input. OpenAI Codex and its more advanced successors like GPT-4 are designed to \"translates natural language into code\" 12, representing a direct prompt-to-code generation step. In all these instances, the human developer acts as the driver, prompting for each segment of code and undertaking its review, as the AI does not autonomously chain actions.</p> <p>Level 3 \u2013 Context-Aware Agents (Supervised Automation):At this level, AI demonstrates more agentic behavior, capable of performing multi-step coding tasks within an iterative loop to achieve a high-level goal defined by the developer (e.g., \u201cAdd a profile feature,\u201d \u201cFix this bug across these files\u201d). The AI takes on the responsibility of planning the necessary steps, writing or editing multiple files, running code or tests, debugging, and iterating with minimal prompting once the initial objective is set. The human developer supervises this process, with the option to intervene or review final changes, but refrains from micromanaging each individual code edit or command execution. This dynamic is comparable to a junior developer autonomously working on a feature under the guidance of a senior colleague [User Query]. These agents typically maintain state, possess a broader understanding of the codebase, and can interact directly with the development environment, including the IDE, command-line interface (CLI), and version control systems. The critical advancement to Level 3 is the AI's capacity to autonomously plan and execute a sequence of actions\u2014multi-step operations\u2014towards a developer-defined goal, which includes interaction with the development environment (e.g., running tests, editing multiple files). For example, GitHub Copilot Agent Mode \"performs multi-step coding tasks... analyzing your codebase, reading relevant files, proposing file edits, and running terminal commands and tests... auto-corrects in a loop\".14 Cursor's Agent Mode \"independently explores your codebase, identifies relevant files, and makes necessary changes... uses all available tools to search, edit, create files, and run terminal commands,\" and it \"breaks complex tasks into manageable steps and executes them in sequence\".15 Windsurf's Cascade feature \"generates or modifies code... asks for your approval before running it in the terminal... prompts you with follow-up questions\" in an iterative cycle 16; its Write Mode can \"create multiple files for you, run scripts, test them, and debug them\".16 Aider \"automatically commits changes with sensible commit messages\" and \"automatically lint and test your code every time aider makes changes... can fix problems detected by your linters and test suites\" 17, indicating multiple automated steps. Anthropic's Claude Code is capable of \"edit files and fixing bugs across your codebase... executing and fixing tests... creating commits and PRs\" 19, a clear sequence of diverse actions. The common characteristic is the AI's initiative in handling sub-tasks after receiving a high-level directive, a hallmark of agency.</p> <p>Level 4 \u2013 Highly Autonomous Coding:AI tools at this level can manage complex projects or features end-to-end, requiring only minimal human input, such as an initial prompt or specification, and theoretically, no mandatory oversight on each individual change. These systems are envisioned to design solutions, write code across the entire codebase, perform testing and debugging, and even proactively improve code. The output is intended to be production-ready, although a final human review might still be conducted as a precautionary measure. This level of autonomy is comparable to trusting a senior engineer: providing requirements and expecting a complete solution for review [User Query]. These agents incorporate advanced planning capabilities, long-term memory of the project, and sophisticated integration of various tools. Level 4 signifies a shift from supervised autonomy to near-complete operational independence for complex, end-to-end tasks, with the human role transitioning from that of a supervisor to a client or final approver. Google's Jules, for instance, is described as an \"autonomous agent that reads your code, understands your intent, and gets to work\" asynchronously in the cloud, managing tasks like \"building new features\" or \"fixing bugs\" and then presenting a diff of the changes 20; the developer assigns the task and reviews the outcome later. Cognition's Devin is marketed as \"the first AI software engineer,\" designed for \"planning and executing complex engineering tasks end-to-end,\" including building full-stack applications or autonomously fixing bugs.21 While its actual success rate on complex benchmarks like SWE-bench is approximately 13.9% 23, the intended capability aligns with Level 4. Experimental tools such as GPT-Engineer aim to \"generate an entire codebase\" from a single prompt, engaging in clarifying dialogue before implementation.24 This \"one-shot\" project generation embodies a high degree of autonomy. The key distinction from Level 3 is the reduced necessity for ongoing supervision and the expectation of a more complete, potentially production-ready deliverable derived from a high-level specification.</p> <p>Level 5 \u2013 Full Autonomy (Experimental/Future):At this largely theoretical stage, an AI coding agent would be capable of setting its own goals and continuously improving software without requiring explicit human-defined tasks. Such an agent might analyze user needs, system metrics, or market trends to autonomously decide to implement new features, refactor existing code, or perform optimizations. As of 2025, this level remains speculative, and no current tools achieve this degree of autonomy, although research projects offer glimpses into this potential future [User Query]. Level 5 transcends mere task execution; it involves AI-driven goal formulation and proactive, continuous software evolution, which introduces substantial challenges concerning alignment, trust, and safety. An AI agent that \"sets its own goals\" [User Query] aligns with definitions where AI \"independently chooses the best actions it needs to perform to achieve those goals\" after humans set initial, very high-level objectives; however, Level 5 implies the AI might even define these initial strategic objectives based on broader system analysis.3 Research into \"self-improving coding agents\" like SICA, which can \"autonomously edit itself, and thereby improve its performance\" 26, represents a step in this direction. If an agent can improve its own codebase, it is a precursor to proactively improving application codebases. DeepMind's AlphaDev, which discovered novel, more efficient algorithms 28, and Meta AI's research on LLM Compilers for code optimization 31 and the autonomous optimization of Agentic AI systems 33, also touch upon aspects of proactive improvement. However, these systems still operate on predefined benchmarks or human-defined problems. The challenges are immense: establishing \"trust, safety, and alignment\" [User Query] becomes paramount if an AI is to autonomously alter production systems based on its own derived goals.35 Sourcegraph, for example, currently views AI as \"not about building a fully autonomous code-writing A.I.,\" emphasizing human-AI collaboration instead.37</p> <p>The following sections will detail popular developer AI tools, grouped by these autonomy levels.Level 1 \u2013 Basic Code Completion AssistantsLevel 1 tools provide intelligent in-line code suggestions to speed up typing and reduce trivial errors. They typically complete a single word, line, or simple code block based on the current context (file content, syntax, and maybe local context from open files). The developer must still drive all logic and make all decisions \u2013 the AI just predicts likely code to follow, much like an advanced autocomplete. These tools don\u2019t \u201cunderstand\u201d higher-level intent; they react to immediate context.Characteristics: Limited scope suggestions (often one line at a time), no long-range planning, no ability to perform actions beyond text insertion. They use either static analysis or trained models on code patterns. The interaction is passive: as you type, they offer completions you can accept or ignore.Tools in this category include basic IDE autocompletion and early ML-powered completers:</p> <p>Visual Studio Code IntelliSense (Level 1):</p> <p>Autonomy: Level 1 (baseline code assist). Key Characteristics: Visual Studio Code (VS Code) IntelliSense offers a suite of code editing features, including code completion, parameter information, quick info, and member lists. This functionality is provided out-of-the-box for languages such as JavaScript, TypeScript, JSON, HTML, CSS, SCSS, and Less.7 For other programming languages, VS Code supports word-based completions by default, but this can be significantly enhanced with richer IntelliSense capabilities by installing specific language extensions.7 These language services deliver intelligent code completions grounded in language semantics and a thorough analysis of the developer's source code.7 Consequently, IntelliSense offers various types of completions, encompassing suggestions from language servers, predefined snippets, and straightforward word-based textual completions.7 It is designed to predict variables, function names, and general code syntax as the developer types, leveraging static typing information and the immediate code context within the current file or, for JavaScript projects, a broader project context if a jsconfig.json file is present.38 Mode of Interaction: Suggestions typically appear automatically in an editor dropdown menu as the developer types. They can also be manually triggered using the Ctrl+Space keyboard shortcut or by typing a language-specific trigger character, such as the dot (.) character in JavaScript.7 Developers can accept these suggestions by pressing Enter or Tab.7 Use Cases: The primary use case is to accelerate the typing of known identifiers and boilerplate code. This includes suggesting object property names, completing function signatures, and leveraging JSDoc annotations to provide richer IntelliSense for JavaScript code.38 IntelliSense can also display quick information about symbols and parameter information for method calls.7 Pros: IntelliSense is generally very fast and highly reliable for known APIs and symbols within the current project's context.7 Its core functionalities, based on local analysis, operate offline, which is a significant advantage for uninterrupted development. Furthermore, these local operations alleviate privacy concerns often associated with cloud-based AI tools [User Query]. As described, it \u201cpredicts keywords, variable names, and suggests basic syntax\u201d to accelerate coding [User Query]. Cons: IntelliSense does not possess true \"intelligence\" in the sense of generating novel code or understanding complex, higher-level developer intent beyond the immediate syntactic and semantic context [User Query]. Its suggestions are predicated on static analysis and recognized patterns within the existing project or language specifications, rather than on deep learning from a vast external corpus of code, unless it is augmented by other, more advanced AI tools [User Query]. Consequently, it cannot perform multi-line insertions beyond simple predefined patterns or snippets.7 VS Code IntelliSense, while a Level 1 tool, establishes a crucial baseline for developer assistance in modern Integrated Development Environments (IDEs). Its effectiveness is often enhanced by specific project configurations (like jsconfig.json for JavaScript projects 38) and language-specific extensions, underscoring the principle that even foundational assistance benefits from richer contextual information. The modular design of IntelliSense, relying on extensions for enhanced support in many languages 7, allows for a layered approach where more sophisticated AI capabilities can be integrated. Many Level 2 and higher AI tools, such as GitHub Copilot and Amazon CodeWhisperer, are delivered as VS Code extensions 10, building upon the foundational editing and completion environment provided by the IDE and its IntelliSense.</p> <p>Tabnine (Level 1, with emerging Level 3 aspects):</p> <p>Autonomy: Primarily Level 1 for its core code completion functionality. However, Tabnine's introduction of \"AI Agents\" signals a strategic move towards capabilities that align with Level 3 autonomy for specific tasks. Key Characteristics: Tabnine is an AI-driven code completion tool that significantly extends basic autocomplete functionalities through the use of machine learning [User Query]. Its underlying models predict code at the token or line level and are capable of completing entire snippets or even function bodies based on the surrounding context.8 Tabnine offers broad language support, covering over 80 programming languages, and provides flexibility in its deployment with both local and cloud-based AI models. A strong emphasis is placed on privacy; proprietary models are never trained on user code, and user code is not stored or shared without explicit permission.8 Furthermore, Tabnine allows for the creation of team-trained models, which learn from an organization's shared codebase to provide more personalized and relevant suggestions.10 The \"Tabnine 2 Protected model\" is specifically trained only on permissively licensed code to mitigate intellectual property concerns.8 Mode: Tabnine is typically installed as an IDE plugin, delivering on-the-fly suggestions for the current and subsequent lines of code as the developer types.10 In addition to code completion, it also offers an AI-powered chat feature that utilizes leading Large Language Models (LLMs) for Q&amp;A and explanations.9 Use Cases: Tabnine is used to accelerate coding by providing more powerful completions than basic IntelliSense, such as for idiomatic code patterns, suggesting function arguments, and generating common code constructs from minimal typed input [User Query]. Beyond completion, its \"AI agents\" can perform a range of tasks including documentation generation, code explanation, test generation, and automated code fixing.8 Pros: Tabnine facilitates faster coding through \"accurate and personalized code completions for snippets, whole lines, and even full functions\".9 It enhances the relevancy of its suggestions by learning from the user's codebase and optionally from global code data.8 The tool is compatible with a wide array of IDEs and offers flexible deployment options (on-premises, Virtual Private Cloud, or SaaS) to cater to various privacy requirements.8 The suite of AI agents for tasks like documentation, code review, and test generation extends its utility beyond simple code completion.8 Cons: The core code completion functionality remains largely reactive (Level 1), meaning it does not autonomously handle multi-step tasks or engage in deep reasoning without the explicit invocation of specific agent functionalities. The quality of suggestions can vary if the contextual information is ambiguous [User Query]. Running large AI models locally can be memory-intensive [User Query]. The free version of Tabnine offers a more limited set of features compared to its paid tiers.10 Tabnine's evolution exemplifies a broader trend where tools initially centered on Level 1 or Level 2 capabilities are progressively incorporating Level 3 agentic functions. This blurs the strict demarcations between autonomy levels within a single product offering. While Tabnine's core strength remains its advanced code completion 9, the introduction of \"AI agents\" like the \"Code Review Agent,\" \"Jira Implementation and Validation Agents,\" \"Testing Agent,\" and \"Code Fix Agent\" 8 clearly indicates a move towards more autonomous, goal-oriented behaviors characteristic of Level 3. For instance, an agent that can \"autonomously generate fixes\" or create \"comprehensive test plans\" operates at a higher level of autonomy than mere code suggestion. This strategic direction suggests an ambition to provide a more holistic AI development platform, building upon a successful completion tool by adding features with greater autonomy. Consequently, users need to be cognizant of which specific Tabnine feature they are employing to accurately understand its operational autonomy level.</p> <p>Other Level 1 Examples:</p> <p>JetBrains IDEs Autocompletion: Integrated Development Environments from JetBrains, such as IntelliJ IDEA, feature sophisticated context-sensitive code completion. This system suggests names for classes, methods, fields, and keywords by analyzing the current context to propose reachable choices, including live templates.40 Basic completion can be triggered manually (e.g., via Ctrl+Space) or can appear automatically as the developer types.40 A notable feature is \"Smart Completion\" (Type-Matching Completion), which intelligently filters the suggestion list to display only types that are applicable to the current context.40 These IDEs also support statement completion, which helps create syntactically correct code constructs, and even offer ML-assisted completion ranking that prioritizes suggestions based on the collective choices of other users in similar coding situations.40</p> <p>Pros: The autocompletion is highly integrated into the IDE, deeply context-aware, and supports a variety of completion types (basic, smart, statement). The ML-ranking feature further refines suggestion relevance over time.40 Cons: These features are primarily rule-based and context-driven within the scope of the current project. They do not generate novel logic beyond established patterns or undertake multi-step tasks.40</p> <p>Visual Studio IntelliCode: This tool extends Visual Studio's standard IntelliSense by incorporating AI to enhance code suggestions. It provides context-aware code autocompletions, which notably include whole-line completions for C# developers.42 IntelliCode's suggestions are distinguished by a star icon and are prioritized at the top of the completion list, as it predicts the most likely correct API based on the current code context and patterns learned from thousands of high-quality open-source projects.42 It also offers argument completion, suggesting the most probable argument names for method calls.42</p> <p>Pros: AI-assisted suggestions are often more contextually relevant than simple alphabetical listings. Whole-line completions can significantly speed up coding in C#. IntelliCode can also track repetitive coding actions and offer suggestions to automate them (currently C#-only).42 Cons: Some of its advanced features may still be in preview. Whole-line completion is primarily available for C#. The reliance on patterns from public code repositories might mean that suggestions do not always align perfectly with the specific conventions or needs of private codebases.42 The common characteristic among these advanced Level 1 tools is the application of machine learning to augment traditional static analysis techniques. This makes their suggestions more \"intelligent\" by considering broader patterns, either learned locally from the user's project or from extensive open-source data. However, they fundamentally remain assistive tools that complete or suggest code based on what the developer has initiated, without independently generating complex logic or undertaking multi-step actions, which are the defining features of higher autonomy levels.</p> <p>Level 2 \u2013 Advanced Code Generation (AI Pair Programmers)Level 2 tools function as AI pair programmers, capable of generating more substantial segments of code in response to prompts or contextual cues, thereby moving beyond the single-line completions typical of Level 1. At this stage, the AI can infer the developer's intent from comments or function signatures and produce code designed to fulfill that intent. Nevertheless, the AI's role remains reactive and confined to single-step operations; it responds to a human prompt with a discrete block of code. The developer continues to drive the overall process, providing instructions for each task, and the AI's output is intended for review and potential modification by the human.Characteristics: These tools exhibit context-aware generation of larger code blocks, such as entire functions or classes, as well as explanations of code. They are frequently powered by large language models (LLMs) extensively trained on code repositories (e.g., OpenAI's Codex family, Code Llama, Anthropic's Claude). While they may integrate into IDEs or operate via chat interfaces, they do not autonomously execute the code they generate or make further changes beyond their immediate response. The typical interaction involves the developer writing a comment or posing a question, to which the AI responds with a code suggestion or an answer.Representative tools at Level 2 include:</p> <p>GitHub Copilot (Level 2 - Standard Mode):</p> <p>Autonomy: Level 2. (It is important to note that GitHub Copilot also offers an \"Agent Mode,\" which operates at Level 3 and is discussed in a subsequent section). Key Characteristics: GitHub Copilot functions as an \u201cAI pair programmer,\u201d seamlessly integrated into popular code editors such as Visual Studio Code and JetBrains IDEs.6 It leverages OpenAI's advanced models (historically Codex, and now more sophisticated GPT versions 6) to generate a wide range of code suggestions, from single lines to complete functions.10 Copilot is adept at inferring developer intent from various contextual cues, including function names, comments, or the surrounding code.6 It offers support for a multitude of programming languages.10 Mode: The primary mode of interaction is through inline completion suggestions that appear as the developer writes code. Additionally, Copilot features \"Copilot Chat,\" an interactive chat interface that allows developers to ask coding-related questions, obtain explanations of code segments, generate unit tests, and receive suggestions for fixes or refactoring. This chat functionality is available within IDEs and directly on the GitHub.com website.10 Use Cases: Common use cases include generating boilerplate code, implementing standard algorithms, and suggesting API calls based on descriptive comments (e.g., a comment like \u201c// function to sort an array of numbers\u201d can prompt Copilot to generate the corresponding sorting function).46 Copilot Chat can be used to explain existing code, offer refactoring ideas, and generate tests.44 Pros: Copilot's seamless IDE integration often makes it feel like an expert assistant is readily available to help.6 It significantly accelerates the process of writing routine code and boilerplate, with some reports indicating productivity increases of up to 40% for experienced developers.6 Its extensive knowledge base, covering numerous frameworks and languages, often saves developers the time they would otherwise spend consulting documentation.10 The Copilot Chat feature further enhances its utility by providing an interactive platform for problem-solving and code explanation.10 Cons: A critical consideration is that the code generated by Copilot can sometimes be inaccurate, insecure, or suboptimal, necessitating careful review and thorough testing by the developer.46 Its understanding of the broader project context is limited, primarily focusing on the currently open files and some related content, which can lead to inconsistencies or non-compiling code if not guided effectively.46 There is also a risk, albeit mitigated by filters, that Copilot might suggest outdated coding patterns or, in rare instances, snippets that resemble publicly available code [User Query]. Furthermore, its reliance on cloud services means it requires a stable internet connection and a subscription (though it is offered free of charge to verified students, teachers, and maintainers of popular open-source projects).10 The success of GitHub Copilot in its standard pair programmer mode has been pivotal in driving broader developer adoption of AI tools and has effectively established a baseline expectation for what an AI pair programmer can offer. It significantly offloads the task of writing common code patterns. However, its inherent limitations, such as a finite context window and the potential for inaccuracies, underscore the indispensable role of human review and validation. This reliance on the developer for verification, integration, and overall architectural direction firmly places its standard operational mode at Level 2. Copilot's core function is to \"suggest whole lines or entire functions\" [User Query] in a reactive manner to developer actions. The consistent emphasis in both user reviews and official documentation on the necessity of human oversight due to potential errors or security vulnerabilities 6 reinforces its role as an assistant rather than an autonomous decision-maker. The interaction model is single-shot: Copilot provides a suggestion and then awaits the developer's next action\u2014be it acceptance, rejection, modification, or a new prompt.</p> <p>Amazon CodeWhisperer (Level 2):</p> <p>Autonomy: Level 2. Key Characteristics: Amazon CodeWhisperer is AWS\u2019s AI-powered coding companion, designed to provide real-time code recommendations based on the developer's existing code and natural language comments.11 Its suggestions can range from single lines of code to fully formed functions.11 A key distinguishing feature of CodeWhisperer is its deep optimization for Amazon Web Services (AWS). For instance, if a developer writes a comment such as \u201cupload file to S3\u201d, CodeWhisperer is likely to generate the appropriate code snippet utilizing the AWS SDK.11 In addition to code generation, it also offers security scanning capabilities to identify potential vulnerabilities in the code.11 CodeWhisperer supports a variety of programming languages including Python, Java, JavaScript, C#, Go, Rust, PHP, Kotlin, SQL, and Infrastructure as Code (IaC) languages like CloudFormation (JSON/YAML), Terraform (HCL), and AWS CDK (TypeScript, Python).2 Mode: CodeWhisperer is delivered as an IDE plugin, for example, through the AWS Toolkit for popular editors like VS Code and JetBrains IDEs.39 Code suggestions typically appear as the developer types or can be explicitly invoked after writing a comment. Users can navigate through the offered suggestions using arrow keys and accept a chosen suggestion, often with the Tab key.11 Use Cases: It excels in scenarios involving cloud-related code, particularly for AWS operations such as creating S3 buckets, defining DynamoDB tables, or handling AWS authentication.11 It is also used for general code generation in its supported languages, for example, generating a function to convert JSON to CSV based on a descriptive comment [User Query]. Pros: CodeWhisperer can generate \u201centire functions and logical code blocks at a time\u201d [User Query]. Its strong understanding of AWS APIs means it often automatically includes necessary import statements and SDK calls for AWS-specific tasks.11 The integrated security scanning feature is a valuable addition for detecting potential vulnerabilities early in the development process.11 It is also available free of charge for individual developers.47 Cons: As with other Level 2 tools, the output from CodeWhisperer requires careful review, as it may be incorrect or suboptimal.47 Its pronounced AWS bias makes it less effective for projects that are not hosted on or interacting with the AWS ecosystem.47 Some users have found its suggestions to be more basic or conservative compared to those from competitors, and it initially supported a narrower range of languages, although this support is expanding.47 Utilizing CodeWhisperer requires an AWS account, and it may transmit code context to AWS servers, which can raise privacy and licensing considerations similar to other cloud-based AI tools [User Query]. It's noteworthy that as of May 2025, CodeWhisperer's features are being integrated into the broader Amazon Q Developer service 11, which encompasses Level 3 capabilities discussed later. CodeWhisperer's specialization in AWS APIs provides a clear advantage for developers working within that particular ecosystem. This focus demonstrates a strategic approach for Level 2 tools: tailoring AI assistance to specific domains to offer more contextually relevant and immediately useful suggestions, even if their general-purpose coding capabilities might be perceived as less broad than some competitors. The ability to generate code for specific AWS SDK calls directly from natural language comments, such as \"upload file to S3\" [User Query], is a significant productivity enhancer for AWS developers, reducing their reliance on extensive documentation for common tasks. This domain-specific knowledge is a primary differentiator. However, this strength inherently leads to the limitation of an AWS-centric bias.47 Like GitHub Copilot, CodeWhisperer operates on a prompt-and-generate model, where the developer is responsible for validating the output. The ongoing integration of CodeWhisperer into Amazon Q Developer 11 signals Amazon's strategic intent to build more comprehensive, and potentially higher-autonomy, AI assistants for developers.</p> <p>OpenAI Codex (API) / Successor GPT Models (Level 2 for direct API use):</p> <p>Autonomy: Level 2, when these models are accessed directly via an API for single-turn code generation tasks. Key Characteristics: OpenAI Codex was the foundational AI model, a descendant of GPT-3, renowned for its ability to \u201ctranslates natural language into code\u201d.12 It was the technology that powered the initial versions of GitHub Copilot.12 Although the original Codex API was deprecated as of March 2023 12, its core capabilities have been subsumed and significantly enhanced by newer and more powerful models in the GPT series, such as GPT-4 and particularly GPT-4.1. These advanced models are accessible through OpenAI's API and integrated into tools like ChatGPT.12 They can generate code, provide explanations, assist in debugging, and offer support for a multitude of programming languages, with Python being a notable area of strength.12 For instance, GPT-4.1 demonstrates major improvements in coding proficiency and instruction-following, with an updated knowledge cutoff of June 2024.13 Furthermore, a new AI agent named \"Codex\" (powered by the codex-1 model), available within ChatGPT for certain users, is a cloud-based software engineering assistant capable of writing features, answering questions about a codebase, fixing bugs, and proposing pull requests, all while operating within isolated cloud sandbox environments.48 However, the fundamental interaction of using the API for straightforward code generation remains a Level 2 activity. Mode: When used as an API or via a playground interface, the developer provides a prompt\u2014which could be a problem description, a piece of partial code, or a natural language instruction\u2014and the model returns a corresponding code segment.12 For example, a prompt like \u201cCreate a Python function to check if a number is prime\u201d would elicit the Python code for that function [User Query]. The newer \"Codex\" agent within ChatGPT is accessed through the application's sidebar, where users can assign coding tasks via prompts.48 Use Cases: Typical applications include single-step generation of code from detailed specifications (e.g., \"compute the moving average of an array for a given window size\" 12), answering \"how-to\" coding questions by providing illustrative code examples, and assisting in script writing by allowing developers to describe the desired outcome in natural language.12 These models are also used for explaining existing code and translating code between different programming languages. The new \"Codex\" agent can be tasked with more complex objectives like \"writing features\" or \"fixing bugs\".48 Pros: These models possess exceptionally powerful language understanding and code generation capabilities across a wide array of programming languages.12 Newer iterations like GPT-4.1 exhibit substantial improvements in coding benchmarks (such as SWE-bench) and in their ability to follow instructions precisely.13 They can generate not only functional code but also accompanying explanations. The new \"Codex\" agent has the ability to read and edit files, and run commands such as tests and linters.48 Cons: Direct utilization of the API necessitates skill in prompt engineering and often requires iterative refinement of prompts by the developer to achieve desired results [User Query]. The generated outputs invariably require human review for correctness, security, and optimality. All LLMs have context length limitations, which can impede their ability to grasp the nuances of very large codebases unless the context is carefully managed, although these limits are continually increasing (e.g., GPT-4.1 supports up to 1 million tokens 13). The original Codex API is no longer available; access to these capabilities is now through newer, often more general-purpose models or via specific product integrations like ChatGPT.12 API usage also incurs costs. The trajectory from the original Codex to the more potent GPT-4 series and the introduction of the agentic \"Codex\" within ChatGPT illustrates the rapid advancement in foundational models tailored for code generation. While the fundamental interaction for a single code generation task via API remains at Level 2, the escalating capabilities\u2014such as file editing and command execution seen in the new \"Codex\" agent\u2014signal a clear push towards more agentic behaviors. The original Codex was a prime example of Level 2 functionality: translating natural language into code in a single pass [User Query]. Its deprecation and replacement by more advanced GPT models 12 reflect the swift pace of LLM development. The new \"Codex\" agent 48, with its abilities to \"read and edit files, as well as run commands including test harnesses, linters, and type checkers\" 48, approaches Level 3 capabilities for that specific implementation. However, if a developer merely uses the OpenAI API to send a prompt like \"write a Python function for X\" and receives code in return, that interaction itself is a single-step, human-reviewed generation, characteristic of Level 2. The underlying model may be capable of more complex operations, but the specific usage pattern determines the realized level of autonomy in that instance.</p> <p>Replit Ghostwriter (Level 2):</p> <p>Autonomy: Level 2. Key Characteristics: Replit Ghostwriter is the AI developer tool integrated into Replit's online Integrated Development Environment (IDE).49 It offers a suite of AI-powered features including code completions, chat-based assistance via \"Ghostwriter Chat,\" code transformation capabilities, and code explanation functionalities.49 Ghostwriter can suggest single lines of code or entire blocks based on comments or the surrounding code context.49 While optimized for JavaScript and Python, it also supports over 14 other programming languages.51 A key feature of Ghostwriter Chat is its awareness of the project's file context.52 Mode: Ghostwriter provides inline autocomplete suggestions (\"Complete Code\") and features a chat sidebar (\"Ghostwriter Chat\") directly within Replit's browser-based IDE or its desktop application.50 Notably, Ghostwriter Chat can proactively offer suggested solutions when errors are detected in the console output.52 Use Cases: It is frequently used for rapidly generating code while working on projects within the Replit environment, such as autocompleting a function in a Python project [User Query]. Developers can ask Ghostwriter Chat questions like \"How do I implement a linked list in Java?\" and receive code examples and explanations [User Query]. The proactive debugger in the chat can assist in identifying and fixing errors 52, and the tool can also be used to explain existing code snippets.50 Pros: The tight integration with the Replit platform allows for immediate execution and testing of generated code.50 Ghostwriter supports multi-line and function completions, and can refactor or fix code when prompted.52 It serves as a valuable tool for learning from examples and accelerating the development of small projects.50 The project context awareness of Ghostwriter Chat leads to more relevant answers compared to non-integrated AI chat tools.52 The proactive debugger is a unique and beneficial feature.52 Cons: Ghostwriter's full feature set is primarily available within the Replit ecosystem, although a VS Code extension is reportedly in preview.51 The quality of its suggestions is comparable to other LLM-powered tools, meaning it can produce incorrect or non-optimal code that requires careful review [User Query]. Access to the advanced AI features of Ghostwriter is typically not included in Replit's free tier and necessitates a paid subscription, such as Replit Core.49 As a cloud-based tool, it is dependent on an internet connection and may not be ideal for very large codebases due to potential cloud-related limitations.51 Replit Ghostwriter's strength at Level 2 stems from its deep integration within a specific development platform\u2014the Replit online IDE. This provides a cohesive AI-assisted coding experience tailored for that environment, which is particularly beneficial for educational purposes and rapid prototyping. Ghostwriter's core functionality aligns with Level 2: it \"suggest[s] single lines or entire blocks of code based on comments or context\" 49 and offers chat-based assistance for Q&amp;A and explanations 52, with the developer initiating these interactions. The \"proactive debugger\" 52, while an interesting feature that initiates contact, is still reacting to a specific event (an error) and offering a single-step solution for the developer to apply; it does not autonomously iterate or manage complex multi-step debugging scenarios without further interaction. The tight coupling with Replit's environment 50 is both a strength, due to ease of use and instant execution, and a limitation, due to platform lock-in.</p> <p>(Level 2 tools provide significant assistance, but the human developer must prompt each action and integrate the results. They do not autonomously refactor code or verify its correctness beyond what they generate in a single operation. The subsequent section explores tools that exhibit truly agentic behavior, capable of taking initiative in multi-step tasks.)Level 3 \u2013 Context-Aware Coding Agents (Supervised Automation)Level 3 tools represent a significant advancement in AI-assisted development, where the AI transitions into an agent capable of autonomously performing multiple steps to accomplish a developer-defined goal, all while under high-level human supervision. Unlike Level 2 tools that output a single code snippet and halt, a Level 3 coding agent can plan a sequence of actions. This may involve reading and modifying several files, compiling or running the project, observing the outcomes (such as test failures), and iteratively adjusting its code to meet the objective. Crucially, at this level, the human is no longer required to prompt every single code modification. Instead, the developer provides one high-level instruction, and the agent undertakes numerous sub-tasks. However, the human typically remains \"in the loop,\" at least to review the final changes or to provide clarification if the agent encounters ambiguity during its process. This operational model is analogous to a capable junior developer who can be tasked with, for example, \"Please add this feature,\" and will then work through the implementation, ask questions if necessary, and finally present a code update for review.Characteristics: These tools are distinguished by their ability to maintain state and memory related to the codebase they are working on. They frequently integrate with the developer's environment (IDE or CLI) to directly apply code edits, execute tests, interact with version control systems, and perform other development-related commands. They possess components for both planning (deconstructing complex tasks into manageable steps) and execution (writing code, running commands). The interaction style is often conversational, where the developer instructs the agent via chat or command, and the agent may respond with questions or confirmations. Alternatively, interaction can be declarative, where the developer issues a \"task command,\" and the agent executes it, reporting back upon completion. The developer's role evolves into that of a product manager or reviewer, focusing on specifying goals and constraints rather than micro-managing implementation details.A growing number of cutting-edge AI developer tools available in 2024\u20132025 fall into this Level 3 category:</p> <p>GitHub Copilot (Agent Mode) (Level 3):</p> <p>Autonomy: Level 3. (It should be noted that \"Copilot X\" was an earlier umbrella term for GitHub's next-generation AI features; \"Agent Mode\" and \"Copilot coding agent\" are the more current and specific terms for these advanced, autonomous capabilities 14). Key Characteristics: GitHub Copilot's Agent Mode, now available in VS Code Stable after a preview period 14, functions as an \"autonomous peer programmer\".14 It is designed to execute multi-step coding tasks based on a developer's command. This includes analyzing the existing codebase, identifying and reading relevant files, proposing edits across multiple files, and running terminal commands such as compiling code, installing necessary packages, and executing tests. A key aspect of its operation is its ability to monitor the correctness of its actions, respond to compile errors or test failures, and auto-correct its approach in an iterative loop until the assigned task is successfully completed.14 This agent can be tasked with creating applications from scratch, performing complex multi-file refactorings, writing and running tests, and migrating legacy code to modern frameworks.14 The agent autonomously determines the relevant context and files required for an edit.14 Complementing this, GitHub also offers a \"Copilot coding agent\" (currently in public preview) which can be assigned a GitHub issue directly. This agent will then work autonomously to implement the required changes and subsequently create a pull request for human review.44 Mode: Interaction is typically initiated via the Copilot Chat interface within the IDE (e.g., VS Code) or by assigning a GitHub issue to the dedicated coding agent.14 The developer provides a high-level instruction, such as \"Add a new API endpoint for user profile,\" or assigns an issue. The agent then autonomously plans and executes the necessary changes, potentially seeking permission for significant steps or iterating its solution based on test failures or other feedback.14 Use Cases: Agent Mode is well-suited for multi-file refactoring, implementing features that span multiple files or layers of an application stack (e.g., creating new applications, migrating legacy code), fixing bugs that involve changes in several code locations, generating documentation, and integrating new libraries.14 An example includes modernizing legacy Python scripts by transforming basic matplotlib histograms into sophisticated, SVG-based animated line charts with minimal guidance from the developer.53 Pros: This mode offers a significant productivity boost for complex or repetitive multi-step tasks by automating much of the \"inner development flow\".14 It capably handles multi-file edits, command execution, and iterative error remediation.14 Developers are kept in control through an iterative UI and requirements for approval before terminal commands are executed.14 The agent can be guided with explicit context references (e.g., using #file) and custom instructions to adhere to specific coding guidelines.14 Transparency is provided through the UI display of tool invocations.14 Cons: As a relatively new set of features (Agent Mode reached VS Code Stable in early 2025, and the issue-solving Coding Agent is still in public preview 14), its stability and performance may continue to evolve. Its effectiveness can vary, particularly in very large or architecturally complex repositories.14 There is a possibility that it may misinterpret high-level requests or produce unexpected changes, necessitating careful review of the final outputs or pull requests [User Query]. The quality of its work is dependent on the underlying LLMs (references mention GPT-4o and Claude 3.7 for different Copilot features 53). Some reviewers have noted that, in its current state, it might not match the full autonomy of more specialized tools like Roo Code.54 The evolution of GitHub Copilot to include an Agent Mode represents a significant strategic move by a leading provider of Level 2 AI tools into the realm of higher-autonomy (Level 3) capabilities. This development directly addresses the inherent limitations of single-step code generation by incorporating sophisticated planning, tool utilization, and iterative execution. The dual approach\u2014offering an interactive Agent Mode within the IDE alongside an asynchronous, issue-solving Coding Agent\u2014caters to diverse developer workflow preferences. The description of Copilot Agent Mode explicitly details an operational \"loop\" that involves editing code, running commands and tests, monitoring the output, and remediating any issues identified.14 This iterative, multi-step process, driven by a high-level goal, is the hallmark of Level 3 autonomy. Furthermore, the capability of the \"Coding Agent\" to take a GitHub issue as input and autonomously work towards producing a pull request 44 is a clear example of supervised automation, where the AI manages the intermediate steps of a larger task. The consistent emphasis on keeping the developer \"in control\" 14 and the necessity of reviewing the agent's work align with the \"supervised\" nature of Level 3.</p> <p>Cursor (AI Code Editor with Agent Mode) (Level 3):</p> <p>Autonomy: Level 3. Key Characteristics: Cursor is a comprehensive AI-powered code editor, developed as a fork of Visual Studio Code, and features deeply integrated AI capabilities, most notably its \"Agent Mode\".55 In Agent Mode, Cursor is designed to \"handle complex coding tasks with minimal guidance.\" It achieves this by autonomously exploring the existing codebase, identifying relevant files, making necessary changes, and utilizing a suite of tools including search, file editing, and terminal command execution.15 A core feature supporting this is its ability to index the entire codebase, which involves computing embeddings for each file to enhance contextual understanding and provide more accurate answers and actions related to the codebase.15 Cursor supports various underlying Large Language Models, such as GPT-4 and Claude, allowing users some flexibility.55 It also includes an \"auto command runner\" and a feature referred to by users as \"YOLO mode\" (this is a user community term for a high-autonomy setting, not always an official term in documentation, but its functionality is described 61) where the agent can automatically execute terminal commands and attempt to fix errors based on the output, sometimes without requiring explicit approval for each step if this mode is active and configured.57 Mode: Developers interact with Cursor's AI capabilities directly within the editor environment. High-level instructions can be provided via a chat pane or through the command palette (e.g., instructing the agent to \"refactor the authentication logic to use tokens instead of sessions\").62 The Agent then autonomously plans and executes the required changes. It may ask for confirmation before performing critical actions, unless a more automatic mode like auto-run or \"YOLO mode\" is enabled.57 Use Cases: Cursor is well-suited for large-scale refactoring tasks (e.g., \"Rename userId to userID everywhere\" 56). It can implement features that touch multiple files or layers of an application (e.g., \"Add a dark mode toggle to my React application\" 15). It also excels at codebase-wide find-and-fix operations and can answer complex questions about the codebase by leveraging its indexed knowledge (using the \"@Codebase\" command 62). Developers can also generate new code from scratch using shortcuts like Ctrl+K.62 Debugging assistance is also a key use case, where pasting a stack trace can lead to targeted fixes.56 Pros: The deep integration of AI within a familiar VS Code-like IDE provides a natural and intuitive developer experience, with continued compatibility for most VS Code extensions.55 The powerful codebase indexing feature allows for global context understanding, which can overcome the token limits faced by many other AI tools when dealing with large projects.15 The Agent Mode can effectively handle complex multi-file operations and refactorings.15 A \"Loops on Errors\" feature attempts to iteratively fix bugs until tests pass, particularly when \"YOLO mode\" is active and a test suite is properly configured.55 The editor also includes a checkpoint system, allowing developers to revert changes made by the agent if necessary, which adds a layer of safety.15 Cons: As a standalone editor (albeit a VS Code fork), it might lack some niche customizations or plugins that developers have in their primary IDE setup, though VS Code compatibility is generally high [User Query]. The \"YOLO mode\" or fully automatic command execution carries inherent risks if the AI misinterprets the task, potentially leading to unintended or destructive changes; thus, it requires careful handling.57 The performance and quality of suggestions are dependent on the chosen underlying LLM, and API key costs can be a factor for users.55 Some users have reported occasional UI clutter and instances of inconsistent AI behavior, ranging from \"brilliant to baffling\".56 The agent can also \"overreach\" or produce undesirable results if given vague prompts.56 Advanced features may have a steeper learning curve.56 Cursor's strategic approach of forking a widely adopted IDE like VS Code and deeply embedding Level 3 agentic AI capabilities, particularly its full codebase indexing and autonomous tool use, represents a significant effort to create a truly \"AI-native\" development environment rather than merely offering a plugin. \"Agent Mode\" in Cursor is explicitly designed for \"autonomous operation,\" \"multi-step planning,\" and \"full tool access,\" including terminal commands.15 The codebase indexing feature 58 is critical, as it provides the agent with a comprehensive understanding of the entire project's structure and dependencies.15 This deep context is essential for effectively performing complex, multi-file tasks and refactorings, which is often a challenge for simpler AI plugins. The described workflow\u2014Understand Request \u2192 Explore \u2192 Plan \u2192 Execute \u2192 Verify \u2192 Task Complete 15\u2014is a clear iterative, agentic loop. The \"YOLO mode,\" as described by users 61, where the agent can \"run tests automatically... iterating on the code until the tests pass,\" is a prime example of supervised automation where the AI handles the iterative fixing loop under high-level directives. This holistic approach aims to make AI a central, rather than peripheral, component of the coding workflow.</p> <p>Windsurf (formerly Codeium) (Level 3):</p> <p>Autonomy: Level 3. Key Characteristics: Windsurf is marketed as an \"AI-native IDE\" and an \"agentic code editor\" 63, having evolved from Codeium's earlier autocomplete tool. Its flagship agentic feature is Cascade, an \"AI Flow\" engine. Cascade enables the AI to generate or modify code, then seek user approval before running it (often in an integrated terminal), observe the results, and subsequently prompt for follow-up actions or make further iterative changes based on the outcome.16 Windsurf is designed to understand the entire project context and can automatically make code changes, run and debug code, and retry operations upon failure.63 It supports advanced features such as image-to-code conversion, web search integration for contextual information, and integration with Model Context Protocol (MCP) servers for extended tool use.16 Mode: Windsurf operates as a custom IDE, which is VS Code-based, with AI capabilities built directly into it [User Query]. Developers primarily interact with the Cascade agent. This can be done in \"Write Mode,\" which functions similarly to AutoGPT by creating files, running scripts, executing tests, and debugging, with user approval checkpoints throughout the process.16 Alternatively, \"Chat Mode\" provides contextual Q&amp;A, code generation, and instructions, requiring more manual execution by the user.16 Inline edits (triggered by Ctrl+I) and terminal chat are also supported interaction methods.16 A \"Turbo mode\" can be enabled to allow Cascade to auto-execute terminal commands.63 Use Cases: Windsurf is suited for complex debugging scenarios where the agent can run code, analyze stack traces, propose and apply fixes, and then re-run the code. It's also useful for test-driven development, as it can run tests and autonomously fix failing ones. Implementing features that require modifications across multiple parts of a project is another key use case [User Query]. Users have also reported using it for migrating projects (e.g., from React to Flutter), performing code reviews, and enhancing legacy projects.66 Pros: Windsurf offers powerful automation through Cascade's iterative plan-execute-debug loop. In \"Write Mode,\" it is claimed to automate approximately 90% of the code generation and debugging process.16 The agent has real-time awareness of user actions within the IDE.65 It can automatically detect and fix linting errors that it generates.65 The platform supports a wide range of AI models and can incorporate external context from web pages and documentation.16 Features like \"Tab to Jump\" and \"Supercomplete\" provide an enhanced code completion experience.67 Cons: As a newer, all-in-one IDE, it requires users to switch from their existing development environments [User Query]. While the agent is designed to \"think 10 steps ahead\" 63, it can still produce errors or deviate from the intended path, necessitating human validation and correction.68 Some user reviews for Codeium (its predecessor) mentioned occasional unreliability or issues with customer support.68 A current limitation of Cascade is that its \"reverts are currently irreversible\" 65, which demands caution from the user. Some advanced features are restricted to paid or enterprise plans.65 Windsurf's Cascade feature, with its explicit workflow of \"generate/modify code \u2192 seek approval \u2192 run code \u2192 observe results \u2192 iterate,\" clearly operationalizes the Level 3 agentic paradigm. This system emphasizes a structured, human-supervised process for tackling complex development tasks. The \"Write Mode,\" which functions like AutoGPT to \"create multiple files for you, run scripts, test them, and debug them\" 16 with checkpoints for user approval, is highly agentic. Features such as \"Lint Fixing,\" where Cascade \"will automatically detect and fix lint errors that it generates\" 63, further demonstrate its multi-step problem-solving capabilities. This structured approach, while powerful, also highlights the \"supervised\" aspect of Level 3, as human approval often serves as a key checkpoint, distinguishing it from the more hands-off nature of Level 4 autonomy.</p> <p>Anthropic Claude Code (Level 3):</p> <p>Autonomy: Level 3. Key Characteristics: Claude Code is an agentic coding tool developed by Anthropic. It typically runs in the terminal and leverages Anthropic's Claude models (such as Claude Opus 4 and Sonnet 4), which are known for their large context windows and strong conversational reasoning abilities.19 Key capabilities include understanding the entire codebase (autonomously exploring files as needed without requiring manual file feeding to the context), editing multiple files, fixing bugs, answering complex questions about code architecture and logic, executing tests and linters, searching through Git history, handling merge conflicts, and generating commit messages and pull requests.19 Claude Code also offers native integrations with popular IDEs like VS Code and JetBrains, where it can display proposed edits directly within the developer's files.69 Mode: The primary interaction mode is via a command-line interface (CLI), initiated with the claude command, which starts an interactive Read-Eval-Print Loop (REPL) within a project directory.19 Users interact with Claude Code conversationally, providing natural language instructions (e.g., \"Claude, refactor the user authentication logic to use OIDC\"). The agent may ask clarifying questions before proposing changes, often presented as diffs in the terminal, which the user can then choose to apply [User Query]. It can utilize a variety of \"tools\" such as FileEditTool for modifying files, BashTool for executing shell commands, and GrepTool for searching file contents.19 Use Cases: Claude Code is employed for complex codebase refactoring, fixing bugs that span multiple files, automatically generating and running tests, resolving merge conflicts, and answering in-depth questions about code (e.g., \"Why is this function recursive?\"). It is also useful for onboarding new developers to a project by explaining the existing code structure, for instance, by generating a CLAUDE.md project guide.19 Pros: Claude Code offers a very broad range of capabilities, effectively acting like an expert pair programmer that not only suggests but also applies and verifies changes [User Query]. It benefits from the strong natural language understanding and large context window of the Claude models, enabling better comprehension of complex codebases.68 The agent excels at explaining its reasoning step-by-step, which many users find helpful.72 Operating directly in the terminal allows it to fit seamlessly into existing CLI-centric workflows.19 It is designed with security and privacy as priorities, running locally and requiring explicit user approval before making any modifications to the system.19 An SDK is available for building custom agents.69 Cons: The CLI-first interface might present a learning curve for developers who are more accustomed to graphical user interfaces [User Query]. Claude Code is currently in beta as a research preview, which means access might be limited, and its features may continue to evolve.19 It utilizes paid API calls to the Claude models, which can become costly with extensive use.19 Even with a large context window, extremely large or complex projects might still pose challenges or require careful scoping of prompts [User Query]. Some user reviews for the general Claude models have noted occasional over-cautiousness or a tendency to miss context if queries are not worded with sufficient precision.72 Claude Code's strength lies in its combination of a powerful Large Language Model with direct agency within a developer's local environment. It is particularly effective for tasks that involve extensive codebase understanding and intricate Git operations. Its command-line interface (CLI) nature appeals to a specific segment of developers who prefer terminal-centric workflows and value the control and scriptability such an environment offers. The ability to \"edit files and fixing bugs across your codebase,\" \"executing and fixing tests,\" and \"searching through git history, resolving merge conflicts, and creating commits and PRs\" 19\u2014all based on high-level natural language instruction\u2014clearly positions Claude Code at Level 3. Its typical workflow, often described as \"Explore, plan, code, commit\" 70, is inherently multi-step and agentic. The consistent requirement for explicit user approval before any modifications are made 70 ensures that the automation remains supervised, a key characteristic of this autonomy level.</p> <p>ChatGPT (with Plugins and Advanced Data Analysis/Code Interpreter) (Level 3):</p> <p>Autonomy: Level 3, when specifically utilized with its Code Interpreter (now often referred to as Advanced Data Analysis) feature or with relevant plugins that enable multi-step, tool-using workflows for coding tasks. Key Characteristics: OpenAI\u2019s ChatGPT, particularly versions powered by GPT-4 and newer models like GPT-4.1 74, is a highly capable conversational AI. Its utility for coding is significantly amplified by:</p> <p>Code Interpreter/Advanced Data Analysis: This feature provides ChatGPT with a sandboxed Python execution environment. Within this sandbox, ChatGPT can write and run Python code, upload and download files, perform data analysis, create visualizations, and iteratively debug its own code based on execution errors.74 Plugins (and increasingly, custom GPTs/connectors): These extensions allow ChatGPT to interact with a wide array of external tools and services. For coding, this can include plugins for web browsing (e.g., WebPilot 77) to find solutions or documentation, or integrations for interacting with version control systems like GitHub or GitLab (e.g., via Zapier or dedicated connectors 78).</p> <p>Mode: Interaction occurs through a conversational interface, typically a web UI or via API. When using the Code Interpreter, users can upload files and instruct ChatGPT to perform analysis or generate and execute code. When using plugins, users first enable the specific plugins they need and then prompt ChatGPT to utilize them for a given task (e.g., \"Search the web for a Python library to parse XML, then write a sample script using it and test it with this example XML data\"). Use Cases: Performing data analysis and generating visualizations using Code Interpreter (e.g., analyzing data from CSV files, creating charts, converting file formats).75 Prototyping and testing Python code snippets within the sandboxed environment, allowing for iterative refinement.75 Utilizing web browsing plugins to find current documentation, solutions to coding problems, or information about new libraries.77 Interacting with GitHub or GitLab via plugins or connectors for tasks such as summarizing pull requests, creating issues, or potentially (with sufficient permissions and robust plugin capabilities) committing code.78 Pros: ChatGPT offers enormous flexibility due to its broad general knowledge and the extensibility provided by plugins and the Code Interpreter feature.81 It possesses strong natural language understanding and reasoning capabilities, especially with models like GPT-4 and GPT-4.1.74 The Code Interpreter enables a powerful iterative loop of code generation, execution, and debugging directly within the chat interface.75 Plugins allow access to real-time information from the web and integration with various external services, helping to overcome the knowledge cutoff limitations of the base LLM.77 Cons: ChatGPT is not a dedicated IDE tool, so integrating its outputs into a development workflow often involves manual copy-pasting of code, unless specific plugins or integrations can bridge this gap [User Query]. The reliability and capabilities of third-party plugins can vary significantly, and managing multiple plugins for a complex task can be cumbersome [User Query]. Context length limitations, although substantial for models like GPT-4, can still be a constraint when attempting to reason about an entire large codebase [User Query]. Access to advanced models (like GPT-4), Code Interpreter, and plugins typically requires a ChatGPT Plus subscription.82 There are also security and trust considerations when granting plugins permission to execute actions or access external data [User Query]. Responses from ChatGPT can sometimes be verbose or require further refinement to be directly usable [47 (a general con of ChatGPT)]. ChatGPT, when augmented with its Code Interpreter and relevant plugins, transforms from a Level 2 code generator into a versatile Level 3 agent for specific types of development tasks. This transformation is primarily due to its acquired ability to perform actions, use tools, and iterate based on execution results or newly retrieved information. The Code Interpreter's capacity to \"write code, run it in a Python environment, and return the results,\" and critically, \"in case of code failure...debugs the code by reading the callback messages...[entering] an automatic loop to fix errors\" 75, is a clear demonstration of a Level 3 iterative execution loop. When this is combined with plugins that allow it to, for instance, \"interact with your Git repositories directly\" 78, it can undertake multi-step, tool-assisted tasks based on a high-level developer request. The developer supervises this entire process through the conversational chat interface, guiding the AI and validating its outputs or actions. This highlights the significant role of \"tool use\" in elevating the autonomy and practical capabilities of LLMs.</p> <p>Aider (CLI Chat &amp; Git Integration) (Level 3):</p> <p>Autonomy: Level 3. Key Characteristics: Aider is an open-source AI pair programming tool specifically designed for use in the terminal, and it features deep integration with Git repositories.17 It employs various Large Language Models (LLMs)\u2014such as OpenAI's GPT-4/GPT-4o, Anthropic's Claude 3.7 Sonnet, DeepSeek R1, or even locally hosted models\u2014to understand and act upon natural language instructions to edit code within a local Git repository.17 Key distinguishing features include its capability for multi-file editing based on a single prompt, mapping the entire codebase to provide better context for its operations in larger projects, automatically committing changes with sensible, AI-generated messages, and an automated system for running linters and tests after each change, with an ability to attempt auto-fixing any reported issues.17 It also supports adding images and web pages to the chat to provide visual context for coding tasks.17 Mode: Developers typically run the aider command in their terminal from within a Git repository and then interact with it via a chat interface [User Query]. Instructions such as \"Implement a function for X feature in these files\" will prompt Aider to modify the specified files. It will then usually present a diff of the proposed changes for the developer's approval before committing them.18 Aider can also be configured to \"watch\" files and respond to specially formatted AI comments added by the developer in their preferred IDE.17 Use Cases: Aider is well-suited for rapid development and iteration on existing codebases. Common tasks include adding new features that span multiple files, fixing bugs that have been described in natural language, and refactoring code across various parts of a project (e.g., \"Clean up all API routes to use the new authentication middleware\").18 It can also be used for generating documentation for existing code or explaining complex code segments.83 Pros: Aider offers strong automation capabilities, particularly due to its excellent Git integration, which includes auto-commits with meaningful messages. This provides a clear audit trail of AI-generated changes and allows for easy rollbacks if needed.17 The automated linting and testing cycle, coupled with its auto-fix attempts, acts as a valuable safety net for AI-driven modifications.17 The ability to use both cloud-based and local LLMs gives users flexibility regarding cost and data privacy.17 Being open-source, Aider is extensible and benefits from community contributions.18 It is generally considered lightweight and fast in its operation.18 Many users and reviewers have praised its effectiveness for multi-file changes.84 Cons: Its primary interface is command-line based, which may not appeal to all developers, particularly those less comfortable with terminal environments, and it is generally not considered beginner-friendly.18 Initial setup requires configuring API keys for the chosen LLMs and may involve some tuning of models or parameters.18 The effectiveness of Aider's suggestions and modifications is dependent on the clarity of the instructions provided and the capabilities of the selected LLM [User Query]. Furthermore, the success of its auto-fixing feature is contingent on the comprehensiveness and quality of the project's existing linters and test suites [User Query]. Aider's deep integration with Git and its \"commit-centric\" workflow for managing AI-generated code changes establish a pragmatic approach to Level 3 autonomy. This methodology emphasizes traceability and provides developers with robust control via familiar tools like Git diffs and commits. The automated testing loop further enhances safety. Aider's capability to \"edit multiple files at once through a single AI prompt\" 18, \"automatically commits changes with sensible commit messages,\" and \"automatically lint and test your code every time aider makes changes...[and] fix problems detected\" 17 clearly demonstrates a multi-step, iterative workflow that operates under developer supervision (through the approval of changes). This operational model extends well beyond the single-shot generation characteristic of Level 2 tools. The tight coupling with established developer practices like version control and automated testing likely fosters greater trust and facilitates adoption, especially for a CLI-based agent, as the AI's actions are rendered transparent and manageable within existing, well-understood workflows.</p> <p>Amazon Q Developer CLI (formerly incorporating CodeWhisperer features) (Level 3):</p> <p>Autonomy: Level 3. Key Characteristics: The Amazon Q Developer CLI is an enhanced command-line interface agent that represents an evolution from earlier CodeWhisperer CLI functionalities, now incorporating more sophisticated agentic features.85 It is designed to provide a dynamic and interactive coding experience directly within the terminal. This agent leverages powerful foundation models, such as Anthropic's Claude 3.7 Sonnet, accessed via Amazon Bedrock.85 The Q CLI agent is capable of using information from the developer's CLI environment to read and write files locally, query AWS resources, write new code, assist with testing and debugging, and iteratively make adjustments to code based on user feedback and explicit approval.85 A significant aspect of its capability is its ability to make use of tools already installed on the user's system, including compilers, package managers, and the AWS Command Line Interface (AWS CLI).86 Mode: Interaction occurs within an interactive CLI session. Developers engage with Amazon Q Developer using natural language commands and queries (e.g., \"Create a new Lambda function in this project that processes S3 events\").86 Based on these instructions, the agent can execute necessary shell commands, install dependencies, modify code files, and even deploy resources to AWS. Throughout this process, it may ask for clarification or provide updates on its progress.86 Use Cases: The Q CLI agent is particularly well-suited for cloud development tasks, such as scaffolding new projects, setting up AWS resources (e.g., finding an existing DynamoDB table, installing the relevant SDK using npm, and then updating application files to utilize that table 86), and managing builds or deployments. It can also handle general local development tasks, like initiating a build process, then running automated tests, parsing the test output, and subsequently attempting to fix any reported errors in an agentic loop [User Query]. Pros: This tool helps reduce context switching by enabling developers to remain within their terminal environment while leveraging AI assistance.86 It is designed to complete more tasks autonomously, moving beyond simply providing hints, by employing step-by-step reasoning.85 It features deep integration with AWS services and related development tools.86 The use of powerful underlying models like Claude contributes to its strong natural language understanding and contextual awareness.85 Cons: As a relatively newer tool, its full range of capabilities and robustness across all scenarios are still being established. Early information suggested some OS limitations (e.g., macOS or Linux only for certain Q CLI features, though this is subject to change) [User Query]. The ability to execute commands automatically on the developer's system inherently raises trust and safety considerations, requiring users to exercise caution [User Query]. It primarily benefits developers working within the AWS ecosystem [User Query]. Setup may require an AWS account and specific configurations (e.g., AWS Builder ID) [User Query]. Some user reviews for the broader Amazon Q Developer service (which includes IDE extensions in addition to the CLI) have mentioned instances of misunderstood prompts or the generation of buggy code, and the pricing for Pro tiers can be a concern for some users.87 The Amazon Q Developer CLI's ability to leverage existing tools on the developer's local system\u2014such as compilers, package managers, and the AWS CLI\u2014through natural language commands represents a potent Level 3 agentic pattern. This effectively transforms the AI into an intelligent orchestrator of the developer's own established toolkit. The agent \"can make use of tools installed on my system including compilers, package managers, and the AWS CLI\".86 This implies that developers do not need to wait for Amazon to build specific, bespoke integrations for every conceivable build system or utility; the AI can utilize what is already present and configured in their environment. For instance, if a developer instructs it to \"build the project and run tests,\" the Q CLI agent can invoke the project's specific make command or npm test script, parse the output generated by these local tools, and then attempt to fix errors based on that output, all by orchestrating the local environment. This \"bring your own tools\" approach significantly expands the agent's practical capabilities and its adaptability to diverse development environments, which is a key characteristic of effective Level 3 automation.</p> <p>Other Level 3 Examples:</p> <p>Cline (Roo): Cline is an open-source autonomous coding assistant designed as a VS Code extension. It uniquely features distinct \"Plan\" and \"Act\" modes, allowing the agent to first devise a strategic plan (a sequence of steps to implement a request) and then execute those steps, modifying code accordingly. Cline can read the entire project, search within files, and perform terminal commands. It connects to various LLMs via API, offering flexibility.84 The tool emphasizes a \"thoughtful collaborator\" approach, not only performing actions but also explaining its steps and suggesting best practices. A significant feature is its integration of the Model Context Protocol (MCP), which enables it to interact with external tools and services, such as running tests, managing Git operations, updating documentation, and even connecting with project management tools.88 This broadens its scope beyond mere code editing to encompass a wider range of development lifecycle tasks. The explicit separation of \"Plan\" and \"Act\" modes allows for developer review of the proposed strategy before execution, fitting the \"supervised automation\" paradigm of Level 3. MCP integration, in particular, significantly enhances its agentic capabilities by allowing it to orchestrate diverse, multi-step tasks.88 QodoAI (formerly CodiumAI): QodoAI positions itself as a quality-first AI coding platform. Its agentic capabilities are primarily focused on testing and code analysis. Key features include \"Qodo Gen,\" an IDE plugin for generating high-quality code and meaningful tests, and \"Qodo Merge,\" a Git agent designed to facilitate smoother pull requests by generating thorough descriptions and providing reviewers with comprehensive walkthroughs to quickly identify potential issues.89 The platform operates on the principle of \"Agentic AI for Continuous Quality,\" aiming to ensure code integrity throughout the development pipeline by automating code reviews and generating comprehensive test suites.89 Users report that it can generate detailed unit tests, sometimes identifying edge cases that human developers might have overlooked.90 QodoAI applies Level 3 agentic principles specifically to the domain of code quality and testing, effectively acting as an autonomous QA assistant that can analyze code, generate appropriate tests, and facilitate the review process. Its functions, such as automating test generation based on code analysis and creating detailed PR descriptions 89, are multi-step processes that assist the developer at a higher level than simple code generation.</p> <p>Level 4 \u2013 Highly Autonomous Coding AgentsLevel 4 tools represent a significant leap towards fully autonomous coding on complex tasks. These agents typically require only minimal human input, often just an initial prompt or a specification document, and are designed to operate with potentially no human review required before deployment (though in current practice, a final review is still common and advisable). Agents at this level can take on a project goal, such as \u201cbuild me an application that performs X,\u201d or address a very complex bug, and are expected to deliver a completed solution with little to no incremental guidance from the developer. They achieve this through advanced planning capabilities, long-term memory of the project's context, and the ability to integrate and utilize various external tools and information sources. While Level 3 agents still operate under fairly close human supervision for their iterative cycles, Level 4 agents aspire to produce production-ready code that could theoretically be trusted after automated validation processes.As of 2025, Level 4 autonomy is at the cutting edge of AI development, with only a few emerging examples, often proprietary or in limited beta stages, that approach this level of capability. These agents aim to function like seasoned software developers who can be given a broad problem statement and will autonomously clarify requirements (to some extent), write the necessary code across the entire codebase, thoroughly test their work, debug any issues, and ultimately hand over a finished product.Key players and experimental systems in this space include:</p> <p>Google Jules (Coding Agent) (Level 4):</p> <p>Autonomy: Level 4. Key Characteristics: Jules is Google's autonomous coding agent, which entered public beta in May 2025. It is explicitly positioned by Google as \"not a co-pilot, not a code-completion sidekick, but an autonomous agent that reads your code, understands your intent, and gets to work\".20 Jules operates asynchronously in a secure Google Cloud Virtual Machine (VM). It begins by cloning the target codebase into this VM, allowing it to understand the full project context. It can then perform a range of tasks such as writing tests, building new features, fixing bugs, and managing dependency version updates.20 Jules is powered by Google's Gemini 2.5 Pro model, providing it with advanced coding and reasoning capabilities.20 Upon task completion, Jules presents its plan, the reasoning behind its actions, and a diff of the changes made. It also allows for user steerability, meaning developers can modify the proposed plan before, during, and after execution.20 Unique features include direct GitHub integration for a seamless workflow and \"audio changelogs\" that provide narrated summaries of recent commits.20 Mode: Jules is accessed via a web interface (Google Labs) or through the Gemini application, and it integrates directly with GitHub.20 Developers assign tasks (e.g., \"Jules, add pagination to the blog posts list\"). Jules then works on these tasks in the background, eventually returning the results, often in the form of a pull request on GitHub.20 Use Cases: Jules is designed for implementing large features, addressing major bug fixes, writing extensive suites of unit tests, and autonomously upgrading library versions while also resolving any resulting compatibility issues.20 It can also be used to tackle a backlog of bugs or to take an initial pass at building out new features.92 Pros: The agent is highly hands-off, allowing developers to delegate significant coding tasks and focus on other priorities while Jules works autonomously.20 It leverages the powerful Gemini 2.5 Pro model, ensuring access to advanced coding and reasoning capabilities.20 Its integration with GitHub fits naturally into existing developer workflows.20 The asynchronous mode of operation is particularly well-suited for time-consuming tasks.91 Jules provides a visible workflow by presenting its plans and reasoning, and the user steerability feature offers a degree of control over the process.20 During its public beta, a free tier with usage limits is available.92 Cons: As a very new tool (public beta launched May 2025) 20, its real-world performance across a diverse range of complex scenarios is still being evaluated. Its cloud-only operation, while offering security assurances from Google (private code is not used for training and data stays isolated 20), might still raise security or privacy concerns for some organizations and inherently introduces latency [User Query]. The asynchronous nature means there is no real-time interaction during task execution, and developers must wait for Jules to complete its work [User Query]. The high degree of autonomy also means that Jules might make architectural or structural decisions that a human developer would not, necessitating careful review of its pull requests, even if the code is functionally correct [User Query]. Initial language support includes Python and JavaScript, with plans for expansion.92 Jules represents a significant endeavor by a major technology company to realize Level 4 autonomy in coding. Its asynchronous, cloud-based execution model, coupled with its objective to handle entire features or complex bug fixes autonomously, clearly targets this advanced level of operation. It aims to function like a delegated, independent engineering resource. The description of Jules as an \"autonomous agent that reads your code, understands your intent, and gets to work\" on tasks like \"building new features\" or \"fixing bugs\" with \"minimal further prompts\" 20 aligns squarely with the definition of Level 4. Its workflow\u2014assign task, Jules works independently in the cloud, Jules presents a pull request for review 20\u2014is characteristic of delegating a substantial work package to an AI, with the expectation of a near-complete solution. The ability for users to \"modify the presented plan before, during, and after execution\" 20 provides a supervisory overlay on an otherwise highly autonomous process.</p> <p>Cognition\u2019s Devin (AI Software Engineer) (Level 4):</p> <p>Autonomy: Level 4. Key Characteristics: Devin, developed by Cognition Labs, is marketed as \"the first AI software engineer\" and is designed to autonomously perform complex software development tasks from end-to-end.21 It operates within a cloud-based sandboxed environment that is equipped with its own Linux shell, code editor, and web browser, enabling it to utilize a wide range of tools much like a human developer would.21 Devin exhibits advanced capabilities in long-term reasoning and planning, capable of breaking down complex tasks into thousands of discrete decisions and steps. It also features context recall, the ability to learn from feedback over time, and mechanisms for mistake correction.21 The release of Devin 2.0 in April 2025 introduced several enterprise-focused enhancements, including interactive planning (allowing users to review, edit, or approve Devin's proposed plan before execution), Devin Search (enabling users to query their codebase and receive responses with cited code), and Devin Wiki (which automatically generates browsable documentation and architecture diagrams for the codebase).22 Mode: Devin is offered as a service, and users typically interact with it by assigning tasks or GitHub issues via a web dashboard, Slack integration, or a command-line interface.22 Once a task is assigned, Devin works autonomously on the codebase. It reports its progress and can proactively ask for human feedback or design choices if it encounters ambiguity or requires a decision that impacts the project's direction.21 Use Cases: Devin has been demonstrated on ambitious tasks such as building and deploying end-to-end applications based on a natural language description (e.g., a demo showcased Devin creating an interactive \"Game of Life\" web application from scratch and then deploying it to Netlify).22 It is also used for fixing complex bugs in existing, mature open-source codebases; Devin reportedly solved 13.86% of issues on the SWE-bench benchmark unassisted, a significant improvement over previous state-of-the-art AI models.22 Other use cases include large-scale code migrations and refactoring (as exemplified by its use in the Nubank ETL migration project 21) and even training or fine-tuning machine learning models.22 Pros: Devin demonstrates a high degree of autonomy in planning and executing complex engineering tasks that may require thousands of decisions and maintaining long-term context.22 Its ability to use common developer tools (shell, browser, editor) allows for versatile and realistic task execution, such as reading online documentation or testing web applications.21 Its performance on the SWE-bench benchmark is notably superior to that of raw LLMs like GPT-4 or Claude 2 on the same tasks.22 The system is designed to learn over time and can fix its own mistakes during task execution.22 Features in Devin 2.0, such as interactive planning and Devin Search, enhance transparency and collaboration.22 Real-world enterprise tasks, like large-scale migrations, have seen significant efficiency gains (e.g., 12x improvement reported by Nubank) attributed to Devin.21 Cons: Despite the significant hype and impressive demos, Devin's real-world success rate on very complex or novel coding work is still limited. The 13.86% success rate on SWE-bench, while a notable achievement for an AI, is low compared to human engineer performance and indicates that it is not infallible; many tasks may require multiple attempts or human intervention.23 It can struggle with highly intricate algorithmic challenges or deep architectural design problems [User Query]. Devin is an expensive tool, with initial pricing plans targeting enterprise users (e.g., figures around $500/month were mentioned in early reports, though pricing models may evolve) [User Query]. Utilizing Devin requires granting a third-party cloud service access to the codebase, which raises standard concerns about security and compliance [User Query]. Some early demonstrations have faced scrutiny regarding potential pre-configuration or the exact conditions of the demo environment.22 The system is compute-intensive, which could lead to high operational costs and potentially affect pricing or access to its full capabilities if revenue growth does not keep pace.22 Devin's approach of equipping an LLM-based agent with a comprehensive suite of standard developer tools (shell, editor, browser) within a sandboxed environment is a pivotal architectural decision that underpins its Level 4 aspirations. This setup allows the AI to mimic human developer workflows more completely than agents with more restricted capabilities. Devin has \"its own shell... its own code editor... and its own browser\".21 This enables it to perform a wide range of actions crucial for real-world software engineering, such as \"search the web for documentation, test web applications it builds, and access tools like Notion and Jira\".21 The ability to \"test its own code, fixing errors until it succeeds\" 21 within this integrated environment is fundamental to its autonomous problem-solving loop. This operational model contrasts sharply with agents that are purely API-based or possess limited toolsets, as Devin can interact with the digital world in a manner much closer to that of a human developer to gather information, execute tasks, and validate its work. Its core design is to \"autonomously complete software engineering tasks when prompted by a user\" 22, encompassing planning, coding, testing, and debugging across multiple files. This aligns with the Level 4 description of handling complex projects end-to-end with minimal human input. The SWE-bench result of 13.86% 23, while far from perfect, represents a significant achievement for unassisted task completion on real-world issues, substantially surpassing previous AI capabilities. The \"interactive planning\" feature introduced in Devin 2.0 22 still fits within the Level 4 paradigm, as it involves upfront approval of a high-level plan rather than micromanagement of each subsequent step by the AI.</p> <p>Experimental \u201cOne-Shot\u201d Coding Agents (Level 4, experimental):</p> <p>GPT-Engineer:</p> <p>Autonomy: Approaches Level 4 in its ambitious goal of \"one-shot\" codebase generation. Key Characteristics: GPT-Engineer is an open-source command-line interface (CLI) tool designed with the premise that a single, well-crafted prompt can be used to generate an entire codebase.24 The process typically involves the tool engaging in an interactive clarification dialogue with the user to refine the initial specifications. Once the requirements are sufficiently clear, GPT-Engineer autonomously generates the necessary code files to construct the desired software.24 Its core function is to convert natural language instructions directly into functioning code.24 Mode: The user initiates the process by providing an initial prompt, usually in a dedicated file (e.g., main_prompt). GPT-Engineer is then run from the CLI. Before proceeding with code generation, the tool may ask the user a series of clarifying questions to resolve ambiguities or gather more details. After this clarification phase, it generates the specified software, creating the necessary files and directory structure within a designated workspace.25 Use Cases: GPT-Engineer is primarily used for rapidly bootstrapping entire new projects, especially web applications, from a high-level conceptual idea (e.g., \"A website that shows random cat pictures and quotes\").25 It can also be employed to generate boilerplate code for common programming tasks or to create basic application structures.25 Pros: It has the potential to significantly speed up the initial phases of project setup and the generation of boilerplate code.24 Its interface can be user-friendly for both non-developers looking to quickly prototype ideas and developers seeking to accelerate initial groundwork.24 Being open-source, it is freely available and customizable, allowing users to experiment with different underlying language models.24 Cons: The quality and completeness of the generated codebase are highly dependent on the clarity and precision of the initial prompt; misinterpretation of complex or ambiguous instructions is a significant risk.24 The code generated by GPT-Engineer often requires substantial review, debugging, and manual refinement to be considered production-ready, especially for applications of moderate to high complexity.96 Some descriptions suggest its primary focus is on web application generation, which may limit its utility for other types of software development.24 It generally lacks the sophisticated iterative feedback and correction loops seen in more advanced agents during the generation process itself; its operation is more akin to a \"one-shot\" generation after an initial clarification phase [User Query].</p> <p>AutoGPT-based dev agents:</p> <p>Autonomy: Approaches Level 4 in its capacity for self-directed task execution. Key Characteristics: AutoGPT is an experimental, open-source Python program that leverages Large Language Models (like GPT-4) to autonomously achieve user-defined goals. It does this by deconstructing the main goal into a series of smaller, manageable sub-tasks, planning their execution, and utilizing a range of tools\u2014such as internet access for information gathering, file system operations, and code execution\u2014in an automatic loop that does not require continuous human prompting.98 While AutoGPT is a general-purpose autonomous agent framework, it can be specifically tasked with software development goals.98 Mode: The user defines an AI agent by providing it with a name, a role, and a set of high-level goals. Once initiated, AutoGPT autonomously generates its own internal prompts and devises a plan to fulfill these goals. This may involve writing scripts, executing them, debugging any errors encountered, and iterating on this process.99 Use Cases (Coding): AutoGPT can be used experimentally to attempt the construction of simple applications or scripts. For example, if tasked to \"create a Python script to collect weather data,\" it might autonomously plan to create the script, then attempt to fix any errors encountered during execution, and finally add comments to the working code.99 It can also be used to automate specific, well-defined parts of a development workflow if the goal is clearly articulated and achievable with its available tools. Pros: AutoGPT demonstrates a high degree of autonomy in its ability to independently plan and execute a sequence of diverse actions (such as web searches, file input/output, and code execution) in pursuit of a defined objective.98 It can operate for extended periods without direct human intervention. Its capability to access and utilize external information via web browsing is a significant asset.101 Cons: AutoGPT is often inefficient and can easily get stuck in unproductive loops or pursue incorrect paths if not carefully guided by a very precise initial goal [User Query, 100 (noting high cost per step and limited functions)]. Setting up and running AutoGPT effectively requires a degree of technical skill, including obtaining and configuring an OpenAI API key and operating it via the command line. The cost of operation can also be high due to the large number of LLM calls it typically makes.99 The quality of the code generated can be low, and it may struggle with complex logic or tasks that are not easily broken down into discrete, verifiable steps or that require nuanced understanding beyond its available tools [User Query]. It is fundamentally a general-purpose autonomous agent framework rather than a specialized coding tool, which means it may lack a refined understanding of software development best practices or complex architectural considerations. Experimental Level 4 agents like GPT-Engineer and AutoGPT highlight the community's strong aspiration for \"one-prompt-to-product\" solutions or fully autonomous task completion in software development. However, their current limitations vividly underscore the immense challenge in achieving consistently reliable, high-quality output without significant human oversight or more sophisticated built-in mechanisms for iterative refinement and verification. GPT-Engineer's goal to \"generate entire websites or apps from a single prompt\" 95 clearly reflects a Level 4 ambition. Similarly, AutoGPT aims to \"autonomously achieve user-defined goals\" by creating its own plans and prompts.98 Despite these aims, practical experience often shows that GPT-Engineer's output \"might not be usable as is\" and typically requires substantial review 96, with its success heavily reliant on the clarity of the initial prompt.24 AutoGPT, too, \"requires some technical skills to set up and use properly\" and is prone to getting stuck or becoming costly.99 This demonstrates that while the aspiration for Level 4 capabilities is evident in these tools, their current execution often falls short of producing consistently production-ready results for complex tasks without considerable human intervention, thus keeping them largely in the experimental realm for such applications.</p> <p>Level 5 \u2013 Full Autonomy and Future OutlookLevel 5 autonomy in AI coding tools represents the aspirational endgame: a state where AI agents can manage the entire software development lifecycle independently, extending to the point of autonomously deciding what software needs to be built or which existing systems require modification or improvement, without explicit human requests for each task. This is analogous to a fully self-driving car that not only navigates any environment but also determines its own destinations based on higher-level objectives. In the context of software, a Level 5 AI developer might proactively identify emerging user needs, anticipate system bottlenecks, or respond to security advisories by initiating and completing development projects. As of 2025, this level of autonomy remains largely theoretical and experimental; no current tools can reliably set their own strategic development goals in a controlled and beneficial manner.Characteristics and Theoretical Nature:Level 5 AI agents would transcend the capabilities of Level 4 by not only executing complex tasks autonomously but also initiating these tasks based on their own sophisticated analysis and understanding of various inputs, such as system performance metrics, user feedback trends, evolving market conditions, or newly discovered security vulnerabilities. This implies a capacity for proactive monitoring, opportunity identification, goal formulation, strategic planning, and complete execution of development initiatives. Achieving this would require profound understanding across multiple domains\u2014technical, user experience, business strategy\u2014and the ability to make high-stakes decisions with long-term implications. The concept mirrors that of Level 5 autonomous vehicles, which are described as \"currently theoretical for widespread use\".5Glimpses in Research &amp; Related Concepts:While no true Level 5 systems exist, several research avenues provide foundational elements and hint at future possibilities:</p> <p>Self-Improving Coding Agents:The research paper \"A Self-Improving Coding Agent (SICA)\" (arXiv:2504.15228) presents an agent system capable of autonomously editing its own codebase to enhance its performance on defined benchmarks, such as SWE-Bench Verified, where it improved its score from 17% to 53% on a subset of tasks.26 SICA achieves this through an iterative \"Meta Agent Loop,\" where it analyzes its past performance, identifies potential improvements (e.g., to its prompting schemes or tool usage), and implements these changes to its own Python code.26 This capacity for self-modification is a crucial precursor for any system that could autonomously improve application codebases. Similarly, Anthropic's research with its Claude models, including Claude Code, has explored agents that can \"extract and save key facts to maintain continuity and build tacit knowledge over time\".69 The availability of the Claude Code SDK for building custom agents 69 also facilitates experimentation with more autonomous behaviors. SICA's ability to modify its own codebase and improve benchmark scores is a concrete step towards self-improving software, a core concept for Level 5. However, it is important to note that SICA is still improving based on predefined benchmarks and human-defined goals; it does not yet set entirely new strategic goals for itself. The agent improves its efficiency and effectiveness at solving given tasks, which is a form of self-optimization. Level 5 implies a higher order of goal-setting, such as deciding which benchmarks to target or what new features would be most valuable for users, capabilities that SICA does not address.</p> <p>AI for Novel Algorithm Discovery &amp; Code Optimization:DeepMind's AlphaDev project demonstrated an AI system that discovered novel, more efficient sorting algorithms (achieving up to 70% speedup for certain C++ library use cases for short sequences and 1.7% for large ones) and hashing algorithms (up to 30% more efficient for specific byte ranges) by exploring assembly instructions from first principles, in some cases outperforming algorithms designed by humans over decades.28 These AI-discovered algorithms have since been integrated into standard C++ libraries. This showcases AI's potential not merely to write code according to specifications but to innovate at a fundamental algorithmic level, a form of proactive improvement characteristic of Level 5. AlphaDev was not just refactoring existing code; it was inventing new algorithmic approaches. This represents a higher level of \"improvement\" than simply fixing bugs. However, it operated on a very specific, human-defined goal (i.e., find a faster sorting/hashing algorithm). A true Level 5 system might autonomously decide which algorithms within a larger system require optimization based on broader system performance goals or energy efficiency targets.Concurrently, Meta AI's research into LLM Compilers\u2014using specialized variants of Code Llama pre-trained on assembly codes and compiler Intermediate Representations (IRs)\u2014aims to optimize code and streamline compilation tasks. This includes automatically tuning compiler flags for code size or disassembling compiled code back to LLVM-IR.31 Such research points towards AI systems that could autonomously improve the performance and efficiency of software at a low level, potentially as a continuous background process, aligning with the \"continuous improvement\" aspect of Level 5.</p> <p>Multi-Agent Systems &amp; Collaborative AI:Active research into LLM-based Multi-Agent Systems (MASs) is exploring how groups of AI agents can coordinate their actions and collectively solve complex problems.102 Frameworks such as MetaGPT (which mimics a software company structure with role-assigned agents) and AutoGen (which enables sophisticated conversations and collaboration between agents) are investigating how specialized AI agents could collectively manage various aspects of the software development lifecycle.103 Even general-purpose agent frameworks like AutoGPT can be structured to allow for a form of agent collaboration if tasks are designed appropriately.104 This suggests that future Level 5 systems might not be monolithic AIs but rather a collaborative ensemble of specialized AI agents that collectively manage the software lifecycle, potentially even negotiating tasks and strategic goals among themselves. Complex software development often involves diverse roles (architect, coder, tester, DevOps engineer). A Level 5 system might mirror this human team structure with specialized AI agents. The ongoing research in MASs 102 is focused on enabling such collaboration, which could be essential for handling the breadth and complexity of tasks envisioned at Level 5.</p> <p>Challenges: Trust, Safety, Alignment, and Control:The path to Level 5 autonomy is fraught with significant challenges: Trust and Reliability: Entrusting an AI with the autonomy to modify and deploy software systems without direct human oversight for each action necessitates an exceptionally high degree of trust in its reliability and correctness. The current performance of even emerging Level 4 tools (e.g., Devin's ~14% success rate on the SWE-bench benchmark 23) indicates that the field is still far from achieving this level of dependability. Safety and Security: An AI agent possessing full autonomy over a codebase could inadvertently introduce critical bugs, security vulnerabilities, or cause system-wide instabilities. Ensuring the safety of such highly autonomous systems is a monumental challenge.36 As noted in safety research, for systems operating above certain risk or capability thresholds, \"the burden of demonstrating such safety guarantees should be on the systems' developers\".36 Alignment: Ensuring that an AI agent's self-determined goals remain consistently aligned with human values, ethical considerations, and overarching organizational objectives is crucial and notoriously difficult [User Query]. An AI optimizing for a poorly defined or incomplete metric could lead to highly undesirable or even harmful outcomes. The concept of recursive self-improvement, a potential capability of Level 5 systems, could lead to rapid, unpredictable advancements that surpass human comprehension or control if not impeccably aligned from the outset.34 Interpretability and Debuggability: Understanding why a Level 5 AI decided to make a particular architectural change or pursue a specific development goal would be essential for oversight and trust, yet this could be exceedingly difficult with highly complex, potentially \"black box\" AI systems. Ethical Considerations: The prospect of Level 5 autonomy raises profound ethical questions regarding accountability for AI-driven decisions, the potential for large-scale job displacement within the software industry, and the risks associated with the misuse of such powerful autonomous systems. Future Outlook &amp; Industry Perspectives:While true Level 5 autonomy is not on the immediate horizon, the rapid pace of innovation, particularly the progression from Level 2 to emerging Level 4 tools observed between 2022 and 2025, suggests that AI capabilities in software development will continue to advance significantly. Industry experts predict that AI will increasingly handle more complex tasks. For instance, Outsystems envisions AI generating entire applications, not just code, and anticipates that human-AI interactions will become increasingly multi-modal.106 A survey by IBM and Morning Consult indicated that 99% of developers were exploring or actively developing AI agents in 2025, although experts debate whether current systems described as \"agents\" possess true autonomy or are more accurately characterized as sophisticated orchestration frameworks.107Sourcegraph, a company developing advanced context-aware AI tools like Cody, currently emphasizes AI's role as an assistant designed to automate \"mundane tasks\" rather than as a fully autonomous replacement for human programmers.37 Their focus is on \"context-aware AI coding assistance\" that leverages a deep understanding of the entire codebase.The immediate future will likely see continued improvements in Level 3 and Level 4 tools, making them more reliable and capable of handling complex, developer-defined tasks with decreasing levels of supervision. True Level 5 autonomy, where an AI might act as a \"project lead\" by setting its own strategic software development goals, remains a longer-term research and development challenge. The evolutionary path towards this ultimate level of autonomy will likely be incremental, perhaps first focusing on \"practical applications of geo-fenced Level 4 autonomy\" before Level 5 becomes a widespread reality, mirroring the cautious and staged progression observed in the development of autonomous vehicles.4The journey towards Level 5 autonomy is not merely an extrapolation of current code generation capabilities. It necessitates fundamental breakthroughs in several areas, including AI-driven goal formulation, the capacity for self-improvement on complex real-world software systems (not just on its own codebase or isolated benchmarks), and the development of robust, verifiable safety and alignment mechanisms. Current leading-edge agents (Level 3/4) still operate based on goals defined by humans (e.g., \"fix this bug,\" \"implement this feature\"). Level 5 requires the AI to define these strategic goals independently. While research like SICA 26 demonstrates self-improvement on an agent's own codebase, applying this capability to improve diverse, evolving enterprise applications is a vastly more complex challenge. Similarly, AlphaDev's success 28 lies in discovering specific, albeit fundamental, algorithms; a Level 5 system would need to apply such innovative capabilities broadly and strategically across entire software ecosystems. The significant concerns surrounding safety, trust, and alignment 35 are not just engineering hurdles but profound research questions that must be satisfactorily addressed before Level 5 autonomy can be responsibly achieved. Simply making LLMs incrementally better at coding will not suffice.Consequently, the evolution towards Level 5 will likely involve a hybrid approach. In this model, AI agents might take on increasingly larger and more complex sub-systems or entire lifecycle stages autonomously, but they would operate within a framework that is still ultimately governed, validated, and strategically directed by humans, at least for the foreseeable future. The current progression shows AI tools taking on more sequential steps (Level 2 to Level 3) and larger, more integrated tasks (Level 3 to Level 4). Even optimistic projections for AI agents in 2025 see them primarily as augmenting human developers or automating specific parts of the workflow, rather than achieving complete replacement.107 The concept of \"AI generating applications\" 106 or AI autonomously handling complex \"migrations\" 21 suggests AI tackling entire work packages. However, the strategic decision to build that application or perform that migration still rests with human stakeholders. Therefore, a practical path towards something resembling Level 5 might involve highly autonomous AI \"teams\" responsible for specific software modules or services, but operating under human-defined strategic directives and rigorous quality assurance gates.Comparison Table of Developer AI Tools by Autonomy and CapabilitiesThe following table provides a summarized comparison of the developer AI tools discussed in this report, categorized by their primary autonomy level and highlighting their key characteristics, interaction styles, strengths, and limitations. This table serves as a quick reference to differentiate the capabilities and intended use cases of various tools in the rapidly evolving landscape of AI-assisted software development. Understanding these distinctions is crucial for developers and engineering leaders seeking to effectively integrate AI into their workflows. Tool NameAutonomy LevelInteraction StyleKey StrengthsNotable LimitationsVS Code IntelliSense1 (Basic Assist)Inline IDE suggestions as you typeFast, context-aware completions for syntax &amp; names based on language semantics and source analysis 7; works offline; reliable for known code patterns; extensible.No true \"AI\" generation of novel logic; limited to current file/project context; cannot understand higher-level intent [User Query].Tabnine1 (with L3 agent features)IDE plugin, ML-based autocomplete; AI chat; AI agents for specific tasks 8ML-driven suggestions for full lines/functions; supports many languages; local models for privacy 8; team-trained models 10; agents for docs, review, Jira tasks.8Core completion reactive; agent features newer; quality varies with context; large local models can be resource-intensive.10GitHub Copilot (Standard Mode)2 (Pair Programmer)IDE plugin (inline suggestions); Copilot Chat interface 10Generates larger code blocks/functions from comments/context 10; broad framework knowledge; seamless IDE integration; chat for Q&amp;A/explanations.10Can produce incorrect/insecure code requiring review 6; limited full-project context; cloud-dependent; privacy/licensing concerns [User Query].Amazon CodeWhisperer2 (Pair Programmer)IDE plugin (inline suggestions) 11Optimized for AWS services and APIs 11; generates multi-line completions from comments; security scans 11; free for individuals.47 (Becoming part of Amazon Q Developer 11)Strong AWS bias, less effective for non-AWS work 47; suggestions can be basic; output needs review; requires AWS account [User Query].OpenAI Codex (API) / GPT Models2 (Pair Programmer)API/Playground; via ChatGPT (prompt-response); new \"Codex\" agent in ChatGPT 12Powerful natural language to code generation (esp. GPT-4.1 13); many languages; new \"Codex\" agent can edit files, run commands.48Original Codex API deprecated 12; API use requires prompt engineering, output needs validation; context limits; advanced access often paid [User Query].Replit Ghostwriter2 (Pair Programmer)Replit IDE integration (inline autocomplete &amp; Ghostwriter Chat) 49Tight Replit integration for instant execution/testing; multi-line/function completion, refactoring, explanation, proactive debugger 50; good for learning/prototyping.Primarily Replit-locked 51; quality similar to other LLM tools (needs review); advanced AI features are paid 49; internet-dependent.51GitHub Copilot (Agent Mode)3 (Agentic \u2013 supervised)IDE chat/command (VS Code); GitHub issue assignment 14Plans &amp; executes multi-file edits, runs tests, iterates on errors autonomously based on high-level goals/issues 14; creates PRs 44; developer kept in loop.Newer feature (stable in VS Code 14); can misinterpret complex requests, output needs review; performance varies with repo complexity.14Cursor (AI Editor)3 (Agentic \u2013 supervised)Full AI-based code editor (VS Code fork) with chat &amp; agent modes 15Deep codebase indexing for global context 15; multi-file ops, refactoring; Agent Mode explores, plans, executes with tools 15; \"YOLO mode\" for auto-execution (user term for auto-run) 57; checkpoints for safety.15Standalone editor (though high VS Code compatibility 56); auto-run/YOLO mode is risky 57; performance/cost depends on chosen LLM API keys.55Windsurf (Cascade Agent)3 (Agentic \u2013 supervised)AI-driven IDE (VS Code-based) with \"Cascade\" agent (Write/Chat modes) 63Cascade agent modifies code, runs it, debugs, iterates until success (Write Mode) 16; uses tools (search, terminal, web); auto-fixes lint errors 63; \"Turbo mode\" for auto-execution.63Newer tool; auto-execution (\"Turbo mode\") risky; must use its IDE; can make errors requiring human fix 68; some features paid/enterprise.65Anthropic Claude Code3 (Agentic \u2013 supervised)Terminal-based chat agent; IDE integrations (VS Code, JetBrains) 19Large context understanding; edits files (presents diffs), runs tests, Git ops (history, merge, commit/PR) 19; explains reasoning; SDK for custom agents.69CLI interface might intimidate some; currently in beta/research preview 19; uses paid API (costly for extensive use) 71; extremely large projects may still pose challenges [User Query].ChatGPT (with Plugins &amp; Advanced Data Analysis)3 (Agentic \u2013 supervised)Conversational AI (web UI or API) with Code Interpreter &amp; plugins (tool use) 75Highly flexible multi-tool agent: can fetch docs, run/debug Python code iteratively, use Git (via plugins) 75; strong natural language understanding (GPT-4).74Not an IDE tool by default (copy-paste workflow unless plugins bridge gap) [User Query]; plugin reliability varies; context limits; requires paid subscription for advanced features.82Aider (CLI + Git assistant)3 (Agentic \u2013 supervised)CLI chat tool working on local Git repo 17Excellent Git integration (auto-commits, diffs); auto-runs linters/tests &amp; attempts fixes 17; supports various LLMs (cloud/local) 17; open-source.18CLI-based (less visual, steeper curve) 18; requires API key setup; auto-fix quality depends on tests/linters; model errors possible [User Query].Amazon Q Developer CLI3 (Agentic \u2013 supervised)Interactive CLI agent (natural language to CLI commands &amp; code edits) 85AI orchestrates local CLI tools (compilers, package managers, AWS CLI) 86; strong for AWS/cloud tasks; uses powerful models (Claude via Bedrock) 85; iterative adjustments based on feedback.85Newer tool; executing commands automatically has inherent risks [User Query]; primarily benefits AWS users; requires AWS setup [User Query].Google Jules4 (High Autonomy)Asynchronous cloud-based agent; GitHub integration (task assignment &amp; PRs) 20Autonomous coding agent: takes high-level goals (features, bugs), works independently in cloud VM, presents PRs 20; uses Gemini 2.5 Pro; provides plan/reasoning; audio changelogs.20Very new (public beta May 2025) 20; cloud-only (code sent to Google VM); asynchronous (no real-time interaction); high autonomy needs careful review of results [User Query].Cognition Devin4 (High Autonomy)Cloud AI service (task assignment via dashboard/Slack/CLI) 22AI Software Engineer: plans &amp; executes complex end-to-end tasks using own shell, editor, browser 21; strong SWE-bench (13.86%) 23; Devin 2.0 has interactive planning, search, wiki.22Expensive enterprise tool [User Query]; success on very complex/novel tasks still limited 23; requires codebase access by third-party; some demo authenticity concerns raised.22GPT-Engineer (open-source)4 (High Autonomy, experimental)CLI tool: single prompt -&gt; clarification dialogue -&gt; codebase generation 24Can bootstrap entire (simple) projects from one prompt; automates initial spec-to-code process 24; open-source and customizable.24Highly prompt-dependent; generated code needs significant review/debugging for complex apps 24; primarily for initial generation, not iterative work on existing code [User Query].AutoGPT (for coding tasks)4 (High Autonomy, experimental)Self-directed AI agent framework (user sets high-level goal, agent plans &amp; executes with tools) 98Fully automated workflow: breaks goal into sub-tasks, uses tools (web search, file I/O, code execution) iteratively without human prompts.98Often inefficient, prone to loops or incorrect paths without precise goals/human feedback 100; not specialized for coding; high API costs; mainly experimental for dev tasks.99 (Note: Tools at Level 5 are omitted from the table as no productized examples currently exist; this level remains theoretical and experimental.)ConclusionThe landscape of AI-powered developer tools is evolving at a remarkable pace, progressing from rudimentary Level 1 code completion assistants to the sophisticated, emerging Level 4 autonomous coding agents. This report has categorized these tools across a five-level autonomy framework, providing a structured understanding of their capabilities, interaction modes, and the shifting role of the human developer in the software creation process.Level 1 tools, such as traditional IDE IntelliSense and early ML-enhanced completers like Tabnine (in its core function), offer essential micro-task assistance, accelerating typing and reducing trivial errors by predicting likely code sequences based on immediate local context. They represent a foundational layer of AI assistance, valued for their responsiveness and low overhead.Level 2 tools, exemplified by AI pair programmers like GitHub Copilot (standard mode), Amazon CodeWhisperer, and direct use of models like OpenAI's GPT series, mark a significant step up. These tools can generate substantial code blocks or entire functions from natural language prompts or broader contextual cues. However, their operation is reactive and single-step, firmly positioning the developer as the architect and quality controller responsible for validating and integrating each piece of AI-generated code. The specialization of tools like CodeWhisperer for specific ecosystems (e.g., AWS) at this level highlights a strategy to provide targeted value.The transition to Level 3 signifies a paradigm shift, with AI becoming a context-aware agent capable of supervised automation. Tools such as GitHub Copilot Agent Mode, Cursor, Windsurf (with Cascade), Anthropic's Claude Code, Aider, and the Amazon Q Developer CLI can autonomously perform multi-step coding tasks. Given a high-level goal, these agents can plan and execute a sequence of actions\u2014editing multiple files, running tests, interacting with version control, and iteratively debugging\u2014while the developer supervises and reviews the outcomes. The deep integration of these agents with the developer's environment (IDE or CLI) and their ability to use external tools are hallmarks of this level. The increasing emphasis on Git integration, automated testing, and even commit/PR generation at this level indicates that these tools are not just writing code but are beginning to automate aspects of the broader development workflow. This trend suggests that as AI tools become more autonomous in code generation, the logical progression is to integrate them more deeply into the surrounding lifecycle processes to maximize productivity gains. This, in turn, implies a growing need for comprehensive and well-maintained test suites, as the AI's ability to \"fix until tests pass\" is fundamentally constrained by the quality of the tests themselves.Level 4 tools, such as Google's Jules and Cognition's Devin, push the boundaries towards highly autonomous coding. These agents aim to handle complex projects or features end-to-end with minimal human input, aspiring to deliver production-ready code. Their architecture often involves cloud-based execution, access to a full suite of developer tools, advanced planning, and long-term memory. While these systems demonstrate impressive capabilities on benchmarks and specific tasks, their general reliability for all complex scenarios is still evolving, and the \"human in the loop\" for final validation remains crucial. The emergence of such tools signifies an ambition for AI to transition from a highly capable assistant to a largely independent \"software engineer,\" though the gap between current demonstrations and consistent, trustworthy production of complex software remains substantial.Finally, Level 5\u2014full autonomy where AI agents set their own strategic software development goals\u2014remains largely theoretical and experimental. Research into self-improving AI systems, AI-driven algorithm discovery, and multi-agent collaboration provides glimpses into this future. However, the profound challenges related to trust, safety, alignment, and the sheer complexity of strategic software development mean that Level 5 is a distant prospect. The path towards such advanced autonomy will likely be incremental, building upon the successes and addressing the limitations of Level 3 and 4 tools.In conclusion, the integration of AI into software development is undeniably transformative. As tools progress through the autonomy levels, they offer the potential to significantly enhance developer productivity, automate tedious tasks, and even augment human creativity. However, this progression also necessitates a continuous re-evaluation of developer roles, skills, and workflows. Understanding the specific autonomy level, capabilities, and limitations of each AI tool is paramount for its effective and safe adoption. While the allure of fully autonomous AI is compelling, the current and near-future reality is one of human-AI collaboration, where AI serves as an increasingly powerful and intelligent partner, empowering developers to tackle more complex challenges and innovate at an accelerated pace. The ultimate objective is not necessarily the replacement of human developers but the augmentation of their capabilities, fostering a new era of software engineering.SourcesInformation and claims within this report are supported by inline citations referencing the provided research materials, designated by IDs such as 10 or.7 These references point to official documentation, technical blogs, academic papers, and reputable industry analyses relevant as of early-to-mid 2025.# From Code Completion to Autonomous Coding Agents: AI Developer Tools by Autonomy LevelThe software development landscape is undergoing a significant transformation, largely propelled by the rapid advancements and integration of Artificial Intelligence (AI). This evolution is underscored by substantial growth in private AI investment, with generative AI alone attracting $33.9 billion globally in 2024, marking an 18.7% increase from the previous year.1 AI is no longer confined to research laboratories; it is becoming a pervasive force in daily life and business operations, evidenced by 78% of organizations reporting AI use in 2024, a notable rise from 55% in 2023.1 This trend extends deeply into the realm of developer tooling, where AI-powered solutions are becoming increasingly sophisticated. The discourse itself is shifting from AI as a tool for mere \"automation\" to AI exhibiting \"autonomy,\" signifying a deeper capacity for independent operation and decision-making.2 Accenture's Technology Vision for 2025 highlights this transition, projecting a future where AI acts autonomously on behalf of individuals, with a reported 75% of knowledge workers already utilizing generative AI.2 This massive investment and widespread adoption signal strong market confidence and a perceived return on investment, suggesting that AI developer tools are being embraced at a pace necessary to remain competitive and enhance productivity, aligning with research indicating AI's potential to boost productivity and narrow skill gaps.1This report provides an expert-level analysis of these AI developer tools, categorizing them based on a five-level autonomy framework. This framework, analogous to the levels established for autonomous driving systems, serves to delineate the evolving capabilities of these tools, from basic code assistants to highly autonomous coding agents. Such a classification is pertinent because both autonomous driving and AI coding involve tasks traditionally performed by humans that demand complex reasoning, contextual understanding, and decision-making. The staged levels in both domains represent milestones in the delegation of these responsibilities to AI, thereby raising similar questions about reliability, accountability, and the evolving role of human oversight.4 The challenges anticipated for Level 5 autonomy in vehicles, such as regulatory hurdles, technological limitations, and development costs 4, are likely to find parallels in the domain of AI coding. This report will explore the capabilities, interaction modes, use cases, and the pros and cons of representative tools at each autonomy level, ensuring factual accuracy through direct references to current research and official documentation. The increasing integration of AI into development workflows promises substantial productivity gains 1 but also introduces new considerations regarding developer roles, code quality, security, and trust. This document aims to navigate these complexities, offering a clear perspective on the current state and future trajectory of AI in software engineering.Autonomy Levels for AI Coding ToolsTo provide a structured understanding of their capabilities, AI coding tools are classified into autonomy levels, drawing an analogy to the widely recognized levels of autonomous driving. This classification helps to differentiate tools based on the degree of human oversight required and the complexity of tasks they can handle.</p> <p>Level 1 \u2013 Basic Code Completion:AI at this level offers single-line or small code suggestions based on the immediate context, such as the current file content and syntax. The developer remains in full control, accepting or rejecting these suggestions for micro-tasks. This functionality is comparable to basic cruise control in automobiles. Tools categorized under Level 1 are designed to accelerate typing and minimize trivial errors but do not generate complex code blocks, understand broader project context, or make independent decisions regarding logic or structure [User Query]. Their primary function is to react to the developer's current input by predicting likely code sequences. The focus is on immediate lexical and syntactic context rather than deep semantic understanding. For instance, Visual Studio Code's IntelliSense provides completions based on \"language semantics and an analysis of your source code,\" but for many languages, this is limited to \"word-based completions\" without specific extensions 7, reacting to typed input within the current file. Similarly, Tabnine offers \"context-aware suggestions based on your code and patterns\" but mainly provides \"code completions for current line and multiple lines for full-function implementation\" 8, which is also a reactive process. The core function is to \"predict likely code to follow,\" not to comprehend higher-level intent [User Query].</p> <p>Level 2 \u2013 Advanced Code Generation (AI Pair Programmer):AI systems at this level can produce larger code snippets or entire functions based on natural language prompts (e.g., comments within the code) or a broader local context. The AI functions akin to a junior pair programmer. However, each output from the AI is typically a single-step operation that is subsequently reviewed and integrated by the human developer. Developers continue to guide the overall software architecture, verify the correctness of the generated code, and explicitly prompt the AI for each significant generation task [User Query]. These tools are often powered by large language models (LLMs) trained on extensive code repositories. The defining characteristic of Level 2 is the AI's capacity to translate natural language intent into substantial code blocks, moving beyond simple completion to actual generation, yet still necessitating explicit human prompting for each distinct operation. For example, GitHub Copilot can suggest \"whole lines or entire functions\" based on comments or existing code context 10, with the interaction being a prompt-response for each generated piece. Amazon CodeWhisperer generates \"suggestions based on your existing code and comments,\" which can range from \"a single line comment to fully formed functions\" 11, initiated by the developer's input. OpenAI Codex and its more advanced successors like GPT-4 are designed to \"translates natural language into code\" 12, representing a direct prompt-to-code generation step. In all these instances, the human developer acts as the driver, prompting for each segment of code and undertaking its review, as the AI does not autonomously chain actions.</p> <p>Level 3 \u2013 Context-Aware Agents (Supervised Automation):</p>"},{"location":"software/code/leaderboards/","title":"AI Coding Leaderboards","text":"<p>Overview</p> <p>This page is a list of benchmarks for LLMs that are used to help with coding.</p> <p>It covers both Coding Assistants and Autonomous Coding Agents</p>"},{"location":"software/code/leaderboards/#leaderboards","title":"Leaderboards","text":"Leaderboard Category Tasks Metrics Aider LLM Leaderboards Coding Assistant 225 Exercism exercises across C++, Go, Java, JavaScript, Python, Rust Two-pass pass rates; cost per run; edit correctness EvalPlus Leaderboard Coding Assistant HumanEval+ (164 hand-verified Python); MBPP+ (399 sanitized Python) pass@1 (greedy); extended efficiency via EvalPerf TabbyML Coding LLMs Leaderboard Coding Assistant Amazon CCEval next-line tasks in Python, JS, Go\u2026 Next-line accuracy (exact-match of very next line) MHPP Leaderboard Coding Assistant 210 \u201cMostly Hard\u201d multi-step Python problems pass@1 (greedy); sampling (T=0.7, 100 runs) Copilot Arena Coding Assistant Paired autocomplete &amp; inline-editing comparisons ELO-style rankings from user votes WebDev Arena Leaderboard Coding Assistant Real-time web development challenges between models Win rate; task completion; user voting SWE-bench Autonomous Agent 2,294 real-world \u201cFail-to-Pass\u201d GitHub issues from 12 Python repos % of issues resolved HAL (Holistic Agent Leaderboard) Autonomous Agent 13 benchmarks (e.g., SWE-bench Verified, USACO, Cybench, TAU-bench) across many domains Cost-controlled evaluations; success rates; Pareto fronts TBench Autonomous Agent Terminal-based complex tasks in realistic environments Task success rate; command accuracy; time-to-completion"},{"location":"software/code/leaderboards/#takeaways","title":"Takeaways","text":"<p>Key Takeaways</p> <p>Leaderboards are a good way to quantitatively and objectively compare solutions.</p> <p>Comparison across multiple metrics and leaderboards avoids solutions that overfit to a benchmark.</p>"}]}
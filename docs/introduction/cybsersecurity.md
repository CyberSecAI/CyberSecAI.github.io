# LLMs for CyberSecurity
   
## LLMs for CyberSecurity References
1. [Generative AI and Large Language Models for Cyber Security: All Insights You Need](https://arxiv.org/pdf/2405.12750), May 2024
2. [A Comprehensive Review of Large Language Models in Cyber Security](https://www.researchgate.net/publication/384500263_A_Comprehensive_Review_of_Large_Language_Models_in_Cyber_Security), September 2024
3. [Large Language Models in Cybersecurity: State-of-the-Art](https://arxiv.org/pdf/2402.00891), January 2024
4. [How Large Language Models Are Reshaping the Cybersecurity Landscape](https://elie.net/talk/ai-for-cybersecurity-get-started-today) | Global AI Symposium talk, September 2024
5. [Large Language Models for Cyber Security: A Systematic Literature Review](https://arxiv.org/pdf/2405.04760), July 2024
6. [Using AI for Offensive Security](https://cloudsecurityalliance.org/artifacts/using-ai-for-offensive-security), June 2024


## Agents for CyberSecurity References
1. [Blueprint for AI Agents in Cybersecurity - Leveraging AI Agents to Evolve Cybersecurity Practices](https://www.cybersec-automation.com/p/blueprint-for-ai-agents-in-cybersecurity)
2. [Building AI Agents: Lessons Learned over the past Year](https://medium.com/@cpdough/building-ai-agents-lessons-learned-over-the-past-year-41dc4725d8e5)




## Comparing LLMs
There are several sites that allow comparisons of LLMs e.g.


1. https://winston-bosan.github.io/llm-pareto-frontier/
   1. LLM Arena Pareto Frontier: Performance vs Cost
2. https://artificialanalysis.ai/
     1. Independent analysis of AI models and API providers. Understand the AI landscape to choose the best model and provider for your use-case
3. https://llmpricecheck.com/
     1. Compare and calculate the latest prices for LLM (Large Language Models) APIs from leading providers such as OpenAI GPT-4, Anthropic Claude, Google Gemini, Mate Llama 3, and more. Use our streamlined LLM Price Check tool to start optimizing your AI budget efficiently today!
4. https://openrouter.ai/rankings?view=day 
     1. Compare models used via OpenRouter
5. https://github.com/vectara/hallucination-leaderboard 
     1. LLM Hallucination Rate leaderboard
6. https://lmarena.ai/?leaderboard
     1. Chatbot Arena is an open platform for crowdsourced AI benchmarking
7. https://aider.chat/docs/leaderboards/
     1. Benchmark to evaluate an LLMâ€™s ability to follow instructions and edit code successfully without human intervention
8. https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro
     1. Benchmark to evaluate language understanding models across broader and more challenging tasks

 
See also [Economics of LLMs: Evaluations vs Pricing - Looking at which model to use for which task](https://medium.com/data-science-collective/economics-of-llms-evaluations-vs-pricing-04802074e095)


